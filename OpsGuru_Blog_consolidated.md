# OpsGuru Announces Expansion to Central and Eastern Canada and Welcomes Adam Chandani as VP, Sales

## Introduction

OpsGuru announces the expansion of its leadership team and services to Central and Eastern for cloud migration Canada. This investment allows OpsGuru to expand the support of its clients across North America. As the Canadian AWS Consulting partner of the year for 2021, a Microsoft Gold Cloud Consulting partner and Google Cloud Premier partner, OpsGuru helps organizations accelerate their cloud transformation initiatives without limits including comprehensive cloud consulting services.

As part of the investments, industry veteran Adam Chandani will build and lead sales efforts as VP, Sales of OpsGuru. Through a wealth of experience in the cloud consulting industry and deep expertise in cloud services spanning Mid-Market, Enterprise, Public Sector and SaaS organizations, Adam will assist OpsGuru clients by leveraging cloud technologies to transform their view of IT and establish new revenue streams.

‚ÄúWe are thrilled to welcome Adam Chandani as our VP, Sales for OpsGuru. Adam joins the extended team of over 60 people based in Ontario, and we look forward to continued investments across Canada. This expansion accelerates our vision to become the leading end-to-end multi-cloud service provider, helping Canadian companies plan, migrate, and operate successfully in the cloud,‚Äù said John Witte, President & CEO at Carbon60.

Before joining the OpsGuru team, Adam was a go-to-market leader, responsible for sales, presales, demand generation and marketing across Canada. Through his previous experience leading sales teams across the country, Adam has found a real passion for helping clients succeed in achieving their goals by delivering the highest cloud technologies skillset ‚Äîfrom cloud adoption to scaling cloud across teams to designing and building cloud-native applications.

‚ÄúI am thrilled to join Canada‚Äôs leading team of multi-cloud certified engineers and architects as OpsGuru continues its impressive growth. OpsGuru does it ‚Äúthe right way,‚Äù guiding, enabling and delivering exceptional results to help customers achieve their business goals. I look forward to leading our teams across Canada and helping clients successfully adopt and leverage cloud technologies,‚Äù said Adam Chandani.

Year over year, OpsGuru has grown its team of cloud-certified professionals by 306% and revenues by 400%, along with helping hundreds of customers leverage cloud technologies. As both an innovator and partner on the cloud journey, OpsGuru has proven success in assisting clients in operating production workloads on public clouds with confidence and peace of mind.

## What This Means for OpsGuru Customers

* As Canada‚Äôs leading multi-cloud experts, OpsGuru‚Äôs services expand to Central and Eastern Canada, with an extended team of over 60 based in Ontario.

* OpsGuru‚Äôs new VP of Sales, Adam Chandani, will leverage his deep experience in the cloud consulting industry and building world-class sales teams to successfully navigate Canadian clients through their cloud journey. with enterprise-grade cloud migration and DevOps solutions

Contact us today to learn more about how OpsGuru can help you on your digital transformation journey.

For more information, please read our press release.

# OpsGuru Achieves the AWS Service Delivery Designation for Amazon EKS

## Introduction

OpsGuru announced today that it has achieved the Amazon Web Service (AWS) Service Delivery designation for Amazon Elastic Kubernetes Service (Amazon EKS), recognizing that OpsGuru has proven success in helping customers architect, deploy, and operate containerized workloads on top of Amazon Kubernetes and Kubernetes consulting,.

Achieving the Amazon EKS Service Delivery designation differentiates OpsGuru as an AWS Partner Network (APN) member that has a deep understanding of Amazon EKS, demonstrated experience, and proven customer success helping customers manage, deliver, and optimize containerized workloads with Amazon EKS. To receive this designation, APN Partners must possess deep AWS experience and deliver solutions seamlessly on AWS.

‚ÄúWe are proud to receive this designation, demonstrating OpsGuru‚Äôs expertise in helping our valued customers achieve new levels of efficiency, innovation, and operational excellence through application modernization with containers and micro-services on Amazon Elastic Kubernetes Services,‚Äù said Robin Percy, CTO and Co-Founder at OpsGuru. ‚ÄúOur team is dedicated to helping companies achieve their technology goals by leveraging the flexibility, breadth of services, and pace of innovation that AWS enables.‚Äù

AWS is enabling scalable, flexible, and cost-effective solutions from startups to global enterprises. To support the seamless integration and deployment of these solutions, AWS established the AWS Service Delivery Program to help customers identify APN Consulting Partners with deep experience in delivering specific AWS services.

Through its Kubernete for cloud-native infrastructure,s Enablement and Cloud Launchpad solutions which provide proven methodologies for accelerating EKS modernization to AWS, OpsGuru has helped many organizations overcome the steep learning curve of Kubernetes and take full advantage of the performance, scale, reliability, and availability of AWS infrastructure.

‚ÄúOur partnership with AWS & OpsGuru is pivotal to Cocoflo fulfilling our growth objectives,‚Äù said Bernie Florido, CEO and Founder at Cocoflo after partnering with OpsGuru for an Amazon EKS-based solution that allowed the company to scale while mitigating costs.

Contact us today to learn more about how OpsGuru can help you on your digital transformation journey.

For more information, please read our press release.

# OpsGuru Achieves the AWS Service Delivery Designation for Amazon OpenSearch Service

**Introduction**

OpsGuru announced today that it has achieved the Amazon Web Service (AWS) Service Delivery designation for Amazon OpenSearch Service, recognizing that OpsGuru provides deep technical knowledge, experience, and proven success in delivering Amazon OpenSearch to customers.

Achieving the Amazon OpenSearch Service Delivery designation differentiates OpsGuru as an AWS Partner Network (APN) member, helping customers to perform interactive log analytics, real-time application monitoring, website search, and more. To receive this designation,  including semantic search and advanced data analytics,APN Partners must possess deep AWS experience and deliver solutions seamlessly on AWS.

‚ÄúOpsGuru takes pride in helping our customers unlock and leverage high-quality data to make better business decisions and create a foundation for ongoing innovation,‚Äù says Mency Woo, VP of Enterprise Transformation at OpsGuru. ‚ÄúWe are proud to receive the Amazon OpenSearch Service Delivery designation, demonstrating our success in helping customers leverage the scale, speed, and flexibility of the Amazon OpenSearch to get the most out of their data.‚Äù

AWS is enabling scalable, flexible, and cost-effective solutions from startups to global enterprises. To support the seamless integration and deployment of these solutions, AWS established the AWS Service Delivery Program to help customers identify APN Consulting Partners with deep experience in delivering specific AWS services.

With its data and analytics expertise, OpsGuru helps organizations harness high-quality data and transform it into actionable insights.

Contact us today to learn more about how OpsGuru can help you on your digital transformation journey.

For more information, please read our press release.

## 

# Kubernetes workloads migration from Azure AKS to Amazon EKS

## Introduction

‚Ä¢ Customer Success

January 6, 2025

## Background

Authomize continuously monitors client organizations‚Äô identities, access privileges, assets, and activities, to secure all apps and cloud services. It seamlessly connects to a client‚Äôs apps and cloud services, and collects all relevant information to graph‚Äôs data-lake to help client organization security teams achieve Zero Trust and compliance with security best practices,.

Authomize provides organizations with comprehensive observability, actionable insights, and remediation automation, enabling adherence to security and compliance requirements.

## The Challenge

Authomize needed a solution that would facilitate rapid launch of their Kubernetes-based with optimal Kubernetes consulting services application platform for complex deployments in AWS making sure to establish AWS Best Practices foundations and to enable robust and reliable application deployments that meet Companies‚Äô security and compliance requirements.

Authomize knew they needed a partner who had a deep level of expertise in the cloud platform, Kubernetes, Helm, and Infrastructure as Code to maximize their success on Amazon Web Services.

## Our Solution

Authomize engaged OpsGuru, a certified AWS Premier Partner, due to the team‚Äôs extensive AWS experience and a proven track record with complex workload migrations. and cloud platform expertise,

With OpsGuru‚Äôs help, Authomize was able to migrate its Kubernetes workloads to AWS with a simple and manageable resources hierarchy using Terraform and Helm for all of their environments.

OpsGuru worked alongside the Authomize engineering team to review and customize their infrastructure code and software development lifecycles. This process ensured that Authomize was able to rapidly deploy workloads to development and production environments leveraging AWS advanced features.

‚Ä¢ Account Isolation

‚Ä¢ Network Design

‚Ä¢ Kubernetes Baseline (best practices for container orchestration)

‚Ä¢ Efficient Load Balancing

‚Ä¢ Secrets and Configuration management

Environment-specific AWS accounts allowed resources grouping within environments as well as workload isolation. A centralized shared services account was used for the CI/CD and as a centralized container image repository for the environment-specific AWS accounts.

Implemented VPCs in each AWS account, with utilization of AWS PrivateLink to safely interact with AWS services such as S3, and VPC peering to inter-connect environment-specific VPCs with a shared services VPC.

Implemented infrastructure components and configuration management solution for frictionless management of the Amazon EKS clusters with configurable Amazon EC2 managed node groups.

Implemented nginx-ingress Ingress Controller within the EKS clusters to utilize Amazon ELB load balancing for the ingress resources and services of type loadbalancer.

Utilized AWS Systems Manager Parameter Store and AWS Secrets Manager for configuration storage of the Kuberenetes secrets through Kubernetes Secrets Store CSI Driver with AWS provider.

OpsGuru then assisted the Authomize team in their migration to the newly built AWS platform by extending the existing Helm charts to support deployment into AWS EKS.

After the completion of the project, OpsGuru provided comprehensive training sessions for Authomize‚Äôs team as well as documentation and operational playbooks for the newly designed systems. The training and documentation included the operation of EKS, short-lived credentials with IRSA and observability tools, among other topics.

## The Result

As a result of the collaboration between the teams, Authomize was able to rapidly migrate the existing Azure AKS workloads to EKS, without any side effects or downtime in the process. All the components have been successfully deployed and validated allowing Authomize to effortlessly continue with their platform development on AWS.

# Automate Deployment to AWS with GitHub Actions and DevOps Automation

## Introduction

In previous posts we have looked at the popularity of GitOps and a number of tools available to implement GitOps. Among the tools there are GitHub Actions. Given the popularity of GitHub in both enterprises and open-sourced communities, let‚Äôs walk through how to set up GitHub Actions. and automated CI/CD pipelines

GitHub Actions is a new feature from GitHub that allows you to use serverless compute resources to build binaries, perform tests, execute deployments or just run any Linux or Windows commands without having to raise servers.

Let‚Äôs demonstrate how easy it is to use, what the possibilities are and how you can create a fully functional CI/CD Pipeline in minutes.

With GitHub Actions Workflows you can automate tests, builds and deployments.

In order to enable GitHub Actions Workflows, all you need is to add a Workflow file into your repository. Workflow file is a YAML file located in the folder ./github/workflows/.

For example you can see we have a Workflow file called my-pipeline.yaml under .github/workflows/

Let‚Äôs explore a simple Workflow file to understand the syntax. By the way, if you are familiar with Ansible Playbooks and Dockerfile syntax, you will find it to be very similar.

Here we have one GitHub Actions Workflow called My-Future-CICD-Pipeline that consists of two jobs my\_testing and my\_deploy, and we configured jobs dependency so only when job my\_testing finishes running, job my\_deploy begins execution.

This Workflow file will start on trigger Push to Master Branch, basically when this workflow should be executed

Consider having a few of the workflow files, for example:

‚Ä¢ lint\_validation.yml, which will be executed on every Pull Request

‚Ä¢ deploy\_to\_staging.yml, which will be executed on Push to staging branch

‚Ä¢ deploy\_to\_prod.yml, which will be executed on Push to master branch

We are using GitHub Action VM Runner ubuntu-latest to execute our workflow, but you can use any other available options.

The first step on each job configured is to check out the repository where this workflow is located, such that the entire repository code will be available on VM Runner for our validation, testing, compiling or any other code manipulation.

We have configured Environment Variables on three different levels:

1\. Top Global Level ‚Äì variables available to all jobs and all steps in this Workflow file

2\. Job Level ‚Äì variables available to all steps in this job

3\. Step Level ‚Äì variable available only to this step

How does it feel now? Looks familiar and not as Groovy for Jenkins right?

It will include this file in our repository and perform any push to master, this workflow will be executed.

If you click on the name of your Workflow (My-Future-CICD-Pipeline) you will be able to see that status of all Jobs and Steps:

As you can see, all steps have passed.

Well we didn‚Äôt do anything special here, it was just an example for basic understanding. Notice we had set up the workflow step ‚ÄúTest aws cli installation,‚Äù where we verify if the aws cli is available. A passing status of this step means that aws cli is pre-installed on this VM Runner machine. We can use awscli to deploy our code from repository to AWS streamlining cloud deployment workflows.

Let‚Äôs make this happen\! For this workflow to work though, we need to configure AWS credentials, which will be used to interact with our AWS account.

GitHub Repository Secrets

GitHub Repository supports secrets. They are secret environment variables that are fully encrypted. You will never see the values of those secrets once they are added. The secret environment variables can be accessed from any part of any workflow in this repository.

Let‚Äôs define our AWS credentials as GitHub repository secrets:

‚Ä¢ Create AWS IAM User with appropriate permissions for deployment tasks

‚Ä¢ Generate AWS\_ACCESS\_KEY and AWS\_SECRET\_KEY.

Now let‚Äôs store your AWS secrets in GitHub.

In your GitHub repository

‚Ä¢ Go to Settings \-\> Secrets

‚Ä¢ Click Add a new secret.

‚Ä¢ Give it a name MY\_AWS\_ACCESS\_KEY

‚Ä¢ Paste the content of your key and click Add Secret.

‚Ä¢ Add another secret and give it the name MY\_AWS\_SECRET\_KEY

‚Ä¢ Paste the content of your key and click Add Secret.

Finally you should have something very similar to this:

Simple GitHub Actions for deploy Python Flask application to AWS

Ok, now as we already know the basics, let‚Äôs create a brand new workflow file that will create a deployment package of Python flask application from our repository code and will deploy it to AWS.

First we need to set our AWS credentials for authentication, and then

we will use aws cli commands to deploy our application to AWS ElasticBeanstalk environment.

Pre-requisites:

‚Ä¢ IAM User‚Äôs Access Key and Secret Key with permission to write and read to S3 and ElasticBeanstalk FullAccess.

‚Ä¢ S3 bucket ‚Äì where we will store our deployment packages.

‚Ä¢ ElasticBeanstalk environment to which we will upload and deploy our new package.

Here is a full CI/CD Pipeline for deployment of Flask application to AWS

We now have two Jobs:

1\. my\_ci\_part, which create deployment package and store our artifact in S3

2\. my\_cd\_part, which create new application version and deploy this version to AWS

As you can see we have to configure AWS credentials in each Job separately as each job executed on isolated runner, in the first job we need AWS credentials to copy our artifact to S3, and in the second job we need AWS credentials to create and deploy new version.

This example is universal, which you can use in your own environment, just replace Global Environment Variables values to match your setup.

Conclusion

GitHub Action is a pretty new feature that has recently been made generally available. This feature simplifies automation that you need to set up to manage deployment from specific repositories. Also it‚Äôs serverless you don‚Äôt need to set up extraneous services like Jenkins and you have 2000 free minutes each month. If your repository is public, you have unlimited running time.

A lot of new GitHub Actions that were developed by the open source community have already been published and featured at GitHub that you can readily deploy. At the same time you can create your own.

GitHub Actions is still very new, but it has a lot of potential. The above is a simple example, but it can be configured to something considerably more complex.

What are your CI/CD needs? How are you deploying your services into production automatically? We are always available to help you with your delivery needs. Plea with enterprise-grade DevOps solutionsse contact us at info@opsguru.com

# OpsGuru Awarded 2023 Canadian AWS Partner Award

## Introduction

OpsGuru is excited to announce it is a recipient of a 2023 Canadian AWS Partner Award, recognizing leaders playing a key role in helping customers drive innovation and build solutions on Amazon Web Services (AWS) in Canada.

Announced during the Toronto AWS Partners Summit, the Canadian AWS Partner Awards recognize a wide range of AWS Partners, whose business models have embraced specialization, innovation, and cooperation over the past year. The Canadian AWS Partner Awards recognize partners whose business models continue to evolve and thrive on AWS as they work with customers.

‚ÄúWe are once again incredibly proud of our achievement as the Canadian AWS Partner of the Year,‚Äù says John Witte, CEO, OpsGuru. ‚ÄúThis award reflects our focus to deliver on enabling companies to innovate and modernize, leveraging technologies such as data, AI, and software within AWS. We are dedicated to supporting Canadian organizations through their digital transformation ‚Äì at any stage of their cloud journey.‚Äù

‚ÄúOpsGuru has been a critical partner during our cloud adoption and transformation journey,‚Äù says Rob Fullerton, VP Product Development, Numeris. ‚ÄúThey have supported us through each stage of this evolution technically and, most critically, in terms of organizational transformation. It is with the help of their expert guidance that we have achieved a cloud-first strategy that enables us to innovate more effectively and efficiently than ever before.‚Äù

‚ÄúAWS Partners are at the center of unlocking greater business value for Canadian customers, across a wide range of industries,‚Äù said Eric Gales, AWS Canada Country Manager. ‚ÄúThe 2023 Canadian AWS Partner awards celebrate partners that share our customer obsession, providing a deep knowledge of AWS services to help customers scale their business, accelerate innovation and deepen digital transformation.‚Äù

The AWS Partner Network (APN), is a global program, focused on helping companies build successful AWS-based businesses or solutions by providing business, technical, marketing, and go-to-market support. The APN includes independent software vendors (ISVs) and systems integrators (SIs) around the world, with AWS Partner participation growing significantly during the past 12 months.

Contact us today to learn more about how OpsGuru can help you on your digital transformation journey.

For more information, please read our press release.

## 

‚Ä¢ 

# The State of Kubernetes in AWS: Persistent Data Storage, Application Engineering and More

## Introduction

When it comes to orchestrating containerized workloads, there are several options in the market, with Kubernetes being the most adopted and sought-after solution.

Here at OpsGuru, we‚Äôve achieved the AWS Service Delivery designation for Amazon Elastic Kubernetes Service (Amazon EKS), recognizing that we have proven success in helping customers architect, deploy, and operate containerized workloads on top of Amazon Kubernetes. We see tremendous opportunity within the Kubernetes ecosystem offered by AWS and we would like to share our real life experience with the community.

As we work with startups and ISVs across the globe, a regular pattern that we help match against is being able to focus their Kubernetes on AWS engineering efforts towards a single release and maintenance process for customers that have clients operating across multiple regions and also being able to operate in non-connected environments.

The choices span from Kubernetes on EC2 to fully managed Amazon EKS and EKS Anywhere (EKS-A, for non-connected environments) or even AWS Outposts.

To start your Kubernetes adoption journey, we ‚Äúwork backwards‚Äù by defining ‚ÄúWhy Kubernetes?‚Äù There are various reasons for a team to adopt it, among them the most common are:

‚Ä¢ Standardization: Containers allow applications of all kinds to have the same lifecycle, simplifying operational tasks.

‚Ä¢ Scalability: Kubernetes allows you to scale your workloads in a variety of ways like horizontal (replicas) and vertical (resource) scaling based on CPU, memory or custom metrics of any kind.

‚Ä¢ Attracting & Retaining Talent: Containerization and Kubernetes are technologies that technical talent looks forward to working with.

Knowing your reasons to adopt Kubernetes will help decide on a path to adoption. While Kubernetes solves a lot of problems, it is also a complex platform in itself and having a partner with deep Kubernetes knowledge by your side is crucial for implementation success. OpsGuru can help you in this journey on AWS.

In this two-part post, we will provide an overview of the current state of Kubernetes in AWS and how you can leverage open-source tools and AWS services in your Kubernetes journey.

## Persistent Data Storage, Ephemeral Workloads

Containers are touted as the perfect solution for ephemeral/stateless workloads, but Kubernetes can also help you handle stateful workloads, allowing you to run legacy applications too.

When needing any kind of storage, stateless workloads will most likely default to using memory or a small space in the host. On the other hand, stateful workloads will need persistent storage.

AWS provides a series of solutions that can be used with Kubernetes, like:

‚Ä¢ NFS shares can be handled by Amazon Elastic File System (EFS). EFS provides NFS endpoints with unlimited storage.

‚Ä¢ Persistent local storage is provided by Amazon Elastic Block Storage (EBS). These volumes are connected to the instance the container is running on, and if it moves to another host, the disk moves with it.

‚Ä¢ Temporary storage can be done in memory or using part of the host‚Äôs disk, but if you require faster storage, some AWS instance types have local SSD disks that can also be leveraged.

As long as there is a CSI driver for it, Kubernetes can work with your storage solution, by adopting managed options like the EBS and EFS you can reduce the operational burden on your team.

## Application Engineering / Data Engineering for Machine Learning

Kubernetes can help standardize your approaches to application and data engineering.

Technologies like Kubeflow, a machine learning toolkit, can help you focus on the development tasks while it leverages Kubernetes to handle all the operational ones.

Other tools like Helm can help you facilitate onboarding developers by abstracting the sea of YAML required by Kubernetes into a simple configuration file. If you have a small team of developers also handling operations, CDK8s (an AWS project, recently joined the CNCF) will allow you to use your language of choice to generate Kubernetes manifests for your applications.

These tools can help you rapidly scale the adoption of Kubernetes on your organization by lowering the learning curve.

## Operating Model, Operational Excellence and Site Reliability Engineering

Kubernetes, from the outside, has a high degree of operational complexity to contend with. As you peel back the layers, just like a normal operating model involving VMs, Kubernetes has a lot going on behind the scenes.

Thanks to the tremendous ecosystem of tools that are available, evidenced by the ever-growing CNCF Cloud Native Landscape, you can have all that you need to safely manage Kubernetes clusters, supported by AWS services.

The OpsGuru team has deep experience and expertise with peripheral cluster services, and we help our customers leverage the following solutions:

‚Ä¢ Prometheus is the standard for metrics collection in Kubernetes, and it is made even easier to manage with Amazon Managed Service for Prometheus.

‚Ä¢ Alertmanager, part of the Prometheus project, can route your alerts to a series of services, including Amazon Simple Notification Service, that in turn can integrate with a series of other AWS or third-party services.

‚Ä¢ Thanos is a highly available Prometheus setup, with long-term storage capabilities. Think of Prometheus with nearly infinite metric retention, all backed by Amazon S3.

‚Ä¢ Argo CD or Flux for GitOps style application deployment into the clusters, using git repositories, like AWS CodeCommit, as the source of truth for your cluster workloads.

‚Ä¢ Fluent Bit to ship all your logs to services like Cloudwatch Logs or ElasticSearch with Amazon OpenSearch Service

‚Ä¢ External DNS to automatically create Route 53 entries for your services

Like we said, there are as many tools as there are needs for them, and we could do an entire blog series on just this subject. AWS has several services that can facilitate running and managing Kubernetes clusters and their workloads, from provisioning to observing and managing them.

## Conclusion

AWS provides managed Kubernetes services in the cloud, which can be extended to on-premises environments with EKS Anywhere. With the wide support across cloud, hybrid and on-premises, Kubernetes is the perfect answer to portability and flexibility across vendors and geographies. In The Current State of Kubernetes on AWS ‚Äì Part 2 we will elaborate on Kubernetes security, scalability, cost-effectiveness, performance engineering and skills development.

OpsGuru has achieved the AWS Service Delivery designation for Amazon EKS, recognizing that OpsGuru has proven success in helping customers architect, deploy, and operate containerized workloads on top of Amazon Kubernetes. To learn more, read our press release.

Are you interested in working with leading-edge technologies? OpsGuru is always looking for highly skilled engineers and architects. Visit our careers page to find the role that‚Äôs right for you.

Interested to learn more? Check out part 2 of The Current State of Kubernetes on AWS: Kubernetes Security, Scalability, Performance Engineering & More

# The State of Kubernetes in AWS: Persistent Data Storage, Application Engineering and More

## Introduction

When it comes to orchestrating containerized workloads, there are several options in the market, with Kubernetes being the most adopted and sought-after solution and container orchestration,.

Here at OpsGuru, we‚Äôve achieved the AWS Service Delivery designation for Amazon Elastic Kubernetes Service (Amazon EKS), recognizing that we have proven success in helping customers architect, deploy, and operate containerized workloads on top of Amazon Kubernetes. We see tremendous opportunity within the Kubernetes ecosystem offered by AWS and we would like to share our real life experience with the community.

As we work with startups and ISVs across the globe, a regular pattern that we help match against is being able to focus their Kubernetes on AWS engineering efforts towards a single release and maintenance process for customers that have clients operating across multiple regions and also being able to operate in non-connected environments.

The choices span from Kubernetes on EC2 to fully managed Amazon EKS and EKS Anywhere (EKS-A, for non-connected environments) or even AWS Outposts.

To start your Kubernetes adoption journey, we ‚Äúwork backwards‚Äù by defining ‚ÄúWhy Kubernetes?‚Äù There are various reasons for a team to adopt it, among them the most common are:

‚Ä¢ Standardization: Containers allow applications of all kinds to have the same lifecycle, simplifying operational tasks.

‚Ä¢ Scalability: Kubernetes allows you to scale your workloads in a variety of ways like horizontal (replicas) and vertical (resource) scaling based on CPU, memory or custom metrics of any kind.

‚Ä¢ Attracting & Retaining Talent: Containerization and Kubernetes are technologies that technical talent looks forward to working with.

Knowing your reasons to adopt Kubernetes will help decide on a path to adoption. While Kubernetes solves a lot of problems, it is also a complex platform in itself and having a partner with deep Kubernetes knowledge by your side is crucial for implementation success. OpsGuru can help you in this journey on AWS with expert Kubernetes consulting and cloud-native infrastructure design.

In this two-part post, we will provide an overview of the current state of Kubernetes in AWS and how you can leverage open-source tools and AWS services in your Kubernetes journey.

## Persistent Data Storage, Ephemeral Workloads

Containers are touted as the perfect solution for ephemeral/stateless workloads, but Kubernetes can also help you handle stateful workloads, allowing you to run legacy applications too.

When needing any kind of storage, stateless workloads will most likely default to using memory or a small space in the host. On the other hand, stateful workloads will need persistent storage.

AWS provides a series of solutions that can be used with Kubernetes, like:

‚Ä¢ NFS shares can be handled by Amazon Elastic File System (EFS). EFS provides NFS endpoints with unlimited storage.

‚Ä¢ Persistent local storage is provided by Amazon Elastic Block Storage (EBS). These volumes are connected to the instance the container is running on, and if it moves to another host, the disk moves with it.

‚Ä¢ Temporary storage can be done in memory or using part of the host‚Äôs disk, but if you require faster storage, some AWS instance types have local SSD disks that can also be leveraged.

As long as there is a CSI driver for it, Kubernetes can work with your storage solution, by adopting managed options like the EBS and EFS you can reduce the operational burden on your team.

## Application Engineering / Data Engineering for Machine Learning

Kubernetes can help standardize your approaches to application and data engineering.

Technologies like Kubeflow, a machine learning toolkit, can help you focus on the development tasks while it leverages Kubernetes to handle all the operational ones.

Other tools like Helm can help you facilitate onboarding developers by abstracting the sea of YAML required by Kubernetes into a simple configuration file. If you have a small team of developers also handling operations, CDK8s (an AWS project, recently joined the CNCF) will allow you to use your language of choice to generate Kubernetes manifests for your applications.

These tools can help you rapidly scale the adoption of Kubernetes on your organization by lowering the learning curve.

## Operating Model, Operational Excellence and Site Reliability Engineering

Kubernetes, from the outside, has a high degree of operational complexity to contend with. As you peel back the layers, just like a normal operating model involving VMs, Kubernetes has a lot going on behind the scenes.

Thanks to the tremendous ecosystem of tools that are available, evidenced by the ever-growing CNCF Cloud Native Landscape, you can have all that you need to safely manage Kubernetes clusters, supported by AWS services.

The OpsGuru team has deep experience and expertise with peripheral cluster services, and we help our customers leverage the following solutions:

‚Ä¢ Prometheus is the standard for metrics collection in Kubernetes, and it is made even easier to manage with Amazon Managed Service for Prometheus.

‚Ä¢ Alertmanager, part of the Prometheus project, can route your alerts to a series of services, including Amazon Simple Notification Service, that in turn can integrate with a series of other AWS or third-party services.

‚Ä¢ Thanos is a highly available Prometheus setup, with long-term storage capabilities. Think of Prometheus with nearly infinite metric retention, all backed by Amazon S3.

‚Ä¢ Argo CD or Flux for GitOps style application deployment into the clusters, using git repositories, like AWS CodeCommit, as the source of truth for your cluster workloads.

‚Ä¢ Fluent Bit to ship all your logs to services like Cloudwatch Logs or ElasticSearch with Amazon OpenSearch Service

‚Ä¢ External DNS to automatically create Route 53 entries for your services

Like we said, there are as many tools as there are needs for them, and we could do an entire blog series on just this subject. AWS has several services that can facilitate running and managing Kubernetes clusters and their workloads, from provisioning to observing and managing them.

## Conclusion

AWS provides managed Kubernetes services in the cloud, which can be extended to on-premises environments with EKS Anywhere. With the wide support across cloud, hybrid and on-premises, Kubernetes is the perfect answer to portability and flexibility across vendors and geographies. In The Current State of Kubernetes on AWS ‚Äì Part 2 we will elaborate on Kubernetes security, scalability, cost-effectiveness, performance engineering and skills development.

OpsGuru has achieved the AWS Service Delivery designation for Amazon EKS, recognizing that OpsGuru has proven success in helping customers architect, deploy, and operate containerized workloads on top of Amazon Kubernetes. To learn more, read our press release.

Are you interested in working with leading-edge technologies? OpsGuru is always looking for highly skilled engineers and architects. Visit our careers page to find the role that‚Äôs right for you.

Interested to learn more? Check out part 2 of The Current State of Kubernetes on AWS: Kubernetes Security, Scalability, Performance Engineering & More

# The State of Kubernetes in AWS: Persistent Data Storage, Application Engineering and More

## Introduction

When it comes to orchestrating containerized workloads, there are several options in the market, with Kubernetes being the most adopted and sought-after solution.

Here at OpsGuru, we‚Äôve achieved the AWS Service Delivery designation for Amazon Elastic Kubernetes Service (Amazon EKS), recognizing that we have proven success in helping customers architect, deploy, and operate containerized workloads on top of Amazon Kubernetes. We see tremendous opportunity within the Kubernetes ecosystem offered by AWS and we would like to share our real life experience with the community.

As we work with startups and ISVs across the globe, a regular pattern that we help match against is being able to focus their Kubernetes on AWS engineering efforts towards a single release and maintenance process for customers that have clients operating across multiple regions and also being able to operate in non-connected environments.

The choices span from Kubernetes on EC2 to fully managed Amazon EKS and EKS Anywhere (EKS-A, for non-connected environments) or even AWS Outposts.

To start your Kubernetes adoption journey, we ‚Äúwork backwards‚Äù by defining ‚ÄúWhy Kubernetes?‚Äù There are various reasons for a team to adopt it, among them the most common are:

‚Ä¢ Standardization: Containers allow applications of all kinds to have the same lifecycle, simplifying operational tasks.

‚Ä¢ Scalability: Kubernetes allows you to scale your workloads in a variety of ways like horizontal (replicas) and vertical (resource) scaling based on CPU, memory or custom metrics of any kind.

‚Ä¢ Attracting & Retaining Talent: Containerization and Kubernetes are technologies that technical talent looks forward to working with.

Knowing your reasons to adopt Kubernetes will help decide on a path to adoption. While Kubernetes solves a lot of problems, it is also a complex platform in itself and having a partner with deep Kubernetes knowledge by your side is crucial for implementation success. OpsGuru can help you in this journey on AWS.

In this two-part post, we will provide an overview of the current state of Kubernetes in AWS and how you can leverage open-source tools and AWS services in your Kubernetes journey.

## Persistent Data Storage, Ephemeral Workloads

Containers are touted as the perfect solution for ephemeral/stateless workloads, but Kubernetes can also help you handle stateful workloads, allowing you to run legacy applications too.

When needing any kind of storage, stateless workloads will most likely default to using memory or a small space in the host. On the other hand, stateful workloads will need persistent storage.

AWS provides a series of solutions that can be used with Kubernetes, like:

‚Ä¢ NFS shares can be handled by Amazon Elastic File System (EFS). EFS provides NFS endpoints with unlimited storage.

‚Ä¢ Persistent local storage is provided by Amazon Elastic Block Storage (EBS). These volumes are connected to the instance the container is running on, and if it moves to another host, the disk moves with it.

‚Ä¢ Temporary storage can be done in memory or using part of the host‚Äôs disk, but if you require faster storage, some AWS instance types have local SSD disks that can also be leveraged.

As long as there is a CSI driver for it, Kubernetes can work with your storage solution, by adopting managed options like the EBS and EFS you can reduce the operational burden on your team.

## Application Engineering / Data Engineering for Machine Learning

Kubernetes can help standardize your approaches to application and data engineering.

Technologies like Kubeflow, a machine learning toolkit, can help you focus on the development tasks while it leverages Kubernetes to handle all the operational ones.

Other tools like Helm can help you facilitate onboarding developers by abstracting the sea of YAML required by Kubernetes into a simple configuration file. If you have a small team of developers also handling operations, CDK8s (an AWS project, recently joined the CNCF) will allow you to use your language of choice to generate Kubernetes manifests for your applications.

These tools can help you rapidly scale the adoption of Kubernetes on your organization by lowering the learning curve.

## Operating Model, Operational Excellence and Site Reliability Engineering

Kubernetes, from the outside, has a high degree of operational complexity to contend with. As you peel back the layers, just like a normal operating model involving VMs, Kubernetes has a lot going on behind the scenes.

Thanks to the tremendous ecosystem of tools that are available, evidenced by the ever-growing CNCF Cloud Native Landscape, you can have all that you need to safely manage Kubernetes clusters, supported by AWS services.

The OpsGuru team has deep experience and expertise with peripheral cluster services, and we help our customers leverage the following solutions:

‚Ä¢ Prometheus is the standard for metrics collection in Kubernetes, and it is made even easier to manage with Amazon Managed Service for Prometheus.

‚Ä¢ Alertmanager, part of the Prometheus project, can route your alerts to a series of services, including Amazon Simple Notification Service, that in turn can integrate with a series of other AWS or third-party services.

‚Ä¢ Thanos is a highly available Prometheus setup, with long-term storage capabilities. Think of Prometheus with nearly infinite metric retention, all backed by Amazon S3.

‚Ä¢ Argo CD or Flux for GitOps style application deployment into the clusters, using git repositories, like AWS CodeCommit, as the source of truth for your cluster workloads.

‚Ä¢ Fluent Bit to ship all your logs to services like Cloudwatch Logs or ElasticSearch with Amazon OpenSearch Service

‚Ä¢ External DNS to automatically create Route 53 entries for your services

Like we said, there are as many tools as there are needs for them, and we could do an entire blog series on just this subject. AWS has several services that can facilitate running and managing Kubernetes clusters and their workloads, from provisioning to observing and managing them.

## Conclusion

AWS provides managed Kubernetes services in the cloud, which can be extended to on-premises environments with EKS Anywhere. With the wide support across cloud, hybrid and on-premises, Kubernetes is the perfect answer to portability and flexibility across vendors and geographies. In The Current State of Kubernetes on AWS ‚Äì Part 2 we will elaborate on Kubernetes security, scalability, cost-effectiveness, performance engineering and skills development.

OpsGuru has achieved the AWS Service Delivery designation for Amazon EKS, recognizing that OpsGuru has proven success in helping customers architect, deploy, and operate containerized workloads on top of Amazon Kubernetes. To learn more, read our press release.

Are you interested in working with leading-edge technologies? OpsGuru is always looking for highly skilled engineers and architects. Visit our careers page to find the role that‚Äôs right for you.

Interested to learn more? Check out part 2 of The Current State of Kubernetes on AWS: Kubernetes Security, Scalability, Performance Engineering & More

# The Current State of Kubernetes on AWS: Kubernetes Security, Scalability, Performance Engineering & More, Part 2

## Introduction

In the first part of our two-part post on the current state of Kubernetes in AWS, we discussed how Kubernetes can help you handle stateful workloads with persistent data storage and standardize your application and data engineering approaches. We also shared how different AWS services can support Kubernetes cluster management.

In the second part of this post, we‚Äôre diving into topics like Kubernetes security, Kubernetes scalability, next-level cloud economy, performance engineering, and more to further unpack the current state of Kubernetes in AWS.

## Kubernetes Security

Security is a pervasive topic, it is present at every layer of the stack and AWS makes it easier all the way.

Either running on EKS, EKS-A or even on EC2, you can authenticate using the standard authentication service in AWS, IAM. Users and roles can authenticate against the cluster and then be mapped to a series of RBAC roles that will scope out their access level to the cluster.

If your workloads need access to AWS resources, IAM Roles for Service Accounts (IRSA) has your back, allowing you to create IAM roles that trust a Kubernetes service account, securely providing credentials to workloads.

You can consume secrets from the AWS SSM Parameter Store, AWS Secrets Manager directly from those services (see the previous paragraph on IRSA), or through integrations like External Secrets, or even mounting them as volumes using the Secrets Store CSI driver.

Network Policies can manage access to your workloads by labels and IP addresses, but if you need your workload to be part of a security group, you can attach one to your pods thanks to the AWS VPC CNI driver.

## Kubernetes Scalability

In order to leverage Kubernetes and scale to handle thousands or millions of customers for your application stack, autoscaling your workloads is a big requirement.

Like we briefly mentioned in the Working Backwards section, Kubernetes allows you to handle scaling of your workloads in different ways.

By default, a properly set up cluster will monitor metrics for each workload running, like memory and CPU utilization, which can then be leveraged for scaling your workloads.

Horizontal Pod Autoscaling (HPA), will handle scaling replicas of your workloads. For applications that can handle more traffic by simply running more copies, it‚Äôs the perfect fit.

Vertical Pod Autoscaling (VPA), will increase the resource reservations for a running workload, instead of increasing the amount of replicas, which is the perfect fit for applications that scale by having more resources available. This is usually the case with legacy applications.

Now CPU and Memory might not be the best way to scale your application, you might need more replicas according to the size of some message queue, or more resources according to the number of requests you‚Äôre getting. To scale based on custom metrics there are several projects available to add this functionality into Kubernetes, from which Keda is one of the most comprehensive, allowing you to scale based on a series of data sources.

## Cost-effective / Next-Level Cloud Economy

AWS Savings Plans and Reserved Instances continue to be extremely popular with the Kubernetes community. When leveraging these for Kubernetes, make sure to properly prepare your cluster‚Äôs node groups according to your workload‚Äôs needs, and also leverage Spot Instances as much as possible.

You should also have the AWS Node Termination Handler to help your nodes terminate gracefully whenever a disruption event occurs. Trust us, people cry without this üòÄ

## Performance Engineering

We often help customers through challenges related to performance and troubleshooting at scale. Observability is critical, so leveraging monitoring tools like Thanos and Prometheus are critical, if not ISVs that specialize in Kubernetes such as Fairwinds Insights.

Collecting metrics is easy, making sense of them requires insight into how all the Kubernetes components work and interact with each other and the AWS infrastructure. To help make sense of this sea of data we can use Grafana, or Amazon Managed Grafana, to visualize it through dashboards, bringing together data from Prometheus/Thanos, Cloudwatch and a series of other data sources.

## Skills Development

Last, but definitely not least, is the whole area of skills development.

Our recommended prerequisite at OpsGuru for Solutions Architects and Cloud Engineers who have an interest in learning more about Kubernetes and already have foundational or Associate / Professional certifications on AWS is to get hands-on experience with local distributions and set a path towards Linux Foundation certification.

If you‚Äôre already familiar with core cloud computing concepts, a great start is by setting a path towards achieving the recently released Kubernetes and Cloud Native Associat (KCNA) certification.

From there, you can select your path towards any of the 3 core Kubernetes certs, depending on your interest and degree of specialization.

‚Ä¢ Certified Kubernetes Administrator (CKA)

‚Ä¢ Certified Kubernetes Application Developer (CKAD)

‚Ä¢ Certified Kubernetes Security Specialist (CKS)

## Conclusion

AWS provides managed Kubernetes services in the cloud, which can be extended to on-premises environments with EKS Anywhere. With the wide support across cloud, hybrid and on-premise, Kubernetes is the perfect answer to portability and flexibility across vendors and geographies.

In conclusion, the Kubernetes on AWS ecosystem continues to grow on a daily basis, so keep learning and keep trying new things\! We encourage you to reach out if you have any questions, or are looking to accelerate your projects with Kubernetes enablement support.

Are you interested in working with leading-edge technologies? OpsGuru is always looking for highly skilled engineers and architects. Visit our careers page to find the role that‚Äôs right for you.

Interested to learn more? Check out part 1 of The Current State of Kubernetes on AWS: Kubernetes Security, Scalability, Performance Engineering & More

## Written by:

Fernando Battistella, Principal Architect at OpsGuru ‚Äì Fernando has over two decades of experience in IT, with the last six years architecting cloud-native solutions for companies of all sizes. Specialized in Kubernetes and the Cloud Native ecosystem, he has helped multiple organizations design, build, migrate, operate and train their teams in cloud-native technologies and platforms.

Bill Hunka, Account Executive at OpsGuru ‚Äì Bill has over 15 years of sales and business development experience supporting customers across Canada and the Western US. After spending 10 years at a Vancouver-based SaaS security leader, Bill pivoted to work with HPC and scale-out customers with focused industry vertical solutions. Over the past 5 years with his head in the ‚ÄúClouds‚Äù, he‚Äôs built his skills by diving deep with his customers‚Äô data-driven initiatives, helping to plan out migrations at scale, and driving DevOps-focused transformations.

# OpsGuru an AWS SaaS Competency Launch Partner

Today OpsGuru announces that we have achieved the Amazon Web Services (AWS) SaaS Competency. We are also honoured to be the only Canadian Launch Partner of the AWS SaaS CompetencyAchieving an AWS Competency designation is not an easy feat. For the past few months, OpsGuru has worked with AWS through a rigorous validation process. The detailed process required OpsGuru to prove our expertise in designing architectures and deployments for multi-tenant, secure and scalable SaaS platforms on AWS. The process also required customer validation and review, obsessing over details of our previous work with our customers to ensure quality.

One of the key differentiators of the AWS SaaS Competency is its focus on multi-tenancy. While the AWS Well-Architected Framework lays out clear guidance for establishing a strong foundation for any AWS workload, multi-tenancy adds a new dimension of complexity. The requirements for operational excellence, security, reliability, performance and cost optimization become that much more uncompromising as the expectations for customer experience on SaaS offerings are constantly being raised.

While OpsGuru prides itself on Big Data and Kubernetes expertise, we believe that such cloud-native technologies only show real value when the cloud foundation is strong and sound. As such, OpsGuru has invested heavily in the Cloud Launchpad, because we believe that a strong foundation is the only way to a secure, scalable, dependable and cost-effective SaaS offering.

Through the last few years, OpsGuru has worked with a number of independent software providers (ISV‚Äôs) to adopt, expand and optimize on AWS. As a result, we have developed deep insights into the exact questions ISVs should ask when deploying and optimizing a SaaS solution on AWS. This allows us to lead our customers to tackle the highest risks early in the design process by identifying performance bottlenecks, cost inefficiencies, operational gaps, and vulnerable security practices well before they could find their way into the product. Despite AWS providing almost infinite scaling potential, SaaS applications regularly push the boundaries of AWS and require deep thought to fortify and ensure platforms can meet growing peak demands. As COVID-19 continues to usher in a new economic landscape, SaaS has rapidly become the preferred software delivery mechanism. CIOs place deep trust in SaaS organizations to operate their entire stack and this trust requires SaaS vendors to target the highest levels of availability. Trust is indeed easy to lose and hard to regain.

To celebrate the achievement of the AWS SaaS competency, OpsGuru will be releasing a whitepaper distilling some key lessons learned entitled ‚ÄúBuilding a SaaS Offering on AWS‚Äù. This whitepaper will share a number of our best practices learned and highlight factors that your team should consider when delivering a multi-tenanted solution. The paper will be released next week, but you can secure one of the first copies by submitting your contact information here.

Interested in learning more about what the AWS SaaS Competency is? Are you curious about how OpsGuru can help your organization build a SaaS offering in tune with the drastically changing commercial environments and customer behaviour? Please reach out to us at info@opsguru.com.

# OpsGuru Achieves the AWS Resilience Competency

OpsGuru announced today that it has achieved the Amazon Web Services (AWS) Resilience Services Competency in the Core Resilience category. This specialization recognizes OpsGuru as an AWS Partner that provides validated solutions to help customers improve their critical systems availability and resilience posture using AWS Resilience Services. As each customer and their critical workloads have unique availability requirements, AWS Resilience Competency Partners provide tailored guidance and solutions to achieve the highest system uptime needs. Trust in AWS Resilience Competency Partners to deliver top-notch services that meet your organization‚Äôs resilience needs in today‚Äôs dynamic business landscape.

Complex systems are susceptible to a variety of failures, both small and large, throughout their lifespan, including code deployment issues, infrastructure problems, data and state failures, and natural disasters. As a result, organizations must plan for and expect system failures, and design their systems to withstand and recover from failures with minimal impact to end users. Remote teams, distributed systems, and frequent releases further highlight the need for increased resilience in today‚Äôs business environment.

Achieving the AWS Resilience Competency in the Core Resilience category differentiates OpsGuru as an AWS Partner that has demonstrated technical proficiency and proven customer success supporting customers‚Äô resilience goals. OpsGuru is equipped to handle resilience related application challenges, especially as expectations from customers shift towards an ‚Äòalways on, always available‚Äô mindset. It‚Äôs important for organizations to expect and plan for system failures, and design workloads to recover from failure in a way that minimally impacts their end users. Additionally, when onboarding critical workloads to the cloud, such as online banking, stock-trading, Enterprise Resource Planning (ERPs) or online sales platforms, higher uptime requirements have become a minimal requirement. AWS Resilience Competency Partner OpsGuru provides professional consulting and engineering services that are validated by AWS experts in the Core Resilience category. This standardized approach allows customers to achieve their resilience goals in the cloud with the expert assistance of AWS Resilience Competency Partners.

‚ÄúOpsGuru is thrilled to announce our achievement of the AWS Resilience Competency,‚Äù declared CTO Robin Percy. ‚ÄúWith the imminent launch of AWS‚Äôs new region in Calgary, we‚Äôre uniquely positioned to accelerate cloud resilience across Canada‚Äôs business landscape. Companies seeking best-of-breed resilient solutions can leverage our validated expertise and services for unparalleled results.‚Äù

In addition to the AWS Resilience Services Competency, OpsGuru also holds the AWS DevOps Competency, the AWS Migration Competency, the AWS SaaS Competency, Microsoft Workloads Competency and the AWS Networking Competency.

AWS is enabling scalable, flexible, and cost-effective solutions from startups to global enterprises. To support the seamless integration and deployment of these solutions, the AWS Competency Program helps customers identify AWS Partners with deep industry experience and expertise.

# OpsGuru Expands Capabilities to Help Canadian Companies Accelerate Digital Transformation Through Strategic Collaboration Agreement With AWS

OpsGuru and a leading Canadian cloud consulting organization, today announced it has signed an expanded multi-year strategic collaboration agreement (SCA) with Amazon Web Services (AWS). This multi-year agreement builds on OpsGuru‚Äôs existing AWS expertise and further allows the organization to accelerate digital transformation initiatives for the benefit of Canadian companies.

‚ÄúThis expanded agreement is a natural next step for OpsGuru‚Äôs long-standing collaboration with AWS,‚Äù says John Witte, President & CEO of Carbon60. ‚ÄúFor years, OpsGuru and AWS have collaboratively empowered Canadian companies to navigate their cloud transformation with impactful results. With this agreement in place, OpsGuru is excited to deliver even more AWS customer-centric solutions to existing and new customers, tailored to support their unique business goals.‚Äù

As an AWS Premier Tier Service Partner in the AWS Partner Network (APN), OpsGuru is continuously adding to its key AWS competencies and holds five AWS competencies including AWS Migration Consulting Competency, AWS SaaS Consulting Competency, AWS DevOps Consulting Competency, AWS Networking Consulting Competency, and AWS Microsoft Workloads Consulting Competency. OpsGuru‚Äôs team maps their deep technical expertise with solutions and leadership in the cloud computing consulting industry.

‚ÄúWith OpsGuru‚Äôs leadership, we have created a resilient, scalable and secure compute platform that can support diverse workloads ranging from finance reporting, resource planning and operational management,‚Äù says Miller Dussan, Vice President, Technology at Equinox Gold. ‚ÄúEquinox Gold is now on the accelerated path to support data-driven decision making, maximize efficiency in daily operations, and holistically leverage cloud-based technologies to drive business transformations.‚Äù

‚ÄúOpsGuru is a trusted member of the AWS Partner Network and we are pleased to expand our relationship through this strategic collaboration,‚Äù says Eric Gales, Canada Country Manager at AWS. ‚ÄúAWS and OpsGuru share a commitment to empower Canadian companies in their cloud strategies and digital transformation journeys. By expanding our agreement, we are giving customers a broader range of capabilities and offerings to accelerate their transitions to the cloud and drive innovation in Canada.‚Äù

Contact us today to learn more about how OpsGuru can help you on your digital transformation journey.

For more information, please read our press release.

# Building the Business Case for AWS Migration

To boost innovation, respond quickly to changing demands, and drive business transformation, organizations are migrating and modernizing their infrastructures and applications on Amazon Web Services (AWS). Successful migrations take planning and expertise, as well as an understanding of the challenges you‚Äôre likely to face as part of the process.

While a successful migration starts with organizational alignment, having a partner by your side to support you through the process is vital. Based on its experience with multiple successful migrations, AWS 2021 Canada Consulting Partner of the Year OpsGuru has identified five areas in which alignment is critical to migration success, along with the challenges and potential pitfalls of each.

Get started with your migration journey today\! Download the AWS & OpsGuru: Building the Business Case For AWS Migration eBook to learn how AWS and OpsGuru can help accelerate and streamline your migration so you achieve results faster.

# A Leading Engagement Platform‚Äôs Journey to the Cloud: Modernizing for Scalability and Efficiency

## Company Background

This purpose-driven engagement platform provides solutions that power B2B social recognition, loyalty, rewards, and incentive programs. Its key offerings include a customizable engagement solution for enterprises and SMBs, a platform providing global access to millions of global curated rewards, and a free shop-and-save program for employers. The company is committed to sustainability through its mission to plant millions of trees annually to combat climate change.

## The Challenge

The company‚Äôs existing on-premises e-commerce platform faced significant scalability, performance, and cost-efficiency challenges. To support future growth, reduce latency, and optimize Total Cost of Ownership (TCO), the company sought expert guidance to migrate its workloads to Amazon Web Services (AWS). Ensuring security reliability and building a compelling business case for the migration were critical priorities for leadership.

## Our Solution

OpsGuru collaborated closely with the company to develop a secure and scalable AWS foundation using the Cloud Launchpad framework. This included implementing essential security guardrails such as temporary credentials, centralized login, network segregation, and a centralized security escrow account for audit logging.

The solution leveraged Amazon Elastic Kubernetes Service (EKS) for high availability and scalability, supported by managed services like Amazon RDS for MySQL, Amazon ElastiCache for Memcached and Redis, and Amazon Elastic File System (EFS). OpsGuru‚Äôs team also provided Terraform code to automate and implement the architecture, ensuring compliance with the AWS Well-Architected Framework.

## The Result

OpsGuru delivered a robust and modernized AWS platform tailored to the company‚Äôs specific needs. The migration reduced operational complexities, improved scalability, and lowered latency, enabling the company to achieve more significant growth and operational efficiency. The company now operates on a reliable cloud foundation that supports its business objectives and sustainability initiatives.

# Cloud Migration in the Public Sector: 6 Key Steps to Success

## Introduction

Whether in government, education, non-profit, or healthcare, public sector organizations face numerous challenges. On a daily basis, authorities and organizations within the Public Sector set out to accomplish disruptive projects despite budgetary restrictions and significant governance, security, and compliance requirements. As an organization that‚Äôs helped handle these challenges head-on, OpsGuru understands the undeniable advantages of the cloud can be realized.

The decision to commit your public sector organization to the cloud is a big move but can be successfully achieved under the guidance of the right partnership. Working with AWS and OpsGuru to map out the reason behind your cloud adoption, you can create a shortcut to success as other companies have.

Download the ebook to learn the 6 steps to realize a successful cloud migration and bring your public sector organization to the next level.

# Conquer 2025 with AI: A Practical Guide for Business Leaders

## Introduction

Generative AI has exploded onto the scene, promising to revolutionize business. But hype aside, 2025 is the year for practical AI adoption. Are you ready to move beyond the ‚Äúwow‚Äù factor and unlock real value? This guide provides a strategic roadmap for effectively leveraging AI.

Identify High-Impact AI Opportunities:

Don‚Äôt chase trends. Focus on solving your most pressing business challenges. AI excels at:

‚Ä¢ Automation: Streamlining repetitive tasks for increased efficiency.

‚Ä¢ Data Analysis: Extracting actionable insights from massive datasets.

‚Ä¢ Pattern Recognition: Uncovering hidden relationships to drive smarter decisions.

Ask yourself:

‚Ä¢ What data do we use most?

‚Ä¢ What insights are we currently extracting?

‚Ä¢ Where are our process bottlenecks?

Focusing on these questions will help you identify the highest-impact AI opportunities. For example, we helped Alida, a global experience management leader, reduce response analysis time from days to hours using AI-powered natural language processing. This ‚Äúquantum leap‚Äù in sentiment analysis gave Alida real-time insights for improved customer satisfaction. Similarly, we helped MLSE boost fan engagement by creating an AI-driven platform for personalized storytelling around league stats.

Build a Robust Data Foundation for AI Success:

While Large Language Models (LLMs) offer a low barrier to entry, sustainable AI success hinges on your data. Proprietary data is the foundation of truth and context for your AI initiatives. It‚Äôs what makes your AI models relevant, accurate, and aligned with your business needs. This is especially true as you move beyond general-purpose LLM chatbots and begin fine-tuning models and customizing them for specific business functions.

Effective data management is crucial. Start by:

‚Ä¢ Inventorying Data Sources: Identify all databases, warehouses, and storage locations.

‚Ä¢ Prioritizing Data: Focus on the most frequently used, largest, and most recent datasets.

Secure the Future of AI: Responsible Development & Deployment:

AI‚Äôs potential is immense, but responsible implementation is paramount. Focus on these key areas:

‚Ä¢ Data Security: Integrate security at every stage of the AI lifecycle. Address:

‚Ä¢ Model Operations & Output Monitoring: Prioritize model quality over cost-cutting. Implement:

‚Ä¢ Data sensitivity (PII, PHI, etc.)

‚Ä¢ Access controls

‚Ä¢ Security measures (encryption, intrusion detection)

‚Ä¢ Regulatory compliance (GDPR, CCPA, HIPAA)

‚Ä¢ Rigorous testing (unit, integration, end-to-end)

‚Ä¢ Relevant evaluation metrics (F1-score, ROUGE, BLEU)

‚Ä¢ MLOps and LLMOps: DevOps tailored to machine learning and AI, providing audit trails, access control, and data lineage for anomaly detection and drift prevention and managing the end-to-end workflows.

‚Ä¢ Continuous monitoring: Track model performance in production to identify and address issues like bias, drift, or unexpected outputs.

Success with AI requires a strategic approach: identify high-impact use cases, build a robust data foundation, and prioritize data security and model quality. Continuous learning and a commitment to responsible AI are essential for navigating this dynamic landscape and achieving sustainable success. Contact OpsGuru today to learn how we can help you unlock the power of AI in 2025\.

Ready to make measurable business impacts using AI? Contact us to get started.

# Bosa Development Quickly Gains Benefits of the Cloud with ‚ÄúLift-and-Shift First‚Äù Migration Strategy

## Introduction

‚Ä¢ Customer Success

January 6, 2025

## Background

Bosa Development has been a leader in the real estate industry in Western Canada and the United States for over four decades. With headquarters in Vancouver, British Columbia, Bosa Development combines functionality and aesthetic excellence to develop dynamic, sustainable, mixed-used environments that energize neighbourhoods and inspire their citizens.

When Bosa Development, decided it was time to consolidate and upgrade their legacy IT infrastructure, they turned to Amazon Web Services (AWS). Together with AWS Premier Partner OpsGuru, they decided on a ‚Äúlift-and-shift first‚Äù migration approach. The OpsGuru team leveraged AWS Application Migration Service (CloudEndure Migration) to rapidly re-host Bosa Development‚Äôs Microsoft File Server workloads into Amazon EC2 instances. Then, once the data was fully available on AWS, the team utilized Amazon FSx to host the consolidated data. This highly effective method of initial workload migration followed by modernization enabled Bosa Development to start benefiting from improved service availability, resilience, scalability, and security within just a few months.

## The Challenge

Bosa Development‚Äôs legacy IT infrastructure, which was distributed across three geographic locations, could no longer keep up with the growing demands of its rapidly expanding business. As such, Bosa Development decided to migrate its IT infrastructure services to the AWS Cloud for its elasticity, scalability, security, and reliability. Maurice Lui, IT manager at Bosa Development, spearheaded the cloud migration project. Because he did not have the extended team required for a complex migration, Lui reached out to a Vancouver-based AWS Premier Partner: OpsGuru.

## Our Solution

In order to best meet their business needs, Bosa Development decided to use a ‚Äúlift-and-shift first‚Äù migration strategy for their Windows File Server workloads: initially rehosting the workloads from on-premises VMware VMs into Amazon EC2 instances, and then refactoring the platform, leveraging an AWS managed service solution, such as Amazon FSx, to modernize the workloads.

Bosa Development in collaboration with OpsGuru chose AWS Application Migration Service as their lift-and-shift solution because they wanted a highly automated block-level replication tool that would enable them to get data quickly and efficiently into AWS. AWS Application Migration Service automatically converts source servers from physical, virtual, or cloud infrastructure to run natively on AWS, minimizing time-intensive, error-prone, high-risk manual processes.

After conducting an initial pilot to validate and de-risk the overall migration process, OpsGuru and Bosa Development embarked on the migration. The process involved installing AWS Application Migration Service agents on Bosa Development‚Äôs source servers, which initiated the replication process into AWS. Lui was able to clearly track the replication progress of each machine on a user-friendly dashboard. The entire replication process took a few weeks to complete for each physical site. According to Lui, ‚Äúthere was no performance impact or service disruption during the entire sync process,‚Äù which takes place in the background.

Paul Podolny, co-founder and principal consultant at OpsGuru, explained, ‚ÄúDespite several challenges such an unstable network uplink with frequent network disruptions, the team was able to successfully migrate the data to AWS thanks to the reliability of AWS Application Migration Service.‚Äù

Once the replication was complete, OpsGuru and Bosa Development used AWS Application Migration Service to conduct non-disruptive tests and verify that all of the workloads operated as expected on AWS.

The team then conducted the final cutover over the weekend in order to minimize any business disruption. According to Lui, there was virtually no impact on end-users.

After the Windows File Server workloads were running on Amazon EC2 instances, Podolny and his team refactored them with Amazon FSx. ‚ÄúWe did the lift and shift first in order to get the data into AWS quickly and reliably,‚Äù reported Podolny. ‚ÄúThen we leveraged Amazon FSx, which is highly scalable and available, and has advanced capabilities for snapshots, audits, and integrations with the AWS ecosystem.‚Äù

## The Result

Lui and his team at Bosa Development were pleased that their first cloud project proceeded smoothly and took less time than expected. Initially, the Bosa Development team had presumed that the migration process could take up to one year.

In the end, OpsGuru was able to migrate and refactor the workloads from each of Bosa Development‚Äôs three physical sites within just six weeks. This included not only replication but testing and verification as well.

The timing couldn‚Äôt have been better. As Lui pointed out, ‚ÄúSince migrating our File Server workloads to AWS, there is a significantly better and more consistent user experience. It doesn‚Äôt matter where you are accessing the files from.‚Äù This was especially important this past year with so many people working remotely due to COVID-19 regulations.

With a more reliable and scalable IT infrastructure, Lui can now stop spending so much of his time troubleshooting on-premise outages. Instead, he can support his company by focusing on digital innovation and optimization.

As a result of this migration project, Bosa Development has a more resilient, available, scalable, and secure IT infrastructure ‚Äì an infrastructure that finally meets its dynamic business needs.

## Conclusion

‚Ä¢ Customer Success

# Deep Learning-based SaaS Enablement on Google Cloud

## Introduction

‚Ä¢ Customer Success

January 6, 2025

## Background

Founded with the aim of simplifying vehicle inspection, Click-Ins introduces AI-driven automated technology that completely redefines its category. Helping insurance and car companies transition from manual procedures to fast and efficient fact-based processes, Click-Ins provides a user experience that is both simpler and more reliable, for all parties involved. To achieve the highest level of transparency and certainty, Click-Ins has developed a hybrid approach to AI.

Using proprietary simulated data to pre-train AI models, and leveraging multidisciplinary technologies, Click-Ins SaaS cloud solution accurately and consistently recognizes and measures any damage and accurately identifies the affected parts, with no training period. With its highly skilled team of technology, automotive, insurance, and business experts, Click-Ins is headquartered in Israel, with offices and partners in the USA, LatAm, and Europe, providing 24/7 service to customers worldwide.

## The Challenge

Click-Ins has already adopted Google Cloud as a preferred cloud provider, however, their environments have been configured in a traditional, non-cloud-native way with the heavy utilization of manually managed VMs that served their SaaS to the customers. This approach prevented the company from scaling to meet the growing business needs as it required a lot of engineering effort. Seeking to optimize their system, the client enlisted the expertise of the OpsGuru team to redesign their solution from the ground up, enabling their SaaS to leverage the full benefits of modern cloud deployments, including elasticity, resilience, and scalability.

## Our Solution

To gain a better understanding of the highly intricate SaaS architecture, OpsGuru has conducted multiple discovery sessions. The SaaS architecture comprises multiple microservices implemented through gRPC, which require very specific GPU resources to publish, optimize and serve Deep Learning models. After carefully examining the SaaS architecture, the joint OpsGuru, and Click-Ins‚Äô engineering team agreed on the necessary changes to optimize the system. The approach involved the utilization of a state-of-the-art container orchestrator ‚Äì Kubernetes via fully managed Google Kubernetes Engine (GKE) service with the utilization of multiple node groups that provided performant and cost-optimized computing needed for the compute-heavy workloads. OpsGuru first deployed the Cloud foundation through the Cloud Launchpad (CLP) to achieve the above, enabling Click-Ins to have a secure, reliable, standardized cloud baseline. With the CLP deployed, OpsGuru proceeded to configure the Kubernetes cluster (GKE), and other relevant resources that will be used to orchestrate all of the SaaS components.

Once the Kubernetes cluster was deployed, the team started an optimization of the Deep Learning model workflows. The optimized architecture utilizes Kubernetes, Helm, ArgoCD, Argo Workflows, and multiple cloud-native services managed by Google Cloud to solve all identified issues with the legacy configuration. This allowed for a fully automated model lifecycle, eliminating the need for a semi-manual approach that was prone to human errors and required downtime for the entire platform.

Besides the Deep Learning related components, OpsGuru has also implemented CI/CD for Click-Ins microservices and web components of the SaaS including Rest API, Frontend, and Portal. These components were also deployed within the Kubernetes cluster, utilizing GKE, GCR, and other Google Cloud managed services. All the components utilize OpenTelemetry for complete observability of the platform.

## The Result

Click-Ins was able to seamlessly scale to meet any load required for future business expansion while ensuring cost-optimized, performant, and reliable architecture is utilized.

‚Ä¢ Secure Cloud Baseline: Deployment of the Cloud Launchpad ensured Click-Ins would have a rock-solid baseline in the Google Cloud that can be used to run any workloads on the cloud.

‚Ä¢ Fully Automated Deep Learning Workflows: Deep Learning workflows are fully automated, ensuring rapid time-to-market model delivery while keeping security, performance, and high availability on the supreme levels

‚Ä¢ Container Orchestration: All the workloads are currently orchestrated through Kubernetes, allowing Click-Ins to effectively develop, maintain, and scale for any future demands.

‚Ä¢ CI/CD: Implementation of the Continuous Integration and Deployment (CI/CD) pipelines allowed Click-Ins to fully automate the management of application code (e.g. API) all the way to Deep Learning models, providing accelerated integration and deployment of the latest artifacts

Upon completion of the project, OpsGuru provided comprehensive training sessions for the Click-Ins team as well as documentation and operational playbooks for the newly designed systems. The training and documentation included the operational guidance of the newly built environment, Google Cloud configuration, and deployment, among other topics.

# Cocoflo Drives the SMART City Movement Forward with the Deployment of AWS Microservices

## Background

Cocoflo is a fast-growing Canadian tech company at the forefront of the SMART CITY movement. They were founded with a vision to help strengthen communities by improving the citizen and municipal administrative experience with a platform that connects community stakeholders through dynamic digital engagement. With one unified platform, their goal is to help the community achieve an effective collective communication flow (co-co-flo) ‚Äì they are your VIRTUAL CITY HALL.

## The Challenge

Security and data privacy is critical for any organization, but for Cocoflo who provides dynamic two-way communication between municipalities and citizens, it is imperative that the best security and governance practices are in place to protect this highly confidential data.

Cocoflo was looking for an Amazon Web Services (AWS) partner to help them establish an AWS cloud best practices foundation, as well as enable a robust and reliable microservices deployment and orchestration framework, leveraging Amazon Elastic Kubernetes Service (EKS). Amazon EKS is a managed container service used to run and scale Kubernetes applications in the cloud or on-premises.

As Cocoflo continues to grow and onboard new customers, their AWS cloud platform is critical in helping them become more agile and innovate faster. Cocoflo‚Äôs customers have varying data isolation requirements, and Amazon EKS provides Cocoflo with the ability to easily meet differing security and segregation requirements of any municipality they onboard.

## Our Solution

At OpsGuru, accelerating cloud-native adoption is the driving force of the company. Cocoflo leveraged OpsGuru‚Äôs Cloud Launchpad service to build a scalable cloud foundation, establishing the necessary AWS security and governance best practices foundations. Cloud Launchpad‚Äôs multi-faceted approach helps businesses move to the cloud rapidly and securely, reducing common errors companies experience when establishing their cloud foundations. ‚ÄúWe have spent years developing and testing our proprietary cloud adoption solutions and provide not only a technical solution, but programs to steadily build client technical proficiency,‚Äù Dave Lindon, General Manager at OpsGuru says.

The Amazon EKS-based solution designed by OpsGuru provides Cocoflo with the ability to rapidly deploy isolated versions of their single-tenant application in an automated and repeatable fashion using industry-standard Helm releases and Flux, the popular third-party tool for managing GitOps workflows. The solution limits redundant infrastructure and gives the company an efficient way to scale while keeping costs down. Additionally, knowledge transfer sessions led by OpsGuru ensured the Cocoflo team have the critical information needed to manage their cloud environment efficiently and independently. As part of this service, OpsGuru produced a set of operational playbooks and documentation covering AWS best practices, cloud design, and cloud operations.

## The Result

The containerization of Cocoflo‚Äôs AWS environment moves the company one step closer to executing its goal of delivering community-centric solutions to municipalities. The team met its key business goals of successfully scaling with growing user demand ‚Äì rapidly and securely onboarding AWS while adhering to AWS Well-Architected Framework best practices.

‚ÄúAs Cocoflo continues our rapid growth trajectory, it is essential that we align with partners that are not only best in class, but that share our vision for a better future. Our partnership with AWS & OpsGuru is pivotal to Cocoflo fulfilling our growth objectives. We look forward to continuing to work with AWS and Opsguru. Together we are championing SMART CITY evolution and making Cocoflo‚Äôs Virtual City Hall an industry standard,‚Äù said Bernie Florido, CEO and Founder at Cocoflo.

The steep learning curve and rapid evolution makes Kubernetes adoption challenging, even for experienced teams. In making the decision to partner with OpsGuru and leverage Cloud Launchpad, Cocoflo eliminated the complexities inherent in cloud adoption and tapped into a set of best practices that will see them through the exciting times ahead as they scale the heights of the SMART CITY movement.

# Edsembli Accelerates Modernization and Business Growth with OpsGuru

## Introduction

‚Ä¢ Customer Success

January 6, 2025

## Background

For software companies, business growth rides on innovation. This is why Edsembli, a software company based in Toronto, adopted a mandate to grow, modernize, and innovate. Specifically, it wanted to modernize its cornerstone application, Edsembli Ecosystem, which helps K-12 schools across Canada manage HR, payroll, and finances. Users include administrators, teachers, and more than 1 million students.

To round out its solution, Edsembli acquired a Student Information System (SIS). The plan was to integrate the SIS into its infrastructure, which was co-located at a telecommunications company, but unexpectedly, in 2021, Edsembli had to accelerate its plans to meet the needs of its customers as costs were escalating, and with waning confidence in its provider‚Äôs ability to align with its growth and modernization roadmap. Clearly, the company had some homework to do to align with its vision to grow, modernize, and innovate. And with only about 50 employees, it needed a partner to help.

## The Challenge

### Collaborating for a Fast Migration

Edsembli‚Äôs vision for modernization was to optimize cost and performance, have better control over scaling, and adopt DevOps to automate software development. Edsembli wanted more than just a cloud provider. The company needed a partner who could help it migrate to a new platform and modernize its product. Frank Ferlaino, Senior Director, Client Management Services, Edsembli, says, ‚ÄúWe were looking for a partner that could walk us through and educate us on the process ‚Äî a team with the skills and ability to navigate us through this change and collaborate with us as a partner in delivering to our customers.‚Äù

Following a comprehensive evaluation of cloud platforms, Edsembli selected AWS as the vendor with the best combination of tools and support, and OpsGuru as its migration partner.

## Our Solution

### MAP Assess

The OpsGuru team guided Edsembli through the AWS Migration Acceleration Program (MAP), which begins with the Assess Phase, a deep dive review of Edsembli‚Äôs current technology and workloads to build a business case for cloud adoption and create a high-level migration plan geared toward reducing risk in the migration process.

The Assess Phase also helps organizations identify gaps using the six dimensions of the AWS Cloud Adoption Framework: business, process, people, platform, operations, and security. OpsGuru helps organizations identify the capabilities required in the migration, along with the direct and indirect costs of migrating and using AWS. This helps customers get an accurate estimate of the costs of cloud computing and builds a TCO model for it.

Ferlaino says the analysis was crucial in helping Edsembli understand how the migration would strengthen its security policies, segment operational disciplines, and ensure that the company met best practices for the cloud. He also says that OpsGuru helped Edsembli identify skills gaps that, when filled, would help it achieve its DevOps and automation objectives.

## MAP Mobilize

With a plan in place to support its migration, Edsembli embarked on the Mobilize Phase of the AWS MAP to build foundational cloud capabilities, establish governance and operating models, create a landing zone, and gain experience through migrating pilot workloads to the AWS cloud.

For Edsembli, the goal was acceleration ‚Äî getting the company in the cloud as quickly as possible. Edsembli chose a rehost migration strategy, which involves moving an application to the cloud without making any changes, to control risk and take immediate advantage of cloud capabilities such as the flexibility to scale up or down as needed. The project team utilized OpsGuru‚Äôs Cloud Launchpad to bootstrap an AWS landing zone that‚Äôs fully managed by Terraform, providing Edsembli with best-practice infrastructure as code (IaC) as a baseline for future code development, rapidly upskilling its team.

Before going live, Edsembli beta-tested its product with customers and surfaced some performance issues. Thankfully, AWS tools came to the rescue. The company was able to easily increase the throughput for its storage, which helped it manage spikes in traffic, something it couldn‚Äôt do with its former cloud provider. ‚ÄúThat was especially important in going live because we didn‚Äôt know what to expect from a 100% production load,‚Äù says Ferlaino.

With guidance from OpsGuru, Edsembli was able to migrate all workloads to AWS, not just the pilot identified in the Assess Phase. This enabled Edsembli to completely decommission its data centre at the conclusion of the Mobilize Phase. ‚ÄúWe achieved our goals in a record time,‚Äù says Ferlaino. ‚ÄúThe migration was very smooth, pragmatic, and well formulated because we identified everything up front in the Assess Phase, and with the help of the OpsGuru engagement team, we stayed on track.‚Äù

## A Clear Path Forward for Modernization

Edsembli‚Äôs modernization strategy included optimizing for cost and performance, expanding scalability, and automating software development using DevOps practices. Edsembli recognized that OpsGuru was also the partner that would support its growth, modernization, and innovation mandate, and entered a Clear Path Forward engagement to conduct an assessment focused on application performance.

The assessment surfaced several issues, including bottlenecks and performance issues around its database, where components were out of support with third-party vendors. The assessment also showed Edsembli a clear path forward to modernize its monolithic application to take advantage of microservices, where the company can selectively scale pieces of the application to give its customers better performance.

The result of OpsGuru‚Äôs Clear Path Forward assessment was a customized roadmap and a conceptual solution architecture to provide Edsembli with a starting point for long-term modernization and a cloud-native solution architecture. ‚ÄúThe assessment was an opportunity to identify what our tech stack gaps were and where we can improve efficiencies,‚Äù says Ferlaino.

‚ÄúIt gave us a path to incrementally modernize,‚Äù says Ferlaino. ‚ÄúIt‚Äôs not a ‚Äòbig bang theory,‚Äô where we do everything at once. We can grow into it one module at a time and then add to it. That way we can gradually manifest our future as opposed to taking on a lot of risk.‚Äù

## Incremental Modernization

The final phase of MAP, Migrate & Modernize, driven by the Clear Path Forward assessment, enabled Edsembli to adopt additional AWS services such as its API Gateway and database server tools. OpsGuru helped Edsembli start the process of modernizing its application in the cloud by transforming its monolithic code into microservices.

OpsGuru Director of Engineering Jonathan Coe, who led the modernization phase, says, ‚ÄúCare should be taken to find the right balance between modernizing and accelerating cloud adoption.‚Äù Ferlaino says, ‚ÄúI admire the MAP process, and I admire OpsGuru‚Äôs effort to ensure no cost or schedule overruns. The migration was very smooth, pragmatic, and well formulated.‚Äù

## The Result

### A Smooth Migration in Record Time

Edsembli is still working with OpsGuru through the execution of its Clear Path Forward modernization strategy, which is helping the company build confidence with its customers that it has a roadmap for the future and a list of items to address moving forward.

Edsembli teams are also taking the ball and running with the next step, which is application development. The company is building out a proof of concept for new attendance modules using microservices and the AWS API Gateway. Ferlaino says, ‚ÄúThat will be proof that we can do it elsewhere and lead us into growing and evolving our product.‚Äù And, he says, it‚Äôs also helping Edsembli clearly align with its mandate to grow, modernize, and innovate.

The company has also adopted DevOps for agility. ‚ÄúDevOps practices help us manage our clusters and availability in a more pragmatic, automated way so that we can focus our resources on things that are more important to the business,‚Äù says Ferlaino. Most importantly, Edsembli can now focus on business growth by incrementally evolving its products, which Ferlaino says, ‚Äúgives us a bigger bang, more opportunities to grow, and a framework to start planning for the future ‚Äî and we have a lot of growth strategies moving forward,‚Äù including plans to expand into the United States.

# Pandemic Proof Food Delivery with FoodX

## Introduction

‚Ä¢ Customer Success

January 6, 2025

## Background

FoodX provides scalable end-to-end grocery software providing online delivery optimization solutions for grocers. Their platform evolved from a local and organic online grocery company to an award-winning global eGrocery Management Solution. FoodX technology optimizes the fulfilment and delivery management, enabling grocers to profitably deliver fresh food from farm to fridge with less overall waste.

## The Challenge

In March, as millions of Canadians stayed home and braced for the impacts of COVID-19, the volume of online orders for groceries spiked significantly.

The sudden increase in demand was difficult to meet, and current methods of fulfilling these orders were no longer sufficient.

Retailers were looking for ways to develop and expand their businesses to satisfy web-based grocery demand. FoodX had a business model that could answer the challenge. Still, at the same time, FoodX was looking for a technology partner that had strong DevOps capabilities in Microsoft Azure to help them scale and optimize their data systems.

## Our Solution

Because the scalability of the data platform was critical to the rapid growth of the eGrocery management system, OpsGuru conducted as the first step of the project an in-depth discovery with the relevant stakeholders within the organization, to understand the existing end-to-end workflow.

This included a full survey of the data types, ingestion mechanisms, stream and batch processing, data lifecycle, workflow orchestration, data catalogue, data stores, service level availability and objectives.

By comparing against the existing workflows and technologies and the desired system that could handle rapidly increasing demand, OpsGuru discerned with FoodX a full scope of requirements and the optimal approach to building a reliable and scalable data platform.

The OpsGuru team introduced a data platform using the Kappa Architecture pattern to streamline, generate and normalize business events from different operational systems as well as third-party vendors. This architecture resulted in a single source of truth based on a framework where other event sources can be added in future to support the continuous expansion of FoodX.

The solution designed and implemented by OpsGuru embeds in it the pillars of the Microsoft Azure Well-Architected Framework, such that it is secure, resilient to disruptions and able to service rapidly growing demand.

## The Result

This solution allowed FoodX retailers to operate dedicated online grocery fulfilment facilities at scale effectively and will be available to Canadian and international retailers via a SaaS model.

As a result of the successful collaboration, FoodX continues to partner with OpsGuru to deliver more eGrocery innovations.

## 

‚Ä¢ Customer Success

# Optimizing Travel Industry Platforms with Fornova

## Introduction

‚Ä¢ Customer Success

January 6, 2025

## Background

Fornova offers businesses in the hospitality industry online monitoring and optimization of performance and distribution of their inventory. Founded in 2010, Fornova now serves over 15,000 hotels globally and has over 100 employees with offices across the globe in London, Israel, Amsterdam and New York.

## The Challenge

Fornova wanted to migrate its platform to the cloud so that they could deploy their product and enjoy higher availability whilst being deployed closer to end-users.

Fornova eventually adopted Google Cloud because of the comprehensive features provided by Google Kubernetes Engine (GKE) and the cost-efficiencies of the Google Cloud Platform.

While Google Cloud and Google Kubernetes Engine form a powerful foundation, the Fornova team knew that their software development lifecycle processes needed to be enhanced before the team could take full advantage of the features of the cloud platform. Migrating to Google Cloud was a forcing function to support Fornova to modernize its delivery workflows with the ultimate goal of simplifying and accelerating their software development processes.

Fornova knew they needed a partner who had a deep level of expertise in the cloud platform, Kubernetes and Infrastructure as Code to maximize their success on the Google Cloud Platform.

## Our Solution

Fornova engaged OpsGuru, a certified Google Cloud consulting partner, due to the team‚Äôs extensive Kubernetes and Google Cloud experience and a proven track record with complex workload migrations.

The OpsGuru team leveraged OpsGuru‚Äôs Cloud Launchpad for Google Cloud to establish a secure and compliant baseline.

Cloud Launchpad is a rapid cloud adoption program that provides organizations with a prescriptive, best practice approach to build workloads on the cloud. Cloud Launchpad offers multi-tenant capabilities that enable the separation of environments by project or region ‚Äì especially area considering GDPR requirements ‚Äî and full auditability with tamper-free audit log storage.

With OpsGuru, Fornova migrated to Google Cloud and GKE with zero downtime.

## The Result

The engagement resulted in a successful migration to Google Cloud.

In this case, success was determined by two criteria:

1\. The migration was accomplished within the estimated time frame and budget.

2\. The migration was accomplished with zero downtime.

‚Ä¢ The core platform was built in a way that supported Fornova‚Äôs future business growth and allowed them to easily scale, as needed.

‚Ä¢ Infrastructure as Code best practices that were implemented throughout the stack has significantly improved Fornova‚Äôs resiliency ‚Äì This meant that the team could rapidly deploy the entire platform to a new region if necessary.

‚Ä¢ The newly built CI/CD platform has enabled an accelerated software release cycle and shortened the time to market. It also increased release and operational reliability.

‚Ä¢ Fornova‚Äôs team was able to quickly become productive on the new platform and technology stack, and are now empowered to innovate at an accelerated pace.

# How Gen AI Improves Accuracy by 20% at a Talent Acquisition Platform

## Background

The talent acquisition and recruitment industry is essential for building a company‚Äôs workforce, focusing on efficiently and accurately matching candidates to roles. This process is time-sensitive, as delays can increase costs and lead to missed opportunities. Accurate matching is crucial to enhance productivity and reduce turnover, making it a key strategic function in competitive markets.

## The Challenge

The 50-person private software company in Toronto has focused on providing a web-based SaaS talent acquisition platform to solve this problem by creating private talent pools for hiring fleets of contractors and providing a direct sourcing experience where candidates can view jobs and directly apply for jobs and talent acquisition specialists to communicate with candidates and manage the hiring processes seamlessly. However, as the volume of resumes keeps increasing and the job descriptions of the roles get more diverse, new solutions need to be explored to provide that timely and seamless experience.

That explains why the company is interested in leveraging language processing capabilities to solve the problem. Working with OpsGuru, the company identified three use cases that GenAI can significantly enhance. They are (a) using GenAI to enhance the clarity, conciseness and compliance of job descriptions (b) summarizing results from the data instantly and (c) ingesting job applicants‚Äô resumes into vectors to more efficiently identify the fit of candidates and job postings.

## Our Solution

As with any new technology, OpsGuru first started with a rapid proof of concept (POC) to validate the technology stack. After validating the POC, which consisted of a vector database and a few agents, the company rapidly committed to productization because of the apparent value of the solution. This led to a complete redesign of the workflow and resources to ensure the performance and validity of the job-matching process, as well as the security of sensitive personal information.

For the production system, multiple agents were set up to break down queries and send the relevant parts to the respective resume, job posting and company intelligent agent, such that the document can be indexed into summary, embedding and question indices to facilitate overall performance and accuracy of semantic search. The end-to-end workflow leverages Claude on Amazon Bedrock as the foundation model and Weaviate as the vector database.

## The Result

The AWS best practices of data protection using Amazon KMS encryption, Amazon S3 as raw data storage, and AWS IAM for access control have been adopted. At the same time, special care has been taken to ensure ethical guidelines and governance are incorporated: benchmark tests using frameworks such as BLEU and ROUGE have been included to measure the performance of the natural language processing workflows. A growing set of referenceable data has been included in the testing process to regularly measure the fairness and basis of the matching and summarization. Human-in-the-loop feedback has also been used to provide feedback on the accuracy of the process ‚Äì it has been found in the early rounds of testing that the Gen AI solution has introduced at least a 20% improvement of accuracy compared with the previous workflows, not to mention the improvement in the time taken to analyze resumes, job descriptions and find matching candidates to the job posts.

At the end of the implementation, OpsGuru also conducted knowledge transfer sessions to ensure the company has full ownership of the solution. As a result, the company is confident that they can shepherd the productized solution and grow the solution with more use cases along with the rapidly growing Gen AI technologies.

## 

# Brewing Business Success: OpsGuru Leads AWS Migration for Global Distributor

## Background

The beverage industry is moving fast. New products and new product categories are attracting new customers and opening doors to new opportunities. But the associated business growth can stretch the capabilities of traditional IT solutions to reliably deliver business services and accommodate continued growth and success.

This was the challenge for a diversified beverage company, which has remote offices in Canada, the United States, and Ireland. The company grew organically from a one-person wine importer into an international brewer and distributor of popular beverage brands. The business was supported by a centralized data centre in Canada that runs business services such as HR, payroll, and invoicing workflow, along with a point-of-sale system and an event and catering system.

## The Challenge

As the number of services required to run the business grew, the company became less comfortable with relying on a single, on-premises data centre to deliver its business services. It wanted to migrate to the cloud for more flexibility in how it deploys its workloads and add resilience by not relying on a physical data centre, which could be a single point of failure in the event of a service disruption. In addition, its monolithic Terraform workspaces were becoming increasingly difficult to maintain.

To get started on its goal to migrate its workloads to the cloud to cut costs, improve resilience, and accommodate business and data growth with easier scalability, the company created a footprint in AWS with a team of two to manage it. But to achieve excellence in cloud operations, the company needed to upskill its on-premises workforce to be proficient in AWS and Infrastructure as Code (IaC).

## Our Solution

OpsGuru helped make the customer‚Äôs AWS migration fast and efficient by using the AWS Migration Acceleration Program (MAP), which helps organizations speed up the migration process by avoiding common pitfalls and streamlining cloud adoption. The program helps organizations assess current workloads, estimate the financial benefits of adopting AWS, build AWS competency, and gain experience through migrating pilot workloads.

MAP is a three-phase methodology that helps organizations reduce complexity and costs while leveraging AWS cloud best practices to improve the success of an AWS migration:

1\. Assess: Identify capability gaps and create a total-cost-of-ownership model to justify a migration

2\. Mobilize: Quickly build competency and an operational foundation

3\. Migrate: Execute the migration plan and optimize the environment

## Assess

In the Assess phase, OpsGuru helped the company prioritize its operational capabilities to align with its business goals. The assessment included a workload inventory, an optimization & licensing assessment (OLA), a migration readiness assessment, and an analysis to determine the total cost of ownership. OpsGuru also helped the company rightsize its AWS footprint to deliver the best performance at the lowest possible cost.

In addition, OpsGuru worked alongside the company‚Äôs IT team to document its on-premises application workflows and develop a migration strategy, which included a new architecture to promote internal knowledge sharing as more internal teams became more familiar with AWS.

## Mobilize

In the Mobilize phase, OpsGuru built and tested the company‚Äôs infrastructure as code (IaC) model and helped it implement a ‚Äúdefence-in-depth‚Äù approach to its cloud security architecture that uses layered defences to protect systems and data. If one defence fails, another is available to block intrusion. In addition, OpsGuru consolidated security tools for simplicity and a zero-trust approach to access, and showed the customer best practices around how to use zero trust to meet its goals. To provide further data security, OpsGuru conducted a proof of concept for automated disaster recovery.

To prepare for the migration, OpsGuru developed a detailed migration plan for each application and selected a small group of applications with moderate risk and complexity to run pilot migrations. In addition, OpsGuru established a cloud operating model for the customer and established and implemented tagging standards for cost allocation and automated resource management. OpsGuru completed the Mobilize phase by establishing and implementing a centralized governance, risk, and compliance framework.

## Migrate

In the migration phase, OpsGuru executed the remainder of the migration plan, implemented the cloud operating model, and integrated cloud security, monitoring, and alerting tools into the system. As a learning exercise for the company, OpsGuru conducted hands-on workshops to help the company refactor its existing monolithic workspaces. Hands-on practice with OpsGuru oversight helped the customer‚Äôs IT team familiarize themselves with Terraform best practices.

A key challenge for companies migrating to the cloud is upskilling existing IT professionals on cloud processes and best practices. OpsGuru brought the company‚Äôs IT team along on the cloud journey by providing it with a training path, documentation, hands-on workshops for each step of the migration, and test environments to support the upskilling process. OpsGuru also helped the company modernize its operation processes by replacing the traditional department-oriented structure with a life cycle approach to operations management using product management principles to deliver operations.

## The Result

The company successfully completed all of its existing workloads and now has a roadmap for continuous improvement. It‚Äôs now identifying and prioritizing future work based on the roadmap, such as continuing to refactor applications for the cloud based on initial demonstrations from OpsGuru. The roadmap is supported by new processes and infrastructure to promote learning, sandboxing, and innovation while maintaining its monitoring, auditing, alerting, and compliance activities for business workloads. In addition, the company has clarity on governance using a shared responsibility model for internal stakeholders.

With its workloads securely in the cloud, and with new infrastructure, tools, and processes in place to deliver its business services, the company now has a fully modernized IT infrastructure that it can rely on to take its brewing and beverage business to the next level, and maybe even set the pace for the industry.

# OpsGuru Customer Success Story: Empowering Numeris on the Cloud Journey

## Background

Discover how Numeris, a leading data company specializing in audio and video consumer insights, transformed their business with OpsGuru‚Äôs expertise and Amazon Web Services (AWS) cloud solutions.

## The Challenge

Numeris faced challenges in information security and privacy due to the evolving landscape. They required an air-gapped backup solution, not connected to the internet, to meet insurance requirements. AWS provided a cloud-based solution and introduced OpsGuru as an implementation partner.

## Our Solution

OpsGuru proved to be more than just a vendor; they became a true business partner. They guided Numeris through cloud technology implementation, change management, and offered expertise in AWS technology. The partnership resulted in a successful transition on time and on budget.

Numeris experienced significant benefits after moving to AWS and partnering with OpsGuru:

‚Ä¢ Technology Upgrade: Their outdated on-prem infrastructure was replaced with modern cloud technology, eliminating the need for costly hardware replacements.

‚Ä¢ Enhanced Product Deployment: Previously, deploying new features or products could take weeks or months. With the cloud, scripting infrastructure allowed Numeris to deploy in minutes.

‚Ä¢ Engaged Teams: Cloud technology motivated their teams, leading to continuous upskilling and improved engagement. The process created a sense of ownership and enthusiasm among team members.

‚Ä¢ Customer Value: The cloud‚Äôs scalability and agility enabled Numeris to offer more timely and insightful data products to their customers.

## The Result

Thoughts from Mency Woo, VP of Enterprise Transformation, OpsGuru

üéôÔ∏è Choosing the Right Cloud Provider Selecting AWS was a strategic decision based on its mature technology and support offerings. OpsGuru‚Äôs partnership with AWS aligned perfectly with Numeris‚Äô needs for resilience, security, and scalability.

üéôÔ∏è The Human Aspect of Cloud Adoption Cloud adoption goes beyond technology rollout. It encompasses upskilling, change management, and aligning technology with business goals. OpsGuru served as a trusted advisor, ensuring Numeris‚Äô success by de-risking the cloud adoption journey and helping align technology with business needs.

üéôÔ∏è Unlocking Business Transformation OpsGuru‚Äôs focus on enabling business transformation through technology and empowering teams makes it a true thought leader. The collaboration with Numeris extended beyond technology, emphasizing people, processes, and governance.

Achieving Success Together Numeris‚Äô journey from on-premises infrastructure to the cloud with AWS and OpsGuru demonstrates the power of partnership, innovation, and customer-focused technology. The combination of resilient infrastructure, empowered teams, and cloud-powered growth positions Numeris for a bright future in the data industry.

Are you ready to embark on your own cloud transformation journey? Let OpsGuru be your guide, driving success through technology, people, and business innovation. Contact us today to learn more about our tailored solutions and partnership approach.

# Trimac Accelerates Business Expansion by Fast-Tracking Cloud Adoption

## Background

Digital transformation is a hot topic in transportation and logistics. For Trimac, North America‚Äôs leading provider of highway transportation for bulk commodities such as fuel and grain, modernization of its computing technology is about enabling business expansion. This includes its Bulk Plus Logistics division, which offers businesses a way to outsource logistics in order to focus more on their core activities, and its National Tank Services division, a network of facilities that provide maintenance services for tractors, trailers, and cargo tanks.

## The Challenge

To enable business expansion, Trimac‚Äôs strategy is to build secure foundations for its cloud infrastructure that will support a reliable and scalable data pipeline and enable application development for deployment on AWS.

The company‚Äôs requirements include a multi-account landing zone using best practices for security, logging, and management, and centralized monitoring to secure Internet traffic in and out of the environment, as well as to secure the movement of data between its MongoDB Atlas database and AWS.

Seeing the potential of adopting the cloud for business expansion and operational efficiency, Trimac wanted to explore how data workflows can help the organization to obtain insight from its operational databases. As the company already commenced adopting MongoDB Atlas as its data lake storage, it wanted to expedite the adoption by creating the data pipelines to realize the goal of optimizing operations through insights into existing operational data.

Trimac approached OpsGuru, an AWS Advanced Consulting Partner, such that Trimac could get expert support and guidance to adopt best practices before completing development on their new production workloads and accelerating its AWS adoption.

## Our Solution

While many organizations learn cloud best practices through trial and error, Trimac found a faster approach in OpsGuru‚Äôs Cloud Launchpad service, which smooths the roadmap to cloud adoption by reducing many of the common problems associated with establishing solid cloud foundations.

Cloud Launchpad combines pre-built, ready-to-go infrastructure as code (IaC) with prescriptive best practices that have been developed over hundreds of successful OpsGuru projects to help organizations move to the cloud quickly and securely. The Cloud Launchpad pre-built IaC includes development, test, and production accounts, giving Trimac a head start in developing its cloud-native applications. Cloud Launchpad was adapted to work with Terraform Cloud to provide the automation and enterprise feature set that Trimac required for managing their infrastructure as code while enforcing best practices in terms of multi-account structure, identity & access management, security and observability. Palo Alto Prisma Cloud was selected to add security scanning into their now DevSecOps development lifecycle.

To secure traffic flowing through the Internet to AWS, OpsGuru recommended and deployed two FortiGate servers, a next-generation firewall solution from Fortinet. To help Trimac fully embrace its cloud-native strategy, OpsGuru set the company up with Amazon Elastic Kubernetes Service (EKS) to deploy its containerized workloads.

To support the goal of getting insight from operational data, OpsGuru first helped Trimac move some business systems and the supporting SQL Server databases from the data centre to AWS using replication technology that minimized disruption to daily business operations. OpsGuru further developed a data pipeline to extract and transform operational data from the SQL Server databases to MongoDB Atlas for downstream business insights. The pipeline results in the adoption of Aurora Serverless and AWS Lambda functions, supporting the company to increasingly adopt an event-driven approach to daily operations while getting insights from data collected from multiple sources to support continuous operation optimizations and exercising true ownership of the data even when multiple third-party applications are continuously used for day-to-day business executions.

OpsGuru also worked with MongoDB Atlas to ensure available features were utilized to provide high availability (i.e., Multi-AZ), cross-Region replication, and the most secure connection (AWS PrivateLink) and authentication (IAM roles) methods, as well as being managed through the same Terraform code base. Cloud native applications connect to the MongoDB cluster from containers running on an autoscaling, multi-AZ Amazon EKS cluster.

To provide native high availability, commercial-off-the-shelf (COTS) software products, data is continuously synced to their cloud native environment via extract, transform, load data pipelines running in a serverless architecture taking advantage of multiple AWS managed services, including AWS Lambda, Amazon SQS, and Amazon Aurora Serverless, to provide native high availability.

In addition, Fortinet FortiGates were deployed from AWS Marketplace in an active-passive Multi-AZ configuration across a primary and DR AWS Region to support high availability, with a secondary active instance pre-configured to support disaster recovery.

Similarly, minimal ‚Äúhot‚Äù resources are deployed in the DR Region to support replication of critical data. In the event of a disaster, vetted and practiced runbooks are followed to deploy the remaining required AWS resources (i.e., the ‚Äúpilot light‚Äù strategy) using IaC and to complete configurations to resume traffic flows to the DR environment and resume required business functionality.

In addition, OpsGuru helped Trimac:

‚Ä¢ Deploy security features such as single-sign on (SSO) integration with Microsoft Azure Active Directory

‚Ä¢ Implement Amazon Cognito to enable authentication and authorization for their applications‚Äô API endpoints

‚Ä¢ Implement AWS App Mesh to provide observability of traffic within their EKS clusters, giving a single pane of glass view of how their application traffic is performing

‚Ä¢ Centralize networking to enable full traffic inspection by the FortiGate servers

‚Ä¢ Validate its data pipeline architecture and implement it in IaC with best practices for logging, monitoring, and tagging

‚Ä¢ Implement AWS CloudWatch Alarms to automate notifications through ServiceNow when operational metrics fall outside of configured thresholds

## The Result

Within three months, OpsGuru fully onboarded Trimac onto AWS with operational data pipelines, fully configured development, test, and production environments for their cloud-native containerized software running on Amazon EKS, and full traffic inspection for comprehensive security. During this time, OpsGuru trained the Trimac team on AWS best practices and provided guidance to enable Trimac to continue accelerating its journey to a cloud-native environment.

As a result of Trimac‚Äôs success with Cloud Launchpad, the company took advantage of OpsGuru Virtual Teams to help it operationalize its AWS production environment. Virtual Teams provides Trimac with a team of OpsGuru experts for guidance on next steps in its cloud journey and additional expertise to fill gaps in skill sets when needed during a project. Virtual Teams enable Trimac to build on its cloud foundations at a predictable cost with Trimac setting priorities and OpsGuru providing technical expertise and guidance.

By working with OpsGuru, Trimac was able to significantly compress its time to value in establishing a solid foundation in the cloud to enable its business expansion. Trimac‚Äôs application team is now actively developing new cloud-native applications on the Amazon EKS clusters and is managing application deployment with GitOps. The team is also able to give its business partners secure access to deploy Trimac microservices and can easily add more partners through simple updates to its IaC.

In addition, Trimac‚Äôs data team is actively using and extending its data pipelines as needed through minor updates to its IaC.

# Geographical Cloud Platform Expansion with Uncoil

## Introduction

‚Ä¢ Customer Success

January 6, 2025

## Background

eCommerce businesses are challenged with growth in a fast-moving, complex, ever-competitive environment. Based in Kelowna, BC, Uncoil offers brands a secret weapon to achieve profitable growth by transforming disparate, disconnected, complex data into real-time insights via proprietary models leveraging ML/AI-driven tools.

## The Challenge

As a growing business expanding into new markets, Uncoil was looking for a Google Cloud partner to assist with identifying gaps in their existing cloud environment. Additionally, Uncoil needed help with defining an action plan to guide them through their European cloud platform expansion.

One of Uncoil‚Äôs key priorities was to achieve General Data Protection Regulation (GDPR) compliance. GDPR is a legal framework that sets guidelines for collecting, storing, and processing personal information from individuals who live in the European Union. GDPR compliance mandates both technical and business alignment, improving the protection of European data subject‚Äôs rights. These legal guidelines also clarify what companies involved in the processing of personal data must do to safeguard these rights.

## Our Solution

Uncoil engaged OpsGuru, a certified Google Cloud consulting partner, due to the team‚Äôs extensive Kubernetes and Google Cloud Architecture Framework experience, along with a proven track record with customer enablement on their cloud journeys.

## A thorough assessment with Clear Path Forward

By leveraging OpsGuru‚Äôs Clear Path Forward framework, OpsGuru conducted a deep-dive business and technical assessment to capture the company‚Äôs platform growth plans and identify any significant gaps. At the same time, OpsGuru sought to ensure alignment of the cloud platform to Google‚Äôs Architecture Framework best practices‚Äî including Security, Reliability, Operation Excellence, Performance, and Cost.

The OpsGuru team thoroughly reviewed Uncoil‚Äôs existing Google platform, starting from VPC and IAM architecture and standards to project structure, compute, storage, message buses, and data processing layers. The reviewed services included Google Kubernetes Engine, Cloud Dataproc, CloudSQL, BigQuery, and others, where each component was thoroughly analyzed to ensure alignment with Google‚Äôs Architecture Framework best practices.

## A roadmap for remediating identified gaps

Based on the assessment results, OpsGuru created a detailed technical report that contained a clear, actionable roadmap for remediating the identified gaps in the form of Scrum sprints. Each identified gap included acceptance criteria, estimates, and priorities. By the end of the collaboration between the two businesses, OpsGuru conducted a thorough handover process to ensure the Uncoil technical team was empowered and educated.

## The Result

From its collaboration with OpsGuru, the Uncoil team gained the in-depth insights and list of actionable recommendations needed to ensure their successful geographical cloud platform expansion on Google Cloud. Furthermore, the Uncoil team was able to enhance their security posture by leveraging the proactive approach offered by Clear Path Forward.

Is your business looking for the right partner to assist with its cloud journey? Contact OpsGuru to learn more about our cloud services and solutions.

# When Isolation and Silos are good: Data stores in a multi-tenant solution

## Introduction

We have explored earlier in this post on a number of points to consider in setting up a multi-tenant Amazon EKS cluster. However, while it is important to safeguard multi-tenancy on the application cluster, it is equally important ‚Äî in fact, arguably more so ‚Äî to safeguard multi-tenancy for data stores.

AWS offers a comprehensive portfolio of data stores, making it a very compelling choice for a SaaS offering platform. However, securing and isolating these data stores in a multi-tenant, highly regulated environment brings its own set of challenges.

## Multi-tenancy on Datastores

At managing multi-tenant data, ‚Äúsilo‚Äù is the goal. Here are a few reasons that support clear tenant-based data partitioning in a multi-tenant SaaS solution.

## Cross-tenant access should generally be denied

Data security is a high priority topic in a SaaS solution ‚Äì not only for protection against external entities but also for security between tenants. Even in the case of inter-tenant collaboration, cross-tenant data access needs to be strictly controlled and monitored, and access should be granted based on business logic rather than direct datastore access.

## Different Encryption and Security Requirements required by Industry Standards

Depending on the industry, the tenants may be subject to different compliance standards. Some require data encryption with a clearly mandated frequency of encryption key rotation, while other standards require tenant-specific encryption keys instead of shared keys. By clearly identifying data assets associated with individual tenants, the different encryption and security controls as required by different compliance standards can be applied to individual tenants only as needed.

## Clear Performance Tuning based on the Tenant Subscription

While tenants follow the same general workflow as offered by the SaaS provider, each tenant may sign up to different sets of features and performance thresholds based on the tier the tenant is in. In a tiered pricing model, some tenants are willing to pay for high volumes of usage, while non-paying customers often have lower performance expectations. As a SaaS provider, it is necessary to monitor the usage of individual tenants and ensure the adherence to the performance promised by the service-level agreement. This does not only ensure that the resources are not monopolised by tenants but also supports clearer identification of up-sell opportunities.

## Easy Data Management

As the SaaS service grows, the number of tenants grows while some tenants will inadvertently decide to adopt other services. When a tenant unsubscribes from a SaaS offering, the tenant may expect the data to be fully exported and the historical data be deleted permanently from the SaaS solution. Given ‚Äúright to erasure‚Äù is one of the key protections in the European Union (EU) General Data Protection Regulation (GDPR), a SaaS provider needs to support that by clearly identify data assets of individual tenants for proper lifecycle management.

## Techniques to Address Multi-tenancy on Datastores

It should be emphasised that there is no ‚Äúone rule to fit all‚Äù at setting up tenant datastore silos because each solution will need to address its particular requirements.

A comprehensive solution depends on factors like service-level agreements with the tenants, read/write access patterns, compliance and regulatory requirements, and last by not least; costs. There are a number of data partitioning and isolation techniques that should be considered based on the requirements. Let us demonstrate the methods using a relational database such as Amazon Aurora.

## Tenant-id based data partitions on shared database and instances

In this case, a table is shared amongst the tenants, but the individual tenant data is separated and identifiable by a tenant\_id key.

The authorization happens on the actual relational database via the ‚Äúrow-level security‚Äù feature. Access of the application is based on an access policy that takes the tenant identity into account.

‚Ä¢ It is cost-efficient.

‚Ä¢ Authorization is enforced on the database level. It implies that there will be more than one authorization mechanism in the solution leveraging both AWS IAM and individual database policies.

‚Ä¢ Custom application logic has to be developed for a microservice to make sure it is aware of tenant-id.

‚Ä¢ No guardrails against potential ‚Äútenant noise‚Äù ‚Äì service-level agreement based on tiers of tenants cannot be enforced.

‚Ä¢ AWS CloudTrail cannot be used to keep track of access actions, because authorizations happen on the database level. Applications will need to compensate by logging the pertinent information extraneously instead for troubleshooting and tracking.

## Data schema isolation on shared instances

In this case, the tenancy is still shared on the instance level. However, the data siloing happens on the database level. This makes AWS IAM authentication and authorization possible.

‚Ä¢ It is cost-efficient.

‚Ä¢ AWS IAM is solely responsible for the authentication and authorization mechanism.

‚Ä¢ Because AWS IAM is in charge of access, the full audit trail is available on AWS CloudTrail without adjusting the logging on individual applications.

‚Ä¢ Because the underlying database instances are still shared by the tenants, it is not possible to defend against potential ‚Äúnoisy tenants‚Äù; service-level agreement based on still tiers of tenants cannot be enforced.

## Database instance isolation per tenant

1\. Database instance isolation per tenant

In this case, we have a complete tenant-based database and instance isolation. This solution is by far the most secure and reliable amongst the options because on top of the AWS IAM and AWS CloudTrail audit features there is also complete tenant isolation.

‚Ä¢ Both authentication & authorization are managed by AWS IAM.

‚Ä¢ The full audit trail is available on CloudTrail because AWS IAM is responsible for access control.

‚Ä¢ Complete noise isolation per tenant.

‚Ä¢ The total costs will be higher because of the database and instance isolation between tenants.

## How Applications can access the multi-tenant data

Being able to store the data in a tenancy isolation model that aligns with business requirements is essential, but regardless of methods, it is important to ensure the applications can access the data accordingly.

Fortunately, if you are using primarily AWS IAM for access control (as illustrated in both data schema and full instance isolations earlier), most of the work is already done. AWS IAM can be readily used by applications to access the data associated with the current tenant. Let us walk through this in the example Amazon EKS.

In EKS, pod-level IAM access can be achieved by using OpenID Connect (OIDC) identity provider together with Kubernetes service account annotations, as it enables an exchange of JWT with STS to offer the applications temporary access to relevant cloud resources. With this approach, no extended permission to the underlying Amazon EKS worker nodes is required. Instead, you can scope only the IAM permissions to the service account associated with the pods, based on the actual permissions the application (running as part of the pod) requires. Other than fine-grained control of permissions based on the applications/pods, another immediate advantage is the ability to have audit trail since each API call by an EKS pod will be logged by AWS CloudTrail to satisfy auditing needs.

The IAM integration, therefore, supports a comprehensive ‚Äútenant-aware‚Äù authorization system that fronts the access to the data stores. This adds another level of security as we do not blindly rely on database connection strings (that only controls access via authentication).

## Example: Amazon EKS accessing a multi-tenanted AWS DynamoDB table

To further illustrate the multi-tenant access, let us take a look at how an application running Amazon EKS can access a multi-tenanted Amazon DynamoDB. Often, multi-tenancy on Amazon DynamoDB is implemented on the table-level (i.e. table and tenant are in a 1:1 relationship). The following AWS IAM policy (called ‚Äò‚Äòaws-dynamodb-tenant1-policy‚Äù) illustrates this access pattern, where all data is associated with Tenant1.

{ "Version": "2012-10-17", "Statement": \[ { "Sid": "Tenant1", "Effect": "Allow", "Action": "dynamodb:\*", "Resource": "arn:aws:dynamodb:${region}-${account\_id}:table/Tenant1" } \] }

Next, we need to associate this role with an EKS cluster service account leveraging OpenID.

eksctl utils associate-iam-oidc-provider \--name my-cluster \--approve \--region ${region} eksctl create iamserviceaccount \--name tenant1-service-account \--cluster my-cluster \--attach-policy-arn arn:aws:iam::xxxx:policy/aws-dynamodb-tenant1-policy \--approve \--region ${region}

To leverage the newly created service account ‚Äò‚Äòtenant1-service-account‚Äù, the pod definition will contain the necessary ‚ÄúserviceAccountName‚Äù specification.

apiVersion: v1 kind: Pod metadata: name: my-pod spec: serviceAccountName: tenant1-service-account containers: \- name: tenant1 ‚Ä¶

While the IAM service account and the IAM policy is tenant-specific, static, and managed by configuration management tools such as Terraform and Ansible, the pod specification can be more dynamically configured. If you are using a templating engine like Helm (note: Helm supports lot more functionalities beyond that, templating is only one of its key features), the ‚ÄúserviceAccountName‚Äù can be set as a variable and set to the corresponding tenant-based service account depending on the deployment. It implies that each tenant will have its own dedicated deployment of the same application ‚Äî in fact, each tenant should have its own dedicated namespace where the tenant-specific applications are running (a point much belaboured in the previous post).

Similar examples can be drawn using Amazon Aurora Serverless, Amazon Neptune and Amazon S3 buckets. However, because of the length and the details involved, we will address the discussion in future posts.

## Summary

At creating a SaaS offering, it is important to consider carefully how data is accessed based on the different storage, encryption, performance and management requirements from the tenants. We have examined a few ways to partition data and showcased that there is no unified approach for data multi-tenancy. The advantage of running multi-tenant workloads on AWS is the availability of AWS IAM, which can be used to simplify tenant-based data access control and how applications can access the data dynamically based on the tenant.

Naturally, this blog only offers a high-level overview of a number of considerations and solutions. As every workload is different, some generic patterns are preferred over others, a comprehensive solution needs to be carefully designed after analysing applicable factors and anticipated requirements.

If you‚Äôre considering hosting your SaaS service on AWS and are looking for opportunities to improve on cost efficiencies or revise the security posture ‚Äì We‚Äôd like to learn more about your use case and work with you to build a solution that suits your needs\!

Simply reach out to info@opsguru.com

Written by: Paul Podolny & Mency Woo

# DeepSeek: The AI Game Changer and How to Use It on AWS

## Introduction

The world of Artificial Intelligence (AI) is buzzing about DeepSeek. DeepSeek R1 is a reasoning model. It can break down complex problems into step-by-step reasoning processes, similar to how humans think. This ability marks a significant step toward Artificial General Intelligence (AGI), and it challenges how we think about AI.

What is DeepSeek and Why Should You Care?

While new AI models emerge constantly, DeepSeek R1 stands out as a potential game-changer, lauded for several key innovations that could reshape the landscape of generative AI:

How is DeepSeek Challenging the Status Quo?

‚Ä¢ A Mixture of Expert(MoE) architecture from DeepSeekV3: The MoE architecture divides the workload among specialized ‚Äúexperts.‚Äù Depending on the token, only a subset of parameters must be activated. This drastically optimizes resource utilization and reduces computational costs.

‚Ä¢ Reinforcement learning reduces the need for extensively labelled data. DeepSeek R1 achieves robust reasoning without relying on extensive labelled data for supervised fine-tuning, drastically reducing the cost and effort of training models.

‚Ä¢ Changing the AI Game: DeepSeek R1 shows that the distilled versions are optimized for consumer hardware. This indicates that you don‚Äôt always need the biggest and most expensive technology to achieve great results in AI, opening up new opportunities for innovation.

What Does This Mean for You?

DeepSeek‚Äôs arrival is good news for everyone:

‚Ä¢ For Businesses: You can use AI to improve your products and services without breaking the bank.

‚Ä¢ For Developers: Experimenting with AI and creating new applications is easier and cheaper.

‚Ä¢ For Everyone: AI-powered tools and applications could become more accessible and affordable.

How to Get Started with DeepSeek on AWS

One valid concern surrounding DeepSeek is the potential for data to be shared with entities in China. While this is a legitimate consideration, especially when using the hosted version on DeepSeek‚Äôs website or mobile app, it‚Äôs important to remember that alternative deployment options exist.

Deploying DeepSeek models in a trusted and secure environment like AWS can significantly mitigate these risks. AWS provides robust data protection mechanisms and allows you to control the location of your data, ensuring it remains within your chosen region and complies with your specific security and privacy requirements. This will enable you to harness the benefits of DeepSeek‚Äôs technology while minimizing potential concerns about data sharing.

Amazon Web Services (AWS) is like a giant toolbox in the cloud, filled with tools and services for building and running applications. AWS makes it easy to start exploring DeepSeek.

Here‚Äôs how you can explore DeepSeek on AWS:

1\. Amazon Bedrock Custom Import: Consider this the ‚Äúeasy button‚Äù for DeepSeek. It lets you quickly import and experiment with DeepSeek R1 without worrying about a complicated setup.

2\. Amazon SageMaker JumpStart: If you‚Äôre more tech-savvy, SageMaker JumpStart provides pre-built DeepSeek models you can fine-tune and customize for your needs.

3\. Amazon EC2 Trainium1 Instances: For advanced users who need a lot of computing power, EC2 Trn1 instances offer a cost-effective way to train and run large AI models like DeepSeek.

4\. Amazon Bedrock Marketplace: A place to find pre-configured DeepSeek R1 models from different providers.

When working with AI, it‚Äôs crucial to use it responsibly. AWS provides tools like Amazon Bedrock Guardrails to help you:

‚Ä¢ Prevent Harmful Outputs: Ensure the AI doesn‚Äôt generate inappropriate or offensive content.

‚Ä¢ Protect Sensitive Data: Keep your confidential information safe.

‚Ä¢ Ensure Accuracy: This will reduce the risk of the AI making things up (also known as ‚Äúhallucinations‚Äù).

Security is also a top priority on AWS. You can control where your data is stored and use encryption to protect it from unauthorized access.

DeepSeek represents a significant step forward in AI, making it more accessible, efficient, and affordable. Using AWS, you can safely and securely explore DeepSeek‚Äôs potential and unlock new possibilities for your business or projects. Think of AWS as your playground for exploring DeepSeek and unlocking its potential.

Connect with our team of experts to learn more about DeepSeek on AWS.

# Demystifying AI Agents: 6 Types and Characteristics

## Introduction

‚Ä¢ Data and AI

‚Ä¢ Generative AI

‚Ä¢ Innovation

The term ‚ÄúAI agent‚Äù is generating significant buzz, but with that buzz comes confusion. Some envision AI agents as autonomous entities capable of navigating complex scenarios and making independent decisions. In contrast, others describe them as simple rule-based bots‚Äînot much different from traditional automation.

Artificial Intelligence (AI) agents are gaining traction because they can revolutionize how we interact with technology. They are a powerful tool that offers enhanced efficiency, improved decision-making, personalized experiences, scalability, and 24/7 availability. We free up human time and resources for more strategic work by delegating tasks to these intelligent agents.

In this blog post, we‚Äôll explore what AI agents are and the various types you can build to support your business across different industries.

## What are AI Agents?

Artificial Intelligence (AI) agents come in various forms, each designed to handle increasing levels of complexity, autonomy, and intelligence. At one end of the spectrum are simple reflex agents: rule-based systems built for automation and predictable responses. As we progress along the spectrum, agents evolve to incorporate context, goals, utility optimization, learning, and coordination.

This progression marks the journey from basic automation to increasingly autonomous and intelligent agent systems. Understanding these different types helps illuminate how AI can be applied in both straightforward tasks and complex problems in dynamic environments. Below, we‚Äôll explore how they pave the way for agentic AI to take centre stage in future applications.

## Types of AI Agents

To understand the diverse landscape of AI agents, we‚Äôll explore a framework that categorizes them based on their purpose and capabilities. There are various ways to categorize AI agents that exist:

1\. Simple reflex agents focusing on Automation

2\. Model-based reflex agents

3\. Goal-based agents

4\. Utility-based agents

5\. Learning agents

6\. Hierarchical agents

We use this framework as it provides a clear understanding of the different agent types and their technological implementations.

## Simple reflex agents focusing on Automation

Simple reflex agents are the most basic type of AI agent. They operate on predefined rules, making them straightforward to implement. They don‚Äôt need generative AI or complex reasoning.

Instead, they follow an ‚Äúif-then‚Äù logic: if a specific event occurs, they perform a predefined action. Technologies like rule engines (e.g., Drools), state machines (e.g., AWS Step Functions), and serverless functions (e.g., AWS Lambda) can be used to build these agents.

A common example is an automated system that sends a welcome email when a new customer signs up.

## Model-based reflex agents

Taking a step beyond simple reflexes, these agents incorporate a ‚Äúworld model‚Äù to enhance their decision-making. This model, often powered by Large Language Models (LLMs), allows the agent to analyze the context of a situation and make more informed decisions. In the shift to incorporating a ‚Äúworld model‚Äù and contextual analysis, we begin to see the nascent form of Agentic AI taking shape.

For example, a customer service chatbot could use an LLM to understand the nuances of a customer‚Äôs query and provide a more relevant and helpful response. Personalized recommendation systems and fraud detection systems are other examples of model-based reflex agents.

## Goal-based agents

These agents are designed to achieve specific objectives. They use reasoning and planning to determine the best action to reach their goals. A popular technique for implementing goal-based agents is ReAct (Reason and Act), which combines reasoning and action for multi-step problem solving. LLMs can play a crucial role in goal-based agents by helping to represent goals in a structured format, enabling more effective planning and reasoning.

A classic example is a chess-playing AI that analyzes the board and plans its moves to achieve the ultimate goal of checkmating the opponent. Other applications include task automation with complex workflows, robotics, and autonomous navigation.

## Utility-based agents

Going beyond simply achieving goals, utility-based agents aim to maximize a specific measure of ‚Äúgoodness‚Äù or utility. This utility function can be defined to incorporate various factors and preferences. Retrieval Augmented Generation (RAG) plays a key role here, allowing agents to access and process real-time information to optimize decision-making. ReAct can be adapted to implement utility-based agents by incorporating the utility function and RAG into its reasoning process.

For example, a travel booking agent could use a utility function that considers factors like price, travel time, and comfort to find the best flight for a user. With RAG, the agent can access real-time data on flight availability, prices, and even weather conditions to refine its recommendations.

## Learning agents

Learning agents represent a significant step towards autonomous AI, embodying the capacity to evolve and adapt through experience. They can analyze their past actions, identify areas for improvement, and adjust their strategies accordingly. This learning process often involves several key components:

‚Ä¢ Memory: Arguably the most essential component, memory allows the agent to store and recall past experiences, including successes, failures, and the outcomes of different actions. This stored knowledge informs future decisions and helps avoid repeating mistakes. Without some form of memory, an agent cannot learn from history.

‚Ä¢ Feedback Mechanisms: Crucial for most learning agents, feedback mechanisms provide information about the consequences of actions, enabling the agent to evaluate its performance and identify areas where it can improve. Feedback can come from various sources, including user input, environmental signals, or self-assessment. While most learning agents rely on feedback, certain unsupervised learning methods can learn patterns and structures without explicit feedback.

‚Ä¢ Knowledge Base: A knowledge base is a repository of information agents can use to learn and reason about the world. This can include facts, rules, and relationships that help the agent understand its environment and make informed decisions. While not strictly necessary for all learning agents‚Äîas some can learn directly from raw data or interactions‚Äîa knowledge base can provide valuable prior information and accelerate learning, especially in complex domains.

‚Ä¢ Retrieval Augmented Generation (RAG): RAG enables the agent to access and integrate real-time information from external sources into its learning process. This allows the agent to adapt to changing circumstances and make more informed decisions. While not essential for basic learning, RAG enhances the agent‚Äôs ability to adapt and evolve in dynamic environments.

Learning agents embody a more mature Agentic AI, surpassing model-based, goal-based, and utility-based agents by learning and adapting. They autonomously refine strategies through memory, feedback, and knowledge, enabling independent action and goal achievement. This self-improvement is key to true autonomy, allowing operation in dynamic environments without constant intervention.

While the level of autonomy varies depending on factors like learning mechanisms, knowledge, and human oversight, learning agents conceptually embody key characteristics associated with autonomous agents, representing a significant step towards realizing truly independent AI systems.

## Hierarchical agents

Building upon the concept of individual learning agents, hierarchical agents introduce a new level of sophistication by organizing these agents into coordinated teams. Multi-agent systems break down complex tasks into smaller, more manageable subtasks, delegating them to lower-level agents and orchestrating their efforts to achieve the overall objective. This hierarchical structure mirrors how a well-organized team functions in the real world, with specialized individuals contributing their expertise to a shared goal.

For example, a project management AI could act as a hierarchical agent, delegating tasks to specialized agents for scheduling, communication, and resource allocation. This division of labour allows for greater efficiency and specialization, enabling the system to solve complex challenges.

## How Can AI Agents Be Used in Different Types of Industries?

AI agents are versatile. They can be tailored to meet the needs of a wide range of industries, from enhancing operational efficiency to improving customer experiences. These intelligent systems are transforming how businesses operate:

## Model-Based Agents in Healthcare

Model-based agents can simulate human reasoning and decision-making, making them ideal for complex environments like healthcare. These AI agents can help diagnose illnesses, personalize treatment plans, and monitor patient health in real-time. By analyzing large sets of medical data, model-based agents assist healthcare professionals in delivering faster, more accurate care.

## Utility-Based Agents in Energy & Utilities

In the energy sector, utility-based agents are used in smart grid systems to optimize electricity distribution. These agents make decisions based on maximizing a specific utility, such as energy efficiency or cost savings, while adapting to fluctuating demand and supply conditions. This leads to stable energy delivery, reduced waste, and lower costs.

We go into more examples in our blog post on Autonomous Systems here.

## AI Agents in Customer Support

Customer service has been one of the most prominent areas for AI agent adoption. From virtual assistants to intelligent chatbots, AI agents can handle routine inquiries, provide 24/7 support, and escalate complex issues to human agents when needed. This not only improves customer satisfaction but also reduces the workload for support teams.

## Amazon Bedrock Agents: The Technology Behind Intelligent AI Agents

Understanding the diverse capabilities of AI agents is crucial for making informed decisions about their design and deployment. Different frameworks and approaches exist because of the diverse array of use cases and their specific requirements. By understanding the strengths and limitations of each type of agent, we can select the right one for a particular task and ensure that AI is used responsibly and beneficially.

Amazon Bedrock is a fully managed service that makes building and scaling generative AI applications easy. It offers a wide range of foundation models (FMs) for text and images, allowing you to find the model that best suits your needs. You can even customize these FMs with your own data, and easily integrate them into your applications using familiar AWS tools.

In our next blog, we‚Äôll dive deeper into the technical aspects of AI agents, exploring how Amazon Bedrock and other technologies can be used to implement these different types of agents. Read on to learn how to bring these agents to life and harness their full potential for your business.

# Enhancing Container Security in the CI/CD Pipeline with Amazon Inspector

## Introduction

In the dynamic world of cloud computing, strong security measures are essential at every stage of the software development lifecycle.

Organizations are increasingly exposed to sophisticated cybersecurity threats, so ‚Äòshifting left‚Äô is not just a best practice; it is a strategic objective. It aligns with the need for agile and secure software development. Ensuring security is a continuous integrated process rather than something that happens moments before the software is promoted to production. As professionals, we must implement tools and practices that support this shift, ensuring security exists consistently across the environment.

The recent integration of Amazon Inspector with CI/CD tools like Jenkins and TeamCity marks a new standard in cloud security. The integration goes beyond a technical milestone; it signifies a strategic shift in how we approach the security function during the development process, especially with containerized environments.

Amazon Inspector has the ability to scan container images. Giving developers the ability to identify and address software vulnerabilities within their pipelines. We are seeing more organizations rely on containerized workflows for efficiency and scalability.

Deploying Amazon Inspector moves the security function from a peripheral concern to a core element of the development workflow. Developers can now respond to security issues quickly and decisively, reinforcing the security of the entire software delivery pipeline. In short, Amazon Inspector closes the gap between the development and security teams.

## Container Image Scanning in CI/CD Pipelines

By integrating container image scanning into the CI/CD workflow, development teams can automatically analyze images for vulnerabilities before deployment. This proactive approach allows for the identification of potential security risks early in the development lifecycle. Additionally, establishing policies to prevent deploying images with critical vulnerabilities ensures that only secure container images make their way into production, contributing to a more resilient and secure CI/CD pipeline.

‚Ä¢ Developers contribute code changes by checking them into the Git repository.

‚Ä¢ Jenkins, configured with webhooks, is automatically triggered in response to code check-ins.

‚Ä¢ The pipeline‚Äôs checkout stage retrieves the latest version of the source code from the designated repository.

‚Ä¢ In the subsequent build stage of the pipeline, a container image is built based on the build definition.

‚Ä¢ The Amazon Inspector Scan stage meticulously examines the container image for potential vulnerabilities.

‚Ä¢ If the vulnerability scan identifies any issues exceeding the defined threshold settings or if specific criteria are not met, an issue is raised. This, in turn, causes the build to fail and triggers an immediate email notification to alert developers.

‚Ä¢ Without any identified issues during the vulnerability scan, the container image proceeds to the final step. The validated and secure container image is seamlessly pushed to the Amazon Elastic Container Registry (ECR), making it available for deployment and further stages of the CI/CD pipeline.

The automated nature of Inspector‚Äôs assessments streamlines the security review process, allowing developers to seamlessly integrate security into their CI/CD pipelines. This proactive approach not only enhances the overall security posture of applications but also fosters a culture of security awareness and continuous improvement within the development team. Ultimately, Amazon Inspector‚Äôs findings empower developers to deliver secure and resilient software, meeting both regulatory requirements and the highest standards of security best practices.

Furthermore, the Security Team plays a pivotal role by establishing threshold limits for the scans, ensuring that all container images must successfully meet these predefined criteria to progress to the next phase of the deployment pipeline. By defining these threshold limits, the Security Team sets stringent standards contributing to a robust security posture.

## Amazon Inspector Vs ECR Image Scanning Integrating Security Feedback During the Build Process

The ‚Äúshift-left‚Äù movement in software development and IT security is more than a passing fad; it is a welcomed evolution in the response capabilities for increasingly complex and more frequent security threats. In our experience, we have witnessed a significant shift in how security is baked into the software development lifecycle. Historically, security was often a final step ‚Äì a gate that software had to pass through before making it into production. This approach no longer makes sense in the current, fast-paced, agile development environments where speed and continuous development are key.

Amazon Inspector‚Äôs integration into the CI/CD pipeline clearly represents this ‚Äúshift left‚Äù philosophy by embedding security checks directly into the development process. Amazon Inspector enables developers to identify and rectify security vulnerabilities at the earliest possible stage. This proactive approach is essential for several reasons.

## Reducing the overall cost of security

Issues which are identified and mitigated early in the dev cycle are exponentially cheaper and easier to fix than those discovered later in the process or worse, after the product has shipped.

## Enhancing the quality of the software

Integrating security controls from the onset ensures that the final product is not just functional but also secure by design.

## Fostering a culture of security

‚ÄúSecurity is everyone‚Äôs business‚Äù. When developers are engaged with security from the beginning, it becomes second nature to them, not an afterthought.

## Amazon Inspector Pricing: A Real-World Scenario

Imagine a software development team at a mid-sized tech company, working on a critical application. This team operates on a fast-paced schedule, aiming for weekly releases to roll out new features and updates. Their development pipeline is moving quickly, with multiple builds and deploys happening. Let‚Äôs take a closer look at how Amazon Inspector‚Äôs pricing model fits into this dynamic environment.

The DevOps team uses Amazon Inspector integrated with their CI/CD tools to scan container images for vulnerabilities. Given the frequency of their release cycle, they have a high number of builds ‚Äì let‚Äôs say, on average, they perform 1000 builds in a month. Each build includes a container image that needs to be scanned for security vulnerabilities to ensure the integrity and security of their application.

Here‚Äôs where Amazon Inspector‚Äôs pricing model shines. AWS charges $0.03 per image scanned using their CI/CD solution. Let‚Äôs break down the cost for this team:

‚Ä¢ Cost per image scanned: $0.03

‚Ä¢ Number of images scanned per job: 1 (as each build involves one container image)

‚Ä¢ Number of jobs per month: 1000 (aligned with their weekly release schedule)

‚Ä¢ The total cost for image scanning for a month would be:

‚Ä¢ Total cost=$0.03√ó1√ó1000=$30

This pricing model is particularly advantageous for the team. The on-demand pricing allows them to align their security expenses with actual usage, which is crucial given the variable nature of their workload. They pay only for what they use, ensuring that their security costs are always proportional to their development activity.

Selecting Amazon Inspector to manage your container image scanning requirements not only provides a budget-friendly solution but also improves your automated security controls. Ensuring compliance with industry best practices. This example showcases how Amazon Inspector‚Äôs pricing and features align perfectly with the demands of a dynamic ‚Äúdeveloper-first‚Äù environment.

Expanding on its suite of security services, Amazon Inspector stands out as a versatile security assessment tool, going beyond just EC2 instances. Inspector adeptly identifies vulnerabilities across a wide range of AWS infrastructure elements, including EC2 instances, applications, Lambda functions, and notably, container images.

Consider the story of a dynamic software development team that uses Amazon Inspector within their CI/CD pipeline. They execute 1000 monthly builds, each requiring container image scans for security vulnerabilities. Amazon Inspector‚Äôs cost-effective pricing model, charging just $0.03 per image scan, translates to a mere $30 per month for this high volume of activity, demonstrating its value in a real-world application scenario.

For teams like this and security-minded developers, Amazon Inspector is a tool that offers comprehensive and cost-effective security assessments. This approach ensures a holistic and unified security strategy, vital for maintaining the integrity and resilience of diverse AWS workloads.

Amazon Inspector‚Äôs scalability, coupled with its on-demand pricing, makes it an ideal choice for organizations of various sizes and types, from fast-paced startups to established enterprises, aligning their security strategy with business requirements and developer workflows.

If you would like to learn more and how OpsGuru can assist you, get in touch with us.

# Grow Your Business with Generative AI

## Introduction

‚Ä¢ Generative AI

If you missed our last post discussing how generative AI can enhance your bottom line, be sure to check it out.

While cost optimization remains a crucial element of any business strategy, it‚Äôs equally imperative to focus on business growth. This is particularly true in today‚Äôs rapidly changing operating environment. A variety of factors ‚Äì including geopolitical, economic, and cultural considerations ‚Äì can greatly influence business operations. Successfully managing these influences and adjusting accordingly can be the difference between mediocrity and genuine success.

## Revolutionizing Customer Experience with Generative AI

Chatbot on websites is a common feature to engage customers and provide support. However, generative AI brings an extraordinary difference by significantly improving the user experience. Generative AI is capable of engaging in intricate two-way conversations, tailoring its communication style to match the customer‚Äôs persona and the brand‚Äôs voice. Compared to conventional, intent-based chatbots, the human-like interaction offered by generative AI greatly enhances the customer experience.

## Taking Customer Support to New Levels with GenAI-based Chatbots

A popular application of generative AI is search and summarization. While this feature bolsters workforce productivity, it‚Äôs equally beneficial for delivering swift, relevant responses to customer inquiries. A GenAI-based assistant can offer the prompt feedback that customers need while freeing up resources for more complex support tasks.

## Crossing Language Barriers

Language barriers can often hinder customer interactions. The large language models that underpin GenAI-chatbots usually possess multilingual capabilities, enabling them to translate retrieved information into the customer‚Äôs preferred language, thus boosting the customer experience.

## Boosting Revenue with Generative AI

Personalized recommendations have become a vital application of data and AI, and they are arguably the foundation of engaging socially. Generative AI takes personalization a step further ‚Äì into the realm of ‚Äúhyper-personalization‚Äù ‚Äì by creating unique content tailored to each customer‚Äôs past behaviours, preferences, and purchase history.

## Creating Virtual Try-on and Visualization Experiences

Generative AI allows customers to virtually ‚Äútry on‚Äù products, fostering a more immersive and personal connection with the item and expediting the purchasing process. Unlike augmented reality, which requires specific physical elements like a headset or store presence, Generative AI transcends these limitations. For instance, an image of a customer wearing an item of clothing can be produced pixel by pixel using a diffusion-based AI model, offering a realistic visualization that aids the purchasing decision.

This virtual try-on process typically involves the use of generative adversarial networks (GANs), which consist of two machine-learning models: the generator and the discriminator. The generator creates samples based on given parameters, while the discriminator works to distinguish between generated and real images. The models compete to enhance the overall quality of the output.

## Utilizing Synthetic Data for Fraud Detection

GANs are also highly effective at generating synthetic training data, a technique often used in fraud detection. This application addresses the issue where fraudulent transactions are greatly outnumbered by legitimate ones, also known as the imbalance class problem. A GAN can create synthetic fraudulent credit card transactions to bolster the data set while also bypassing privacy concerns. The higher number of realistic fraudulent credit card transaction samples can then be used to train models, improving the quality of the detection system and mitigating issues such as model overfitting.

In conclusion, our exploration of generative AI use cases has revealed a world of opportunities. Yet, it‚Äôs critical for businesses to probe key questions before diving in. Considerations should include whether readily available off-the-shelf services can be integrated into your workflow if specific training for your model is necessary for certain use cases, and what the security and cost implications of adopting generative AI might be. These are substantial topics, and we look forward to dissecting them further in our upcoming blog entry. Stay tuned for more insights into the fascinating realm of generative AI.

# Healthcare Agency Builds a Strong Cloud Foundation for Governance and Security

## Introduction

‚Ä¢ Customer Success

January 20, 2025

## Background

A provincial healthcare agency has a big mandate: broker healthcare and technology services to healthcare providers throughout its province. As the primary healthcare authority in the province, the organization‚Äôs mandate is to coordinate regional healthcare services to ensure the efficient delivery of clinical services and bringing critical healthcare services closer to where people live.

The agency centralizes several business services such as payroll and HR, and provides centralized computing services to deliver business and clinical services to other provincial healthcare providers such as hospitals, lab systems, and pharmacies.

## The Challenge

With the patient population growing and a significant increase in the number of services it is mandated to deliver, the agency‚Äôs on-premises data centre couldn‚Äôt handle the exponential growth of its workloads.

To meet business and clinical requirements, the organization needed to go online fast with new applications and services to ensure citizens have access to a coordinated network of high-quality specialized healthcare services. For efficiency, it also needed to consolidate its technology services across disparate internal groups.

Migrating all or part of its workloads to the cloud was a logical solution to get quick and cost-effective scalability and bring many new applications online quickly. But the agency didn‚Äôt have in-house expertise to make the cloud operable, sustainable, and scalable. Specifically, the organization wanted to build a solid cloud foundation to ensure compliance with stringent governance and security mandates.

Since many provincial healthcare organizations will come to depend on the agency‚Äôs ability to run a reliable cloud operation that meets the healthcare needs of Canadians, it needed an operating model that would work across clouds and internal departments.

Being steeped in traditional approaches to technology, the organization was looking for a partner that‚Äôs on the leading edge of cloud adoption and has the experience to give the agency an expedited path to scalability in the cloud.

To get up to speed with the cloud quickly and get consistent operational governance to support a multi-cloud environment, the agency selected OpsGuru as a technology partner.

OpsGuru was a good fit for this project because of its extensive experience with hundreds of cloud migrations across cloud providers. OpsGuru also has a deep background in the healthcare industry and understands its requirements for data privacy and compliance, including standards such as HIPAA and NIST. The agency also appreciated OpsGuru‚Äôs ability to understand its business needs and challenges, and the ability to explain technology recommendations in the context of a business rationale.

## Our Solution

The OpsGuru team started with an assessment of the organization‚Äôs environment. The team found that a key challenge in building a governance framework was the agency‚Äôs diverse internal groups, each with its own priorities and ideas about cloud governance and security. Some internal groups had already been experimenting with AWS, but the organization wanted more confidence on the platform before going into a migration.

To get consistency in best practices across a multi-cloud environment and across the agency‚Äôs diverse internal groups, OpsGuru delivered a model for infrastructure, migration, and implementation, including the tools and processes needed to implement security and governance in the cloud and promote security to end-users.

Because the agency acts as a broker for healthcare and technology services, OpsGuru showed the agency how to do chargebacks and showbacks, and how to write contracts with its partner healthcare agencies. A clear charging model helps build confidence and manage expectations in the budgeting process.

OpsGuru also helped the agency develop a cloud-native mindset to help it understand how to take advantage of the benefits of the cloud rather than use the cloud simply as an extension of its on-premises infrastructure.

## The Result

As a result of OpsGuru‚Äôs consulting work, the agency now has a strong foundation in cloud-based operational governance. This foundation includes a shared responsibility model that defines the roles and responsibilities of key stakeholders, giving it internal alignment on the implementation of best practices and its cloud adoption goals. In addition, the organization also has a strong foundation in security and governance for external needs to ensure information privacy and compliance with legislation.

OpsGuru delivered the light at the end of the cloud adoption tunnel the organization was looking for. With a strong cloud foundation in place and a clear roadmap for governance and security, the agency is now running experiments as a prelude to consolidating systems and migrating workloads to AWS, which the organization has confirmed as its primary cloud provider. The agency is looking forward to working with OpsGuru as needed with the next steps in its cloud journey.

## 

‚Ä¢ Customer Success

# Infrastructure as Code and Continuous Delivery Makes Database Development Easy

## Introduction

‚Ä¢ DevOps

Infrastructure as Code (IAC), Continuous Integration and Continuous Delivery (CI/CD) are becoming part of the standard pattern for delivering application code into production environments. Unfortunately, this methodology is rarely applied when deploying models for relational databases, often favoring more classic and manual methods that are thought to be safer. This is unfortunate because Infrastructure as Code and Continuous Delivery Makes Database Development easier for the developer and less risky for the business.

Implementing IAC combined with a CI/CD pipeline usually follows a progression from manual, to semi-automated with infrequent releases, to automated with more frequent releases. The common occurrence when CI/CD is applied to database models is to transition from manual to semi-automated but often, the process gets stuck at this stage, never reaching a stage with frequent releases. The cause of this stall will be slightly different for each company, but it is generally fear of losing or corrupting data. Every database developer I know has a war story about a production database mistake they have made that caused data loss or corruption.

Usually, these mistakes have a semi-happy ending where there was some downtime, and the database backup was used to do a full restore, but that is not always the case, and sometimes data is gone forever.

In the postmortems of database events, the common suggestions are to go slower to better understand changes. On the surface, this is a good suggestion, but what ‚Äúslowing down‚Äù means to most companies is to release less frequently. Unfortunately, releasing less often is not going slower, it just feels like it is. If the same number of developers are making roughly the same number of changes and those changes are just being released less often but all at once, that is better characterized as doing nothing broken up by short periods of going very fast. This can create a negative feedback loop where large batches of changes are made all at once to a database model, causing errors, leading to releasing less often, leading to larger batches, and more errors.

## Here are changes you can make to improve your database deployments, and why they help:

‚Ä¢ What: make small changes all the way to production. Why: small changes are less complex, they are easier to understand, easier to quickly code review, and if a mistake is made the impact of that mistake should be smaller, thus easier to fix. These changes need to be made all the way to production so we can ensure that all our databases (development, test, and production) are in the same state as well as so that the time between when a change is made and when a change is deployed is minimized so if there is an issue, the developer is still operating in the same context. Example: If you are adding a column and making it not null, do it in multiple steps rather than a single step. First add the column, release, then populate, release, then make not null, release.

‚Ä¢ What: Do not combine multiple changes into a single release, each change should get its own release. Why: You want to clearly understand what change each release is making, rolling up multiple releases into a single deployment can make changes to complex to easily understand. As well dependencies and order of operation errors can be introduced when rolling up releases.

‚Ä¢ What: Deploy all changes through your entire pipeline(I.E through the development and testing environments). Why: Development and testing environments exist to help catch mistakes before proceeding to production, but we have to make sure we use them. These environments should be combined with manual or automated testing to not just check for syntax and dependency errors, but for logic errors as well.

‚Ä¢ What: Never make manual changes to your databases. Why: Manual changes directly in production are one of the easiest ways to make large mistakes quickly. Any manual change you can make should be check in to source code and deployed via a release. This will allow changes to be code reviewed, tested, and validated before entering production. This allows manual changes, even critical ones, to go through the same validation process as normal code, this is the best way to solve problems quickly, rather than making them worse. As well there are some additional benefits that checking in manual changes/fixes allows other developers in the future to see and copy how other problems in the past were resolved.

The steps above may look like they introduce a lot of overhead, but realistically they are just a reversal of most people‚Äôs current practices. Rather than doing a lot of work less often, the pattern above is to do a little work more often. As well, mistakes will always happen. This applies to databases just as much as any other software. The best approach you can take is the path that limits impact and increases the chance of a safe fix. The current standard for deploying databases does not do this.

Addendum: As stated above, mistakes will always happen, the pattern above does not remove all mistakes as nothing can, it is important that a disaster recovery strategy exists for when large mistakes happen. If you do not have one for your databases, that should be your highest priority above all else.

# Lessons from ‚ÄúHowl's Moving Castle‚Äù: Defense through Agility

## Introduction

Ever since Netflix released all of Studio Ghibli‚Äôs masterpieces earlier this year, I have been taking a trip down the memory lane, revisiting animated movies that have made up a massive part of my formative years. While ‚ÄúHowl‚Äôs Moving Castle‚Äù (2004) is an excellent viewing, as usual, it turns out to be a great analogy for my day-to-day cloud architecture work.

Howl‚Äôs castle is located in the wild. The door to the castle turns out to be also a portal to the capitals of several kingdoms. Transporting oneself from one place to another is a matter of changing the sign on the door before opening it. Owned by a wizard named Howl, the castle can move through the moors and hills, and still appears to be just another house in the different towns. Naturally, there are many plots and twists in the movie, but it has always been the castle with its arthropod-like limbs that impresses me the most. A fantasy gothic castle with the best view of the landscape that can move anywhere, and yet can have easy connectivity to the hoi polloi ‚Äì wouldn‚Äôt it be great to be in one?

## Security Level 1: Castle and Moat

It did not take long for my thoughts to flow from Howl‚Äôs castle to cloud security because the castle has been a popular metaphor in security designs. For many years, ‚Äúcastle-and-moat‚Äù has been the primary security strategy, where teams focus on building network firewalls, proxy servers, honeypots, and other intrusion prevention mechanisms. There are strong defenses and verification on the data packets, and identity of users that enter and leave the network perimeters, but once inside the network ‚Äì the castle ‚Äì it is assumed that traffic therein is trusted and safe.

## Security Level 2: Zero Trust

Several well-known security attacks in recent years have exposed that the most damaging security threats often come from compromised identities. As workflows are touching more and more dispersed assets in disjoint networks, a comprehensive ‚Äúcastle-and-moat‚Äù is no longer feasible. Replacing ‚Äúcastle-and-moat‚Äù is a ‚Äúzero-trust‚Äù policy that often involves multi-step authentication and fine-grained authorizations. With zero-trust, even when one gets passage through the gate to the castle, every attempt to enter a different chamber, a different turret or even the dungeons of the castle is verified, and each of such attempts is recorded in tamper-proof access logs.

## Security Level 3: The Moving Castle

While ‚Äúcastle-and-moat‚Äù and ‚Äúzero-trust‚Äù are known security designs, how about taking it one step further, where we add mobility into the design? Howl‚Äôs moving castle is effectively the mission-critical infrastructure. It is fortified with the guardrails at the network perimeter and zero-trust access policies. Additionally, the infrastructure gains the ability, from on-premise to cloud, from one (availability) zone to another and/or one region to another, when it is needed, where it is needed.

This topic is in fact covered by what is popularly known as ‚Äúdisaster recovery planning‚Äù. Previously, people usually associate disaster recovery with catastrophic (albeit unintended) human error or a facility outage, but the solution applies well to security. With robust disaster recovery strategies, you get to move at the first signs of trouble ‚Äì be it natural disasters or malignant forces ‚Äì at a particular location to somewhere safe, instead of needing to face the onslaught of attacks, floods or storms head-on. As cyberattack is increasingly unavoidable, applying disaster recovery planning as a security response becomes a pragmatic security approach. As the legality of paying ransomware attacks is being debated on \[1\], it is obvious that the company has to bear the major responsibilities to keep the business running.

In ‚ÄúHowl‚Äôs Moving Castle‚Äù, because Howl closed the portals and moved the castle to a new land, even when the soldiers of the multiple kingdoms attacked and eventually broke down the door to the castle, they found themselves in empty courtyards or warehouses. Wouldn‚Äôt it be sensible to do the same thing ‚Äì close the link and move away ‚Äì when an organization faces a relentless and targeted attack through the same methods of entry?

This is actually an approach that is taken by a client with whom I am working, who has made the investment to build the recovery network, plan out the IP spaces and firewalls, schedule judiciously backups and share the backups across zones and regions, such that the client has the confidence to keep the mission-critical systems running in event of outage and attacks. Through building the multi-zone and multi-region disaster recovery procedures, the client has acquired completely different sets of encryption and remote access keys for the disaster recovery procedure, such that the attackers will find the security assets they have to be quite useless.

## Summary

Of course, one can only draw the analogy from the movie to cloud infrastructure so far. Using ‚ÄúHowl‚Äôs Moving Castle‚Äù as a precursor to a cloud security discussion may seem far-fetched to some, but it is apparent that security posture is not only about improving defense, but it is also about agility to move away from direct attacks.

As attack methods are getting more diverse and mechanisms more sophisticated, while an organization needs to keep reducing attack surfaces, identify suspicious activities as early as possible, a verified disaster recovery workflow is indispensable to improve odds against attacks, whenever and wherever they may come.

\[1\]

Image Credits: Studio Ghibli, StudioCanalPress

# Delivering Limitless Live Video for Today‚Äôs Needs with LiveSwitch

## Introduction

‚Ä¢ Customer Success

January 6, 2025

## Background

For LiveSwitch Inc. and its customers, network quality and reliability are everything. LiveSwitch delivers live video streaming to people around the world. It shares moments from news-breaking scoops to the game changers in sports. It also enables high-quality digital patient visits and remote learning.

The company was established in 2008 by two brothers, Jerod and Anton Venema. At the time, they planned to develop a collaborative diagramming tool. In 2017, in response to customer demand for large-scale videoconferencing, they launched LiveSwitch. The highly flexible server software and SDK supports dynamic scaling between video connection types depending on the session needs. Now, it offers the most flexible live video, audio and messaging software on the market today.

## The Challenge

Managing explosive growth and quality of services

In mid-2020, the Venema brothers realized that it was time to take the next step in the evolution of their business. With the onset of the COVID-19 pandemic, the demand for LiveSwitch‚Äôs video-streaming services increased exponentially. Corporate video conferencing needs shot up overnight when workforces were sent home en masse. However, LiveSwitch‚Äôs existing infrastructure provider platform couldn‚Äôt handle the volume and couldn‚Äôt scale fast enough.

At the same time, some red flags began showing up on the network performance of the underlying hardware. Then there was a major live streaming event that didn‚Äôt go well. ‚ÄúJust based on the nature of what we do, we are so sensitive to any variations in network performance,‚Äù said CTO Anton Venema. ‚ÄúIf there‚Äôs any instability, it gives everyone a poor user experience.‚Äù

The quality of its service was vital, especially given that it has many customers in the financial and health care industries that must comply with strict regulations regarding the security and integrity of their video feeds. ‚ÄúWe knew it was time to move on to a new provider,‚Äù said Venema.

## Our Solution

Working with OpsGuru and AWS to build the future

In June of 2020, LiveSwitch reached out to OpsGuru and Canada‚Äôs fastest-growing multi-cloud and DevOps consultancy. OpsGuru has partnered with hundreds of customers to leverage the power of the AWS cloud platform to help their business scale, and improve agility and integrity.

LiveSwitch initially sought help to unblock, scale and deploy across a variety of service providers. In November, OpsGuru and AWS helped LiveSwitch to leverage its Migration Acceleration Program. This allowed OpsGuru to conduct a deep assessment categorizing the needs of LiveSwitch, before mobilizing cloud foundations with Cloud Launchpad in December and finally migrating the entire platform into AWS by January of 2021\.

Prior to the migration, OpsGuru had worked with LiveSwitch to deploy their software using a container-based approach. With the foundation in place, OpsGuru deployed the Amazon Elastic Kubernetes Service (EKS) to orchestrate the microservices. This AWS service provides highly available and secure clusters for LiveSwitch and automates key tasks such as patching.

While deploying microservices orchestration on Amazon EKS greatly improved the availability and scalability of the services, it was only part of the solution. The LiveSwitch platform captures performance and user interaction metrics as a baseline to maximize customer experience. Such data was previously stored in SQL Servers. To help the platform scale up, OpsGuru helped the LiveSwitch team migrate to Aurora Postgres, such that the cloud-based platform now adopts cloud-native solutions end-to-end. Amazon Aurora not only alleviated the storage usage often faced by traditional relational database solutions, but the high availability and performance of the database also ensured data propagated efficiently to bring about a truly real-time live experience for the end user.

Disaster recovery policies and procedures were also developed by OpsGuru in collaboration with LiveSwitch to ensure that required business continuity targets are met.

‚ÄúOverall, the transition was fast and smooth because of OpsGuru‚Äôs expertise‚Äù, said Venema.

## The Result

Achieving blazing fast speed and scale

OpsGuru considers LiveSwitch one of its pandemic-growth success stories. ‚ÄúWe helped increase their reliability and velocity,‚Äù says General Manager Dave Lindon. ‚ÄúWe were building a trustworthy platform that can scale up and down according to customer demand. We also pushed the needle in security standards to support their diverse customer base.‚Äù

‚ÄúNetwork quality was one of the driving factors behind the migration,‚Äù said Venema. ‚ÄúThe number of outages and incidents is essentially zero at this point.‚Äù As well, Venema is happy that the AWS service is constantly monitored. ‚ÄúNo news is good news, which is the report all of the time,‚Äù he said. He also noted that round-trip latency is always ‚Äúnice and low.‚Äù Venema sees this level of reliability as a foundational component of the LiveSwitch service.

Improved security and compliance is also a key benefit. Recalling LiveSwitch‚Äôs early days, Venema says, ‚Äúwhen you‚Äôre a small startup, the goal is to just get the service up and running securely. As our target customer size increases, it becomes more important to adhere to independent guidelines and audits. That‚Äôs really where OpsGuru‚Äôs experience is very useful. They‚Äôre making sure that we‚Äôre doing things in a way that is regulatory-compliant from an objective point of view.‚Äù

The new environment using containers and the AWS Elastic Kubernetes Service allows LiveSwitch to release new features more quickly and securely. ‚ÄúIt gives us greater test capabilities without impacting the existing service,‚Äù Venema said.

Now, with the help of OpsGuru and AWS, LiveSwitch has a stronger foundation and greater stability, said Venema. ‚ÄúWe‚Äôre definitely in a better position than we were four months ago.‚Äù

Next steps:

The two companies are looking forward to further collaboration as LiveSwitch proceeds with expansion plans using AWS in Europe. By enhancing the Kubernetes-based platform, LiveSwitch is looking to grow its revenue 20 to 30 percent each year over the next five years. ‚ÄúThe demand is still going up, and we want to make sure the infrastructure is there to support it at any scale,‚Äù says Venema.

# Make Knowledge Sharing Effective with Amazon Q Business

## Introduction

‚Ä¢ Data and AI

‚Ä¢ Generative AI

‚Ä¢ Innovation

Effective knowledge sharing across an organization is crucial for success. However, many organizations struggle with this‚Äîapproximately 40% of participants in a McKinsey survey reported that complicated organizational structures are causing inefficiency. Information often remains siloed within individual departments or teams, hindering collaboration and innovation due to complex systems.

In 2025, business leaders must look to technological advancements like Generative AI to boost efficiency with limited resources. Below, we‚Äôll discuss the components of an effective knowledge management system and how you can achieve it with Amazon Q Business.

## What is Knowledge Sharing?

Knowledge sharing happens when teams and departments exchange information, skills, and insights to work better together and make smarter decisions. It covers everything from formal reports and presentations to casual tips and real-world know-how.

When information is easy to find and share, teams stay aligned and avoid working in silos. With business moving faster than ever, tools like GenAI can help by connecting different data sources and making it easier to uncover the knowledge that matters.

## 7 Components of an Effective Knowledge Sharing System

Success in information sharing depends not only on effective systems, but also on maintaining the quality and security of that shared information. Maintaining accurate and up-to-date information across different formats while also prioritizing information security can become challenging, ensuring that sensitive data is only accessible to those who need it.

An effective knowledge sharing system is made up of 7 key areas:

1\. Capture: the collection of explicit (e.g. documents and guides) and tacit knowledge (e.g. insights and best practices).

2\. Organization: a structure for information with clear categories, tags, and metadata.

3\. Storage: secure, scalable repositories to centralize content, like the cloud.

4\. Sharing & access: intuitive search and collaboration tools.

5\. Application: embedding knowledge into workflows, training, and decision-making.

6\. Governance: maintaining accuracy with regular updates, ownership, and compliance.

7\. Measurement: tracking usage and impact to drive continuous improvement.

Effectively sharing knowledge also relies heavily on employees. Finding and utilizing the right information can be time-consuming and challenging. Employees may not even be aware of the existence of critical information, leading to missed opportunities and inefficient workflows.

## How You Can Use GenAI to Improve Knowledge Sharing

Employees are wasting valuable time searching for what they need. GenAI can act as a bridge between systems, seamlessly integrating with platforms like Confluence, SharePoint, and customer relationship management (CRM) tools like Salesforce and HubSpot to boost productivity and efficiency.

Unified access

Imagine a sales representative accessing relevant customer data from the CRM in real time, product information from Confluence, and past sales records from the company database‚Äîall within a single, intuitive interface. This unified access eliminates the need for employees to navigate multiple systems and manually consolidate information, streamlining workflows and improving efficiency.

Intelligent analysis

GenAI can intelligently analyze and prioritize information based on factors like age, quality, and relevance. This ensures that employees are presented with the most up-to-date and reliable information, empowering them to make more informed decisions and improve the quality of their work.

Improved organization and accessibility

Beyond improved access, GenAI can enhance the organization and accessibility of existing knowledge. By analyzing documents and identifying key themes and concepts, GenAI can automatically tag and categorize information, making it easier for employees to find the specific information they need. This improved discoverability reduces the time spent searching for information and empowers employees to locate relevant knowledge quickly, improving overall productivity.

Security measures

Security is paramount in any enterprise system, and GenAI assistants are no exception. Robust security measures are essential to protect sensitive organizational data. These security measures‚Äîcombined with continuous monitoring and vulnerability assessments‚Äìhelp safeguard sensitive information and ensure that only authorized personnel can access and utilize the information within the platform.

## Amazon Q Business Brings GenAI-Powered Knowledge Sharing to Life

Amazon Q Business embodies these principles by acting as a central hub that integrates with a diverse range of data sources, internal databases, and Amazon services like Amazon Relational Database Services (RDS) and Amazon S3. This comprehensive data integration provides employees with a unified access point to information scattered across various systems, eliminating the need to navigate multiple platforms.

Amazon Q Business leverages powerful machine learning algorithms to rank and prioritize information based on relevance, recency, and source credibility. Administrators can refine this ranking by prioritizing certain documents or data sources through boosting metadata. With multiple data sources, a higher-ranked data source will be searched first for the answer before the lower-ranked ones. For example, information from the CRM could be given higher priority than general company news.

Amazon Q Business incorporates robust security measures, including:

‚Ä¢ Data Encryption at Rest and in Transit: Amazon Q Business is tightly integrated with AWS Key Management Systems (KMS), so encryption at rest can be implemented using either Customer-managed keys or AWS-owned AWS KMS keys. Amazon Q Business also uses the HTTPS protocol for data in transit.

‚Ä¢ Fine-grained access control: Information access is tightly controlled through integration with the IAM Identity Center (or IAM Federation). This ensures that only authorized personnel can access the information they are entitled to based on multiple factors. For example, a sales representative can access customer data, but not sensitive HR information.

‚Ä¢ Data privacy: Customer data remains within the customer‚Äôs control and is not used to train or improve the underlying AI models without explicit consent.

By leveraging these capabilities, Amazon Q Business empowers organizations to break down information silos, improve information discoverability, enhance information quality, and ensure the security of their valuable knowledge assets. This ultimately enables employees to make more informed decisions, improve productivity, and drive innovation across the organization.

## The ROI of Knowledge Empowerment

These improvements sound promising, but how do you ground them in real data? For any organization, the cost of implementing tools like Amazon Q Business is a critical factor. That‚Äôs why OpsGuru‚Äôs own journey began with a deep dive into its potential value and a careful cost-benefit analysis. In our next post, we‚Äôll unpack the key drivers behind our decision to move forward with Amazon Q Business, from the specific challenges we were facing to the real-world use cases that tipped the scale. We‚Äôll also share essential considerations for a smooth implementation‚Äîand how to measure whether your investment is actually paying off.

# Official Announcement: Appointment of New Chief Executive Officer

## Introduction

Today marks a significant moment in OpsGuru‚Äôs journey, as we are pleased announce the promotion of our EVP and CFO, Ryan Smyth, to the role of President and CEO. Since joining the organization, Ryan has been an invaluable member of the OpsGuru team, demonstrating a strong commitment to the AWS partnership, a laser-focus on providing exceptional service to our customers, and a deep understanding of our industry and its market dynamics. Prior to joining the company, he held progressively more senior leadership roles across a broad range of technology sectors.

We also want to recognize our outgoing President and CEO, John Witte, who has decided to retire. John‚Äôs visionary leadership and unwavering commitment to excellence over the past 3 years has been instrumental in shaping OpsGuru into the thriving organization it is today.

Please join us in wishing John all the best on his retirement and congratulating Ryan on his well-deserved appointment\!

OpsGuru is a Premier Partner for AWS, providing technologically advanced professional and managed services to clients around the world.

## 

# OpsGuru Achieves AWS Migration Competency

## Introduction

Today OpsGuru announces that it has achieved the Amazon Web Services (AWS) Migration Competency. This designation recognizes OpsGuru‚Äôs deep expertise and capabilities in assisting customers to successfully conduct complex migration projects, including discovery, planning, execution and operation.

Achieving the AWS Migration Competency has not been an easy feat, but it is well worth the effort as OpsGuru is now able to unlock the AWS Migration Acceleration Program (MAP). The AWS Migration Competency is required to leverage the Migration Acceleration Program, and is designed to help enterprises that are committed to a migration journey achieve a range of business benefits by migrating existing workloads to Amazon Web Services. MAP provides consulting support, training, services credits and additional funding to reduce the risk of migrating to the cloud and to ensure strong operational foundations are established.

OpsGuru Cloud Launchpad provides enterprise-grade cloud foundations. By utilizing best-practice concepts like infrastructure-as-code, Governance, Risk & Control (GRC) policies, isolated environments, tamper-proof audit trails and centralized billing with chargebacks, Cloud Launchpad ensures your organization is successful in their cloud adoption journey.

‚ÄúSince the inception of the company, OpsGuru has invested heavily in developing a scalable approach to support AWS adoption and migration. Our effort eventually culminated into the OpsGuru Cloud Launchpad,‚Äù remarked Anton Mishel, the CEO of OpsGuru. ‚ÄúEvery customer is different, but all customers can benefit from cloud adoption. Achieving the AWS Migration Competency validates OpsGuru‚Äôs experience and framework to support our customers in adopting the cloud. To continue on the success, the OpsGuru team is developing more products to support our customers not only during migration to the cloud but also on the modernization and innovation on the cloud. At OpsGuru, we always aim to be our customers‚Äô North Star. Our success is defined by how expediently our customers can realize the value of running workloads on the cloud, enjoy the security, scalability and cost benefits provided by the cloud, which in turn gives them more bandwidth to develop their individual business advantages.‚Äù

In addition to the AWS Migration Competency, OpsGuru holds the AWS SaaS Competency, which further supports customers looking to modernize their solutions and ultimately provide SaaS procurement of their existing or new technology. OpsGuru combines deep migration and modernization experience with our existing Public Sector, Immersion Day and AWS EC2 for Microsoft Windows Server service delivery statuses to ensure that your business takes full advantage of the cloud.

OpsGuru has helped dozens of customers across the globe to adopt and/or migrate to AWS. Interested in learning more about OpsGuru‚Äôs migration and modernization capabilities and how to de-risk migration, digital transformation using MAP and other AWS funding programs? Contact us or read more about how OpsGuru unlocks AWS MAP workflows.

# OpsGuru Achieves the AWS Small and Medium Business Competency

## Introduction

‚Ä¢ Announcement

‚Ä¢ Generative AI

‚Ä¢ SMB

OpsGuru is proud to announce that we have achieved the Amazon Web Services (AWS) Small and Medium Business Competency. This specialization recognizes OpsGuru as an AWS Partner with a unique focus on small and medium-sized customers (SMBs).

Achieving the AWS Small and Medium Business Competency differentiates OpsGuru as an AWS Partner with demonstrated proficiency and proven customer success, helping SMBs solve their business and technical problems. OpsGuru is equipped to handle these challenges with solutions designed with their customer‚Äôs unique needs in mind, including consideration for SMB‚Äôs typical deployment models, their level of IT capabilities and financing preferences, and their local and industry requirements.

‚ÄúOpsGuru is incredibly proud to achieve the AWS Small and Medium Business Competency,‚Äù said Ryan Smyth, President & CEO of OpsGuru. ‚ÄúThis achievement underscores our commitment to delivering tailored cloud solutions, including AI-powered tools, that empower SMBs to thrive. We understand the unique challenges SMBs face, and we‚Äôre dedicated to providing the expertise and support they need to succeed on AWS, leveraging the latest advancements in technology like AI.‚Äù

AWS enables scalable, flexible, and cost-effective solutions for startups and global enterprises. To support the seamless integration and deployment of these solutions, the AWS Competency Program helps customers identify AWS Partners with deep industry experience and expertise.

OpsGuru offers a comprehensive suite of services designed to drive cloud success, including:

‚Ä¢ Data Modernization & GenAI: OpsGuru unlocks the transformative power of data with its comprehensive data modernization and generative AI solutions, enabling businesses to extract valuable insights, improve decision-making, and drive innovation.

‚Ä¢ Modernization for Cloud Migration: OpsGuru goes beyond ‚Äúlift and shift‚Äù migrations to help businesses modernize their applications and infrastructure on AWS to maximize security, agility, and cost-efficiency. Its proven expertise ensures a smooth and successful cloud transformation journey.

‚Ä¢ Cloud Native Development: Empowering development teams to build, deploy, and manage modern, scalable applications on AWS, leveraging the latest cloud-native technologies and best practices.

‚Ä¢ Managed Cloud Operations: OpsGuru provides 24/7/365 monitoring and management of AWS environments, freeing businesses to focus on their core objectives while ensuring optimal cloud performance, security, and cost optimization.

‚ÄúOpsGuru has been a true partner in our cloud journey,‚Äù said Troy Heibein, Chief Commercial Officer of StellarAlgo. ‚ÄúTheir commitment to our success is evident in their proactive support and expert guidance. They consistently anticipate our needs and provide the solutions we need to stay ahead. We‚Äôre thrilled to see them recognized for their outstanding work with SMBs.‚Äù.

Contact us today to learn more about how OpsGuru can leverage Generative AI solutions to enhance your business outcomes.

# OpsGuru acquired by Carbon60, establishing an end-to-end Canadian Cloud Partner.

## OpsGuru announces acquisition by Carbon60, continuing operations as OpsGuru.

Today, Carbon60, a leading managed cloud hosting service provider, announced the acquisition of OpsGuru, Canada‚Äôs fastest-growing multi-cloud and DevOps consultancy. This acquisition makes the combined company the preeminent Canadian multi-cloud transformation and managed services organization.

This acquisition is a Canadian success story. Our combined 160-person team includes certified cloud consultants across AWS, Azure and Google Cloud, software developers with application modernization expertise, system administrators, security professionals and a 24/7 operations team. Jointly, Carbon60 and OpsGuru will continue to serve as trusted advisors to customers on their digital transformation journey.

OpsGuru was recently recognized as the 2021 Amazon Web Services Consulting Partner of the Year in Canada and has gained impressive traction with both customers and cloud providers alike. With our innovative capabilities in cloud adoption ‚Äî along with expertise in Kubernetes enablement, application modernization, DevOps, data insights and cloud security ‚Äî we‚Äôre thrilled to join the Carbon60 team.

‚ÄúThe acquisition of OpsGuru dramatically accelerates Carbon60‚Äôs realization of our vision to become the leading end-to-end multi-cloud service provider, helping Canadian companies plan, migrate and operate successfully in the cloud,‚Äù said John Witte, President & CEO, Carbon60. ‚ÄúCombined, Carbon60 and OpsGuru can fully support clients across Canada in their cloud evolution.‚Äù

‚ÄúWe‚Äôre excited to join the Carbon60 family and provide our combined customers with a true end-to-end multi-cloud offering,‚Äù said Anton Mishel, CEO of OpsGuru. ‚ÄúOpsGuru‚Äôs trademark culture of innovation and curiosity fits very nicely with Carbon60‚Äôs emphasis on providing responsive and reliable 24/7 managed services with a focus on excellence in customer experience.‚Äù

## What This Means for OpsGuru Customers

‚Ä¢ By combining OpsGuru‚Äôs cloud consulting and DevOps professional services with Carbon60‚Äôs 24/7 managed services, clients will enjoy end-to-end hybrid cloud service capabilities from the combined organization.

‚Ä¢ Together, Carbon60 and OpsGuru are now the preeminent Canadian multi-cloud transformation and managed services organization ‚Äî providing the agility, creativity and white-glove service needed in the Canadian market.

## About Carbon60

Carbon60 specializes in delivering secure managed cloud solutions for public and private sector organizations with business-critical workloads. Carbon60 provides secure and compliant 24/7 managed services using its cloud platforms across Canada, including AWS, Microsoft Azure and Google Cloud, to deliver scalable hybrid IT solutions. Carbon60 guides customers on their cloud journey and can engage with clients at any stage of their transition. Carbon60 provides a white-glove customer experience with a deep level of technical support services combined with a nimble approach making them the trusted choice for companies that need a partner to deliver solutions with exceptional reliability, performance, and security.

For more information, please read our press release or visit www.carbon60.com.

# AWS Hero Denis Astahov Brings Commitment to Community and OpsGuru Customers

## Introduction

Denis Astahov is a Solutions Architect by day and a YouTube content creator by night. Both are full-time jobs. Having the drive to take his skills in two directions at once is more than heroic. It‚Äôs what keeps Denis motivated, connected to the cloud developer community, and at the top of his game at OpsGuru, a cloud consulting company in Vancouver, Canada.

Denis has shown an extraordinary commitment to developing his own IT, cloud, and DevOps expertise and helping others learn it simply, efficiently, and effectively. In 2021, he was rewarded for those efforts and named an AWS Community Hero.

## ADV-IT, a YouTube Channel that Supports IT Skills

When Denis tried to build an Android app in 2014, he headed to YouTube for assistance. It helped. ‚ÄúI thought I could try to teach people something in the same way,‚Äù says Denis. ‚ÄúShort videos and information presented in an easy and simple way so even a non-technical person can understand it.‚Äù

Denis pursued the idea in late 2016, with videos in Russian, his native language (originally from Crimea, he spent 19 years in Israel before finding work in Canada). At about the same time Denis launched the channel, he started learning AWS. ‚ÄúIt was the best decision in my life,‚Äù he recalls.

Knowing firsthand that AWS was a great choice for career advancement, Denis was inspired to share that career advice with his growing subscriber base. ‚ÄúI‚Äôd started my first job in Canada and gained some experience, so I started teaching people about AWS; they‚Äôre constantly introducing new features and new services. It‚Äôs a never-ending learning process.‚Äù

ADV-IT‚Äôs largest playlist of 130 videos is dedicated to AWS topics and Denis expects it to grow. A perennial student himself, Denis has more than 12 cloud certifications, 9 of which are from AWS including his two most recent: AWS Certified SAP on AWS and AWS Certified Machine Learning‚Äîtoday‚Äôs leading-edge, must-have subject.

Denis‚Äôs Bachelor‚Äôs Degree in Education helps him translate AWS topics for his YouTube viewership and for OpsGuru clients. ‚ÄúWhen you‚Äôre confident in the topic, it‚Äôs easy to present it to audiences because you really know what you‚Äôre talking about.‚Äù

## With Hero Work Comes Hero Perks

Denis was thrilled to receive an email of congratulations in 2021 announcing he‚Äôd been named an AWS Community Hero. One of five heroes named in August of that year, he remains one of only five heroes in all of Canada.

AWS Heroes are nominated internally by AWS employees. Rather than having a fixed set of requirements, candidates must demonstrate a substantial impact on the AWS community. They should go above and beyond in helping people build, create, and innovate on AWS.

That‚Äôs exactly what Denis does in maintaining his channel. ‚ÄúOne video can sometimes take ten hours to create ‚Äì recording, editing, reviewing.‚Äù says Denis, ‚ÄúAnd you need to reply to a lot of comments. It‚Äôs another full-time job.‚Äù

Those efforts don‚Äôt go unrewarded. Heroes are invited to special sessions by AWS top brass, with the opportunity to meet and talk. At the 2022 AWS re:Invent in Las Vegas, Denis met Amazon Chief Technology Officer Werner Vogels. He rubbed shoulders with Amazon luminaries again this year at the 2023 Americas AWS Heroes Summit in Seattle, attending sessions by Amazon‚Äôs principal engineers and snapping selfies with Chief Evangelist Jeff Bar and Senior VP and Distinguished Engineer James Hamilton.

A Shining Star for OpsGuruOpsGuru is incredibly proud to have an AWS-elite expert on its staff. With invites to insider events, Denis has early access to updates and announcements from AWS. Being part of the development process, getting an insider look at what the company is working on and where it‚Äôs taking product development, is an invaluable perk. Customers can be confident knowing OpsGuru has the bleeding edge of information about the latest AWS services. AWS Heroes are also valued for the feedback they provide and OpsGuru customers benefit from that direct line of communication when their input is heard and acted upon.

With Denis on our team, IT and business leaders know their AWS cloud journeys at OpsGuru are guided at every stage by capable, award-winning cloud specialists. Denis has more technical certifications than most engineers and is deeply specialized in Amazon Cloud solutions and technologies. This makes him stand out and benefits the entire team. OpsGuru customers know they‚Äôre in good hands. We know exactly what AWS tools to recommend and how they support our customers‚Äô business objectives.

Denis‚Äôs achievements give OpsGuru a leg up on its competitors and deeper insight into AWS tools and capabilities. We‚Äôre knowledgeable and integrated into the cloud community locally and at a global level. With a deep, holistic view of the cloud, we‚Äôre able to advise customers at all stages of their cloud journeys.

Not every cloud company can boast about having an AWS Hero on its staff. OpsGuru customers know their workloads on AWS are handled with the utmost expertise. They also have added confidence knowing their cloud partner employs highly engaged people who are instrumental to keeping the cloud community engaged, educated, and innovating.

## Building Clouds and Changing Lives

In the meantime, ADV-IT continues to grow. Denis feels a responsibility to the community and a pride in helping other developers and engineers get ahead. ‚ÄúPeople write me on LinkedIn saying they got their first job or their first AWS certification, and tell me they started learning on my YouTube channel.‚Äù Viewership and subscriber statistics prove Denis‚Äôs channel is a success, but helping people get ahead is what‚Äôs most gratifying.

Does Denis have any advice for aspiring AWS Heroes? That‚Äôs simple, given he practices it every day at OpsGuru and on YouTube: ‚ÄúNever stop learning.‚Äù

# OpsGuru Named 2024 AWS Canada Regional Partner of the Year

## Introduction

‚Ä¢ Announcement

We are proud to announce that OpsGuru, an AWS Premier Tier Services Partner, has been named the 2024 AWS Canada Regional Partner Award winner. This prestigious recognition, unveiled at AWS re:Invent, celebrates partners who drive innovation, collaboration, and business transformation using AWS technologies.

At OpsGuru, we are known for our innovative approach to cloud consulting. We empower businesses with services such as Clear Path Forward, Generative AI Ideator, and Customer Reliability Engineering (CRE). These offerings reflect our dedication to delivering measurable results and accelerating our customers‚Äô cloud success.

‚ÄúThis award underscores our team‚Äôs relentless commitment to excellence and innovation,‚Äù I stated. ‚ÄúWe are honored to be recognized for our transformative impact and look forward to continuing to lead in cloud transformation.‚Äù

Our proven expertise and tailored strategies position us as a trusted partner for organizations navigating the complexities of digital transformation.

Press Release

## 

‚Ä¢ Announcement

‚Ä¢ 

# OpsGuru joining FoodX-led Consortium on Food Logistics Innovations

## Introduction

‚Ä¢ Cloud Launchpad

We are very excited to announce that OpsGuru has partnered with Canada‚Äôs Digital Technology Supercluster member Food-X Technologies to continue to enhance its eGrocery Management Solution (eGMS) to scale safe food delivery for retailers. OpsGuru will assist Food-X Technologies and other consortium members by delivering data platform capabilities for the eGMS solution built on Microsoft Azure. The Digital Technology Supercluster project consortium includes Food-X, Microsoft, OpsGuru, Adaptech, AltaML, ETG Consulting, Routific and Spud.

The COVID-19 pandemic has completely disrupted the business landscape. As our lives are changed forever because of the sweeping effect of the pandemic, it is more important than ever to carve a path forward with new ways to solve daily problems. We have all seen the queues in front of grocery stores in the past few months, and the emotional footages of people scrambling for daily essential goods in panic. That‚Äôs why when OpsGuru received the opportunity to assist in delivering innovations on food logistics, we knew right away we wanted to be part of it. Our strong passion for technology is fueled by knowing we can make a difference and in particular, helping to make daily supplies accessible to all those in need.

With cloud-native technologies lowering barriers to access computing power and storage, OpsGuru aims to help companies leverage cloud-native technologies to implement their visions and solve tangible challenges. Aside from the accelerating innovation in food logistics, we are steadfast in enabling companies to adopt cloud-native technologies. For established enterprises, we offer rapid migration to the cloud and cloud-native technologies adoption via the OpsGuru Cloud Launchpad. For start-up and small businesses, we offer programs to kickstart, reset and scale their businesses. As we specialize in rapid cloud deployments, Kubernetes and Data Analytics, we aim to make the cloud-native adoption journey of our customers to be effective and rewarding.

OpsGuru‚Äôs work can help enable your organization to adopt the cloud to boost your cloud-native journey, particularly in times where digital transformation has been accelerated.

## 

‚Ä¢ Cloud Launchpad

# OpsGuru Pioneers Launch of Professional Services within the AWS Marketplace

## Introduction

OpsGuru is proud to be distinguished as a launch partner of Professional Services on the AWS Marketplace. This program allows our AWS customers to procure our Professional Services through their existing AWS billing arrangements. While the cloud has made infrastructure provisioning much simpler, AWS adopters must ensure that they are adopting the cloud scalably, securely and cost-effectively. The AWS partner ecosystem plays a critical role in assisting customers to navigate the often complex cloud design paradigms, best practices and technology innovations. A true partner doesn‚Äôt just sell technology, a true partner offers a solution through the use of technologies to drive your competitive edge. OpsGuru navigates these complex trends and expediently implements and deploys technology-based solutions to real-world business problems.

If your organization has an existing AWS billing relationship, Professional Services in the AWS Marketplace can help you to accelerate your procurement of OpsGuru AWS Professional Services. OpsGuru is proud to be one of the few launch partners for this innovative program.

To learn more about our Professional Services in AWS Marketplace offering, please visit

The AWS announcement is available at

Full press release available here.

## 

‚Ä¢ AWS

‚Ä¢ Cloud Native

# Financial Technology Company Partners with OpsGuru for Cloud Transformation and Enhanced Control

## Introduction

‚Ä¢ AWS

‚Ä¢ Cloud Transformation

‚Ä¢ Customer Success

November 27, 2024

## Company Background

This financial technology company specializes in providing core banking solutions. With over 30 years of experience, it offers platforms for loans, mortgages, term deposits, investments, and retail banking alongside an API for open banking integrations. Committed to modernizing the financial services sector, the company sought to transition from traditional infrastructure to a more scalable and cost-efficient cloud environment to meet the industry‚Äôs evolving needs.

## The Challenge

The company faced several challenges with its existing infrastructure, including hosting applications across multiple on-premises locations. This setup led to scalability limitations, high capital expenditures (CapEx), and significant infrastructure management costs. The company also needed more oversight and control over their cloud environments. Despite working with third-party vendors to manage AWS environments, they needed more direct visibility into those accounts. They encountered long lead times for provisioning new customer environments, negatively impacting customer experience. Moreover, the costs associated with the vendor were opaque and higher than expected. The company needed a solution to provide more control over its cloud hosting platform while optimizing infrastructure for security, scalability, and cost-efficiency.

## Our Solution

OpsGuru collaborated closely with the company to design a comprehensive AWS Landing Zone in alignment with AWS Well-Architected Framework best practices. This solution provided the necessary network and security baselines, enabling the company to have better oversight and direct control over its cloud environment. Through the Migration Acceleration Program (MAP) assessment, OpsGuru assisted in migrating pilot applications to validate the cloud migration process. The solution reduced provisioning lead times and empowered the company‚Äôs team to fully own the provisioning process. Additionally, OpsGuru implemented automation to streamline provisioning timelines further and helped modernize core practices and infrastructure, establishing a self-service platform for developers and the operations team.

## The Result

The partnership between the company and OpsGuru led to successfully deploying a secure and scalable AWS environment. The new landing zone provided complete visibility and control over the cloud resources, reducing operational complexities and improving provisioning speed and efficiency. By transitioning to an in-house model and leveraging automation, the company was able to lower costs, enhance agility, and improve security. Both teams‚Äô iterative, agile approach helped optimize cloud operations, setting the stage for ongoing innovation and growth.

## 

‚Ä¢ AWS

‚Ä¢ Cloud Transformation

‚Ä¢ Customer Success

# How to Transform Your Public Sector Organization with Cloud Adoption

## Introduction

Whether you work in government, education, non-profit, or healthcare, we know that your public sector organization is unique ‚Äì and so are its individual challenges, from budgetary restrictions to significant governance, security and compliance requirements.

For many organizations, cloud adoption can ease the burden of these challenges through the many benefits that come from digital transformation. As an experienced cloud consultant that‚Äôs handled these challenges head-on, OpsGuru understands how to help organizations in the public sector leverage the undeniable advantages of the cloud.

‚ÄúIt‚Äôs all about the people.‚Äù

Team members in public sector organizations have an inherent interest in helping drive citizen outcomes, patient wellness, or the general public good. Adopting cloud technologies means that your organization will need to transform your existing staff‚Äôs hiring practices and career paths. A comprehensive enablement and migration plan will be essential for ensuring success. To understand more, let‚Äôs take a closer look at the steps for ensuring a successful cloud migration.

## The Key Steps for Ensuring a Successful Cloud Migration

Working with AWS and OpsGuru to map out the reason behind your cloud adoption, you can create a shortcut to success as other companies have by following these six essential steps to cloud migration:

1\. Establish the Goal for Cloud Adoption: The first step focuses on establishing a plan based on your organization‚Äôs ultimate goal for cloud adoption. OpsGuru works with you to create a mission-driven case for migration, helping you begin the process of instilling a change management mindset throughout your workforce.

2\. Understand your Compliance Requirements: In the second step, OpsGuru helps customers understand the robust controls needed to maintain security and compliance in the cloud. Understanding compliance governance will help your organization better understand the cloud environment and help define the activities to be performed.

3\. Assess your Organizational Capabilities: In the third step, OpsGuru guides you in developing a team of your own people to lead the transformation according to your goals for adoption. As your move to the cloud evolves, so will the effectiveness of your organization‚Äôs Cloud Center of Excellence (CCoE) team.

4\. Establish Compliant Cloud Foundations: The fourth step focuses on establishing a compliant cloud foundation. With tools like OpsGuru‚Äôs Cloud Launchpad, a multi-faceted approach to move organizations to the cloud rapidly and securely, public sector organizations can achieve rapid foundational cloud deployments by rolling out a scalable and secure cloud foundation.

5\. Plan the Right Migration Strategies: There are a variety of strategies to review when seeking to migrate workloads to the cloud. In step five, OpsGuru helps your organization find the right combination, grouping your applications according to the ‚Äú7R‚Äù strategies to ensure the smoothest migration.

6\. Execute your Migration: The final step involves ramping up the migration effort. As you land more applications and workloads on AWS, the focus shifts from the individual application level to the portfolio level. A continuous improvement approach is often recommended, with OpsGuru serving as your committed expert consultant throughout the journey.

## Expert Guidance Through Your Cloud Migration

Awarded 2021 Canada AWS Consulting Partner of the Year, OpsGuru provides your organization with all of the training, tools, security, and expertise necessary to move forward, achieve success, and master your migration to AWS.

OpsGuru has empowered hundreds of customers from multiple industries to successfully adopt cloud technologies and achieve success by delivering Cloud Adoption, Application Modernization, Kubernetes Enablement, Cloud Security, Data Analytics and Managed Cloud services to customers across the world.

Our deep experience with the cloud in enterprise and SaaS makes us uniquely qualified to help you on your cloud migration and digital transformation journey. As an AWS partner, OpsGuru can offer expert assistance on:

‚Ä¢ Cloud Migration Strategy: Providing all the tools and capabilities to successfully adopt cloud solutions.

‚Ä¢ Security & Compliance: Providing tailored cloud security approach in alignment with compliance and security best practices.

‚Ä¢ Build your Internal Cloud Capabilities: OpsGuru provides not only a technical solution but programs to steadily build your team‚Äôs know-how to develop your AWS Cloud Center of Excellence and continually extend, expand, and evolve.

The decision to commit your organization to the cloud is a big move but can be successfully achieved by following the six essential steps to migration under the guidance of the right partnership. Interested in examining those steps more closely as you consider adopting cloud?

Download the Cloud Migration in the Public Sector: 6 Key Steps to Success Ebook to learn the steps for ensuring a successful cloud migration and transform your mission-driven initiatives.

# How to Streamline & Accelerate Your SMB Cloud Adoption

## Introduction

‚Ä¢ AWS

‚Ä¢ Cloud Launchpad

‚Ä¢ SMB

The most successful businesses in today‚Äôs fast-changing, digitally fueled society are those who take advantage of innovation and are data-driven. However, small and midsize businesses (SMBs) typically have fewer resources to fund the significant upfront investment that is often required to innovate, putting them at a disadvantage.

Are you looking to innovate your small or medium business in the cloud? Contrary to popular belief, cloud migration is not reserved for large companies with seemingly unlimited budgets and resources. With the right partner by your side and the use of out-of-the-box, cost-effective solutions, you can reduce your barrier of entry and time to market, successfully manage your cloud environment based on best practices, and even achieve compliance such as HIPAA, SOC2, or PCI-DSS, if relevant.

Research shows a trend of decision-makers for businesses of all sizes being drawn to cloud service providers and public cloud services, private cloud services or hybrid cloud environments. Some of the most important reasons for this shift involve the opportunities provided by cloud computing vendors, such as the ability to reduce costs, increase security, and improve agility.

By migrating some or all of your on-premise servers or infrastructure to Amazon Web Services (AWS), your business can achieve transformational results while revolutionizing your business processes and unleashing greater insights.

However, cloud adoption can lead to undue business interruption if it is not accomplished efficiently and securely. That‚Äôs why this process requires careful consideration of the complexity of your environment and the level of your in-house expertise.

By making AWS cloud adoption more easily accessible, affordable, and efficient, OpsGuru unlocks the innovation potential of every business, whatever its size. In this article, we‚Äôre discussing how our cloud adoption solution, Cloud Launchpad, can help you ensure your company is successful in the cloud while keeping more money in your pocket.

## Achieving Rapid Cloud Deployment

Built on years of experience in AWS cloud adoption solutions, the OpsGuru team has developed Cloud Launchpad. This streamlined cloud adoption solution provides a five-step roadmap along with personal coaching on the best practices of AWS cloud design and operations.

In a few as five days, Cloud Launchpad looks to reduce common errors companies experience establishing their cloud foundations. The solution also helps SMBs build the internal expertise and technical experience they need to own, operate, and continue to evolve on their own cloud environment while meeting their new technology goals.

Here are some of the benefits that SMB owners can achieve with the help of our Cloud Launchpad solution:

‚Ä¢ A reduced cloud learning curve so your team can become self-sufficient in your cloud environment faster.

‚Ä¢ Security and reliability guardrails.

‚Ä¢ Increased cost savings to reduce the barrier to entry.

‚Ä¢ Accelerate your time to market with a shorter cloud adoption timeline and faster deployment of solutions.

‚Ä¢ Compliance readiness for standards such as HIPAA, SOC2 and PCI.

‚Ä¢ The ability to extend beyond the Cloud Launchpad with ownership of the infrastructure as code codebase.

## Bosa‚Äôs Roadmap for Success

Bosa Development has been a leader in the real estate industry in Western Canada and the United States for over four decades. With headquarters in Vancouver, British Columbia, Bosa Development‚Äôs legacy IT infrastructure was distributed across three geographic locations and could no longer keep up with the growing demands of its rapidly expanding business.

OpsGuru worked with Bosa to architect and deploy a secure AWS foundation leveraging Opsguru‚Äôs Cloud Launchpad before doing a ‚Äúlift-and-shift first‚Äù migration strategy for their Windows workloads, taking advantage of the CloudEndure Migration service.

With the help of OpsGuru‚Äôs Cloud Launchpad and AWS, Bosa now has a more secure, reliable and scalable IT infrastructure that supports the company‚Äôs business goals.

OpsGuru also ran a series of hands-on Cloud Launchpad training workshops to equip the Bosa team with AWS cloud best practices to leverage in the coming years. These enablement sessions produced a comprehensive set of operational playbooks and documentation covering AWS architecture and day two operations to empower the Bosa team to rapidly manage their cloud environment efficiently and independently.

## Unlocking SMB‚Äôs Cloud Launchpad Potential

Cloud technology is evolving rapidly, and it can be challenging for SMBs to appreciate everything that is currently achievable through the use of cloud-based solutions. By leveraging Cloud Launchpad, SMBs can take advantage of OpsGuru‚Äôs expertise, guidance and a range of value-added services. Based on your business goals and drivers, OpsGuru can identify the opportunities available with AWS cloud technology and help businesses capitalize on the benefits that it provides.

The OpsGuru team has helped many businesses of all sizes unlock their innovation potential to enhance customer responsiveness, power new services and business models, and improve business resilience.

Check out our Cloud Launchpad page for more information, or contact us to learn more.

# Top 10 Misconceptions around Migrating Hadoop to the Cloud

## Introduction

‚Ä¢ Big Data

Lots of mid-size companies and Enterprises want to leverage the Cloud for their Data Processing requirements for obvious reasons such as: agility, scalability, pay as you go model etc.

But the reality shows that migrating a production, Petabyte scale, multi-component Data Processing pipeline from on-prem to the Cloud is never an easy task. With the sheer amount of different services (Hadoop, Hive, Yarn, Spark, Kafka, Zookeeper, Jupyter, Zeppelin to name just a few) and conceptual environment differences, it‚Äôs easy to get lost and succumb to one of many pitfalls.

In this article, I will present some common misconceptions and tips on how to make your on-prem to Cloud migration smoother and potentially save you some pain. I will use AWS as a reference but of course, all the mentioned is valid for other providers like GCP and Azure which offer similar solutions.

## \#1 ‚Äî Copying data to the Cloud is easy

Transferring several PB‚Äôs of data into Public Cloud Object Store such as S3 (which will be used as your Data Lake) is not as trivial as it sounds and can quickly become extremely cumbersome and time-consuming process.

Despite the fact that numerous solutions (both open-source and commercial) exist out there, I haven‚Äôt really found one solution that covers all my needs (transfer, data integrity & verification, reports).

Usually, a good strategy is to break the data transfer task into two phases;

If a certain portion of the data is relatively static (or rolling in a reasonably slow pace) a good idea will be to use a solution like AWS Snowball to copy a good portion of your data to a physical device ‚Äî usually, such device will be plugged into your DC and when the copy process is done will be physically transferred to AWS DC‚Äôs and uploaded to your S3 bucket. Keep in mind that several Snowballs might be required ‚Äî so your data will be sharded between them. After the major portion of the data was transferred and uploaded to the object store, use a Direct Connect uplink to the cloud provider in order to fill the remaining gaps (a variety of methods can be used here, for example: scheduled Hadoop DistCP‚Äôs or Kafka Mirroring). Both of these methods have their own issues: DistCP requires pretty serious tuning and continuous scheduling, also not all versions of DistCP allow you to blacklist/whitelist objects. Kafka MirrorMaker requires a lot of tuning and visibility (metrics export via JMX) in order to be able to measure its throughput, lag and overall stability.

Verification of that data in the Cloud object store is even harder and in certain cases requires some custom tooling. For example, a custom Data Catalogue with object name hashes or something similar.

## \#2 ‚Äî ‚ÄúCloud is just another DC, things should work the same‚Äù

What once behaved well in the on-prem world might not work as expected in the Cloud.

The best example for this would be Zookeeper and Kafka, where the ZK client library caches the resolved ZK server addresses for its entire lifetime, this is obviously a huge deal for Cloud deployments which are sometimes ephemeral in their nature and requires custom workarounds, such as static ENI‚Äôs for the ZK server instances.

From the performance perspective, it‚Äôs a good idea to perform a series of NFT‚Äôs (non-functional tests) on the Cloud infrastructure to see whether the instances and storage layer you chose suffices your workloads. Remember the environment is multi-tenant so it‚Äôs possible that ‚Äúnoisy neighbours‚Äù will steal some of your precious capacity.

## \#3 ‚Äî ‚ÄúObject Store can replace HDFS 100%‚Äù

Separating the compute layer from the storage layer sounds like a great idea, but with great power comes great responsibility.

In most cases, the object store is eventually consistent (with the exception of Google Cloud Storage which claims to have strong consistency), which means that it can be used for both raw and processed data input as well as for final results output.

However, it cannot be used as a temporary storage, that require multi node access where HDFS is still required.

## \#4 ‚Äî ‚ÄúDeployment of Cloud Infrastructure is just a couple of clicks in the UI‚Äù

While this might be true for a small, very early stage, test environment, you will probably want a reliable and repetitive method of provisioning your infrastructure via code. You also probably want to have multiple environments (Dev, QA, Prod). Tools like CloudFormation and Terraform can do the job, however, their learning curve is not always trivial. You will find yourself re-factoring and re-writing the code-base multiple times.

It‚Äôs usually a good idea to integrate it with a CI/CD flow that will include a pre-deployment verification stage (pre-flight tests) and smoke tests, which is also not always trivial.

## \#5 ‚Äî ‚ÄúVisibility on Cloud is easy ‚Äî I‚Äôll just use ${SaaS\_name}‚Äù

Clear visibility (logging, monitoring) of both of your old & new environments is crucial for a successful migration.

Sometimes this becomes non-trivial due to the fact that different systems are used in the two environments, for example; Prometheus and ELK are used on-prem and NewRelic and Sumologic are used in the Cloud.

Even if a SaaS solution is used for both of the environments (which can get costly at scale), there is an effort with exporting and processing the application metrics, for example extracting JMX metrics from the apps, setting up aggregations and dashboards, creating alerts etc.

## \#6 ‚Äî ‚ÄúCloud just scales indefinitely‚Äù

Users are often excited to hear about feature such as Auto Scaling Groups and think they can easily apply them on their data processing platforms; while in some cases it‚Äôs relatively trivial (for example EMR worker nodes without HDFS), in other cases, where persistent storage is involved ‚Äî it‚Äôs not and sometimes requires quite a significant effort (for example Kafka brokers).

Before shifting traffic to your cloud infrastructure, it‚Äôs usually a good idea to check the current resource limits (number of instances, disks etc) and pre-warm the load balancers; without that your serving capacity can be limited and can be easily avoided.

Last but not least, it‚Äôs worth remembering that as scalable as cloud is, the depth the budget has its boundaries üòâ

## \#7 ‚Äî ‚ÄúI will just lift and shift my infra as-is‚Äù

While it‚Äôs usually a good strategy to avoid potential vendor lock-in and rely on proprietary data stores (such as DynamoDB), it‚Äôs a good idea to leverage services that are API compatible, for example using Amazon RDS for Hive Metastore DB is probably a good idea.

EMR is another good example, no need to re-invent the wheel here, just keep in mind that heavy customization of EMR might be required via post-installation scripts (tunables such as heap sizes, 3rd party jar‚Äôs, UDF‚Äôs, security addons) etc.

In case of EMR it‚Äôs also worth mentioning that there is still no HA available for the master node (NameNode, YARN ResourceManager etc), so it‚Äôs up to you to architect your pipelines to be able to tolerate a failure and adjust to more ephemeral state.

## \#8 ‚Äî ‚ÄúI will just port my Hadoop/Spark jobs to the Cloud‚Äù

This is where things get complicated. For a successful job migration you will need to have clear visibility into your business logic and pipelines; from initial ingestion of RAW data to meaningful, distilled aggregations. It gets even more complicated where the results of pipeline X and pipeline Y are inputs of pipeline Z. These flows and interdependencies have to be clearly mapped (a DAG chart might be a good idea) with all the components involved. Only after this mapping, you will be able to effectively start moving your analytical pipelines keeping the business SLA.

## \#9 ‚Äî ‚ÄúCloud will reduce operational expense and staff‚Äù

While it can happen that operations with owned hardware will require more personnel for physically supporting on-premise resources, with cloud the personnel will still need to respond to business needs ‚Äî development, operations support, troubleshoot, plan the expense, and also tool up (find/develop tools) your new infrastructure.

Eventually, someone in your organization must possess the knowledge of what you have, how it operates. This means a higher skill-set of personnel, which are not trivial to hire. So while the \# of Operation people may slightly decrease, you may still find yourself paying the same price if not more.

Another noticeable item is service/licensing fees (for example EMR), which can become costly, at scale. Without careful planning/modelling, you will find very quickly that you‚Äôre paying more for the actual service license rather than the compute resources you‚Äôre using.

## \#10 ‚Äî ‚ÄúCloud will finally allow us to have No-Ops‚Äù

‚ÄúNo-Ops‚Äù is a great buzzword coined to a situation when a business can totally opt-out of the operational expertise of 3rd party service. For some companies‚Äô needs, it is satisfactory to have pretty thin operations teams, unfortunately, this is totally untrue for Data-Intensive companies.

You will still require someone to integrate and duck-tape all the systems, benchmark these systems, automate provisioning, provide meaningful visibility and respond to alerts, the role of Data Operations simply shifts to the higher stacks and by no means disappears\!

To conclude, while migration of your Data Processing pipelines to the Cloud can obviously bring multiple undeniable benefits, the migration process has to be thoroughly planned and executed with all of the above points taken into account. Plan ahead and don‚Äôt get caught unprepared.

# Transforming Asset Management: A Journey to a Centralized 3D Content Management Platform

## Introduction

‚Ä¢ AWS

‚Ä¢ Cloud Transformation

‚Ä¢ Customer Success

November 27, 2024

## Company Background

A global software provider based in Canada is revolutionizing asset management with cutting-edge visual intelligence solutions. Their flagship product empowers capital-intensive industries to digitally transform operations with unlimited, data-enriched 2D and 3D visualizations. By bringing physical assets online, the company enables teams to improve productivity, collaborate seamlessly, and manage risk from anywhere, ensuring business continuity and operational efficiency.

## The Challenge

The company‚Äôs platform excelled in visualizing and analyzing 3D data but lacked an intuitive solution for uploading, managing, and processing 3D assets. Customers faced significant manual effort in preparing and integrating data, leading to inefficiencies and resource strain. Furthermore, the decentralized nature of customer data made maintaining security and accessibility a challenge. To address these issues, the company sought to build a robust 3D content management platform leveraging AWS Visual Asset Management System (VAMS), aligning with tools already used by one of their largest clients. Their ultimate goal was to enhance customer experience while creating opportunities for business growth.

## Our Solution

OpsGuru collaborated with the company through its Virtual Teams engagement model to design and implement a scalable solution. By augmenting the internal team, OpsGuru ensured focused execution of the project backlog to develop a new 3D content management platform. The resulting solution streamlined the upload, ingestion, and management of 3D assets, centralized data handling, enhanced security with access controls, and integrated seamlessly with the visualization platform for real-time insights. The platform utilized serverless technologies to deliver scalability and minimize operational overhead.

## The Result

The company launched a modernized 3D content management platform that addressed customer challenges and unlocked new capabilities:

‚Ä¢ Simplified Onboarding: Deployment through AWS Marketplace enabled transparent, on-demand cost visibility.

‚Ä¢ Comprehensive Asset Management: Customers can now upload, download, preview, and modify 3D assets effortlessly.

‚Ä¢ Improved Data Security: Advanced access controls restrict data to authorized users only.

‚Ä¢ Seamless Integration: Real-time ingestion of assets into the visualization platform enhances insights and collaboration.

The new platform improved operational efficiency and delivered a superior customer experience, positioning the company as a leader in visual asset management.

# What does a Service Mesh do? (Part 2\)

## Introduction

‚Ä¢ Cloud Native

This is the second post in our Service Mesh series. Where the first post introduced service meshes at a high level, this one will drill down further into common service mesh features and the problems they address.

Understanding the problems addressed by a service mesh is key to determining their suitability for your systems. However, it‚Äôs not until we uncover these system issues that we can begin to identify the trade-offs involved in adoption. In broad strokes, service meshes have three fundamental benefits:

1\. Mitigate network failures

2\. Improve security

3\. Minimize maintenance costs

## Mitigates Network Failures

Network communication can fail in a number of ways. It may be due to an error generated by the remote service, or failure in the infrastructure. Overloaded networks may also result in latency spikes and bandwidth contention. Whatever the cause, applications should handle these issues gracefully. The worst response is for the application to enter a dysfunctional state that masks the underlying error.

Service Meshes implement the most common strategies for enabling graceful degradation in the event of communication errors, namely timeouts, retries, and circuit breakers.

## Timeouts

Every request to a remote service must have an explicit timeout. Without it, your application may wait indefinitely for a response from a failing or overloaded upstream service. It is one of the simplest defensive measures, but also one of the most easily neglected. Service meshes allow timeouts to be specified as simple configuration rather than being baked into each application.

## Retry Policies

In distributed systems ‚Äì specifically, applications built with microservices ‚Äì it is quite common for failures to be transient. For example, when calling a load-balanced upstream service, the first request may fail due to errors on one service instance while a subsequent request would be successfully routed to a different instance immediately. The logic for retrying a request is implemented transparently within the mesh so that applications code remains uncluttered by transient error handling.

## Circuit Breakers

Circuit breakers are one of the more complex failure handling strategies and should be used with care. They allow you to define conditions under which no further connections will be sent to a given host/instance on the mesh. For example, you could set a limit on the number of concurrent connections to a host to prevent overloading. Another example is to specify the number of consecutive error responses that cause a host to be proactively removed from load balancing.

## Improves Communication Security

Due to the way service meshes address network reliability concerns, their architectures lend well to layering on additional cross-cutting functionality. Security features may be among the most critical of those value-add features. Described below is how the security features ‚Äì beginning with public-key infrastructure to application-tier authorization and authentication policies ‚Äì quickly develop to provide an in-depth defence.

## PKI and Workload Identities

Public Key Infrastructure (PKI) may seem like the least exciting feature of a service mesh, but it is one that most security features depend on. While the exact implementations differ between meshes, PKI allows cryptographic identities to be dynamically generated for workloads and distributed via certificates. These certificates then provide the basis for encrypting, authenticating, and authorizing communication within the mesh. Due to their dynamic lifecycles, certificates are automatically rotated by the mesh control plane ‚Äì a recommended security practice that can be painful to implement without a team.

## Mutual TLS

An extension of the PKI features is to encrypt communication between workloads with the same certificates and keys used to verify their identities. Mutual TLS (mTLS) not only ensures that traffic is encrypted (‚Äúone-way‚Äù TLS) but also provides a two-way authentication mechanism that can ensure there is communication within a legitimate workload. A service mesh removes the complexity of managing the TLS handshake process within the application code. Instead, microservices can simply address communication in plaintext, and the service mesh will transparently encrypt and decrypt the traffic between workloads.

## Authentication and Authorization Policies

Another advantage of having cryptographic workload identities is that those identities can be used to restrict access patterns to microservices. Most service meshes also support alternate authentication mechanisms, like JWT tokens ‚Äì which are typically used for user-based authentication. Regardless of authentication methods, once the source of a request has been identified, it can be compared to authorization policies for the microservice. These authorization policies typically provide rich control over which HTTP methods. Paths may be accessed on the service for layer 7 traffic or layer 4 traffic. Essentially, the capabilities vary by service mesh, but restricting application-tier traffic is a valuable tool for achieving defence in depth.

## Minimizes Maintenance Costs

The complexities introduced by microservice architectures lead to a number of new costs in the development and operations lifecycles. The increased coordination required when deploying interdependent microservices results in a multiplication of failure modes and, therefore, more planning efforts and an increased risk of outages. These issues are further exacerbated when coordination must occur across multiple and/or hybrid environments.

Adding insult to injury, modern-day redundancy typically means running microservices across two or more failure domains (e.g. availability zones) and traffic that crosses failure domains incurs transit costs.

Fortunately, service meshes provide mechanisms to reduce or eliminate all of these costs; namely traffic shifting, multi-cluster support, and topology-aware routing.

## Traffic Shifting/Splitting

Traffic shifting, or traffic splitting, typically refers to the ability to flexibly route traffic between two versions of the same workload. This provides significant risk mitigation for microservice deployments with dependencies from other services. It enables practices like canary testing, where a new version receives only a small fraction of requests until it is proven stable, and blue-green deployments. Orchestrating these practices outside of a service mesh is time-consuming and error-prone. The effort required increases exponentially with the number of microservices involved. Service meshes make these operations relatively trivial at any scale, providing a declarative language for routing traffic based on workload labels.

## Multi-Cluster and Hybrid Cloud Deployment

Multi-Cluster and Hybrid Cloud support is a featureset most service meshes have well underway ‚Äì though most are still being iterated on. While some meshes, like Linkerd and Istio, were designed with specific underlying infrastructure in mind (e.g. Kubernetes), others, like Consul, have been platform agnostic from the start. Regardless of initial constraints, most service meshes attempt to solve the problems of service discovery, load balancing, and failover across clusters and cloud providers. While your journey may vary within the current state of these features, attempting to design an equivalent solution from scratch is generally a non-starter.

## Topology-Aware Load Balancing

Network transit fees may be one of the most stealthy costs when it comes to modern microservice deployments. East-west traffic increases exponentially with the number of microservices. It is also common for microservices to be spread across availability zones for example, if load balancing algorithms are unaware of the underlying zones, at least 50% of your requests will end up incurring transit fees. Furthermore, these costs are amplified by the number of microservices involved in processing a request.

Consider a simple application made up of three microservices each with three instances, spread evenly across three zones. If service A receives incoming requests and passes them to service B for processing, which in turn passes to service C for processing for every 90 requests received by service A, you will incur egress charges for 120 requests worth of data. Each instance of service A has a two-thirds chance of routing to an instance of service B in a different zone and service B has the same chances of routing to a different zone. Therefore at each ‚Äúhop‚Äù of the transaction, 60 of 90 requests will be sent across zone boundaries.

In order to prevent this explosion of costs, some service meshes, like Istio, provide topology-aware load balancing. This makes routing decisions based on available failure domain (e.g. availability zone) labels. The most common configuration is to have all traffic routed to service instances within the same zone when possible. Only if nodes in the current zone become unhealthy, will traffic be routed to another zone. This provides a good balance between cost optimization and resiliency that is difficult to achieve on most platforms. (However, as of v1.17, Kubernetes also provides this functionality with Service Topology, though it is currently in alpha).

## Observability

As previously mentioned, microservice architectures come at the cost of complex application topologies. It is common for a single web transaction to result in dozens of calls within the system, as front-end services fan-out to multiple tiers of backends. Diagnosing failures requires a detailed view of exactly which services failed or incurred latency. Fortunately, service mesh proxies are ideally located in the mesh to provide exactly that information. Linkerd, Envoy, and Consul proxies all contain rich telemetry and tracing subsystems that yield detailed insight into network performance, application behaviour, and communication patterns. Armed with this data, it is possible to see exactly which microservices were called in a transaction, which calls resulted in errors, and how long each call took to complete. Not only is this level of visibility essential to microservice architectures, but it is costly to implement yourself.

## Summary

Whilst there is a lot of hype around service meshes, there are also very good reasons to adopt one. Obviously, there are always trade-offs, but hopefully, this post has clarified reasons to begin evaluating some of the products out there. In a future post, we will provide guidelines to evaluate trade-offs and compare available solutions.

# Strategies for AWS Cost Optimization in 2025

## Introduction

With a volatile economy and global tensions driving even more uncertainty, getting the most out of your cloud investment isn‚Äôt just smart‚Äîit‚Äôs essential. Cloud computing delivers enormous flexibility and power, but without proper cost control, it can quietly drain your budget. This guide offers a strategic roadmap to cloud cost optimization, designed to help you reduce cloud spending while maximizing value and driving innovation.

Key Takeaways:

‚Ä¢ Understand Your Cloud Bill: Use tools like AWS Cost Explorer to analyze spending, uncover hidden costs, and manage your cloud budget with precision.

‚Ä¢ Align Cloud Spend with Business Goals: Optimize for value, not just savings‚Äîensure every cloud dollar supports performance, resilience, and innovation.

‚Ä¢ Automate Waste Elimination: Cut costs fast by automating the removal of idle resources with AWS Config and Lambda.

‚Ä¢ Foster Cost Accountability: Empower teams with real-time cloud cost monitoring tools and embed cost responsibility into everyday decisions.

## Decode Your Cloud Bill and Uncover Hidden Savings

The first step in any cloud cost optimization framework is understanding where your money is going. A cloud bill isn‚Äôt just a line item; it‚Äôs a roadmap to smarter spending.

Go beyond the bottom line and analyze usage trends. Tools like AWS Cost Explorer and AWS Cost Management help you identify high-cost services, forecast future spend, and set up cloud budget alerts to avoid surprises.

Focus on these key areas for potential savings:

‚Ä¢ Storage: Optimize S3 storage classes, delete unused data, and implement lifecycle policies.

‚Ä¢ Databases: Right-size instances, leverage reserved instances, explore managed services, automate backups, and optimize queries. Consider Multi-AZ vs. Single-AZ deployments.

‚Ä¢ Networking: Secure and optimize network configurations and utilize private IPs efficiently.

‚Ä¢ Compute: Right-size instances, implement autoscaling, and optimize workload configurations.

‚Ä¢ Workloads: Design workloads for scalability and cost-efficiency.

‚Ä¢ Environments: Control non-production environment spending.

‚Ä¢ Licensing: Optimize licensing costs with committed use discounts and correct license types.

‚Ä¢ Governance: Implement governance for R\&D environments to prevent runaway costs.

Mastering these areas helps reduce AWS cloud costs, enhance efficiency, and ensure every dollar works harder.

## Define Cloud Value: Align Spending with Business Goals

True cloud cost optimization goes beyond cutting: it‚Äôs about maximizing value. Define what matters most to your business and align your cloud resources accordingly.

## Map Workloads to Resources and Spend with Purpose

Connect spending to specific workloads. For each one:

1\. Define its business purpose.

2\. Assign ownership and cost responsibility.

3\. Identify creation date and relevance.

4\. Apply detailed tagging for clarity.

This focuses your cloud cost monitoring efforts where they matter, helping you manage spending proactively.

Consider these key areas:

1\. Uptime & Availability: Balance availability needs with cost considerations.

2\. Resilience & Disaster Recovery: Choose a cost-effective disaster recovery strategy aligned with business needs.

3\. Performance & Scalability: Select cost-effective instances and services, leveraging autoscaling.

4\. Security & Compliance: Implement efficient security measures with managed services.

5\. Innovation & Agility: Explore cost-effective experimentation and prototyping.

6\. Cost Optimization Priority: Define the importance of cost optimization relative to other factors.

A clear understanding of your cloud values allows for smarter, data-driven decisions.

## Eliminate Waste and Automate Efficiency

One of the fastest ways to reduce AWS cloud costs is by eliminating waste‚Äîresources you‚Äôre paying for but not using. Common culprits include idle EC2 instances, unattached EBS volumes, obsolete snapshots, and unused elastic IP addresses. These often go unnoticed, quietly inflating your cloud bill month after month.

Unused resources like idle instances, unattached storage, and dormant IPs are silent budget killers‚ÄîAutomate cleanup with AWS Config, AWS Lambda, or third-party tools.

## Continuous Monitoring: The Key to Ongoing Optimization

Cloud cost optimization isn‚Äôt a ‚Äúset it and forget it‚Äù game. It requires continuous monitoring and refinement. Even the best-laid plans can unravel without ongoing visibility into usage and spending.

Focus on these key areas for effective monitoring:

1\. Resource Utilization: Track CPU, memory, network, and storage I/O.

2\. Cost Tracking: Monitor costs by service, account, workload, and user.

3\. Operational Overhead: Automate routine tasks to reduce costs and improve efficiency.

4\. Value Assessment: Regularly evaluate workload cost-benefit and explore alternatives.

5\. Reality Check: Compare actual costs to budget and forecasts.

This level of cloud cost monitoring helps you stay proactive rather than reactive. With forecasting tools, you can compare current spend to your cloud budget and avoid budget overruns. Set threshold alerts to catch overspending early, and run monthly reviews to spot trends before they become problems.

## Data-Driven Optimization is a Three-Stage Lifecycle

Smart cloud cost optimization follows a lifecycle: Provisioning, Optimization, and Deprovisioning. By managing each stage intentionally, you ensure efficiency from start to finish.

1\. Provisioning: Start with the right resources. Choose appropriately sized instances, opt for AWS Reserved Instances or Savings Plans for predictable workloads, and explore managed services that reduce operational overhead. Commit to only what your team truly needs.

2\. Optimization: Continuously refine your environment. Right-size over-provisioned instances using AWS Trusted Advisor, switch storage tiers for cost-efficiency, implement autoscaling, and modernize applications with AWS Lambda or container-based architectures. This is where cloud resource optimization delivers serious ROI.

3\. Deprovisioning: Unused resources are budget killers. Automate decommissioning with AWS Config rules and Lambda functions. Remove old snapshots, delete unattached volumes, and terminate test environments no longer in use.

Using this lifecycle model ensures that resources and money are never wasted. It aligns technical decisions with financial accountability and fosters ongoing cloud cost control.

## Key Optimization Techniques

Here are some key optimization techniques to apply within this lifecycle:

‚Ä¢ Automated Idle Resource Termination: Use AWS Config and Lambda to automate resource deprovisioning.

‚Ä¢ Right-Size Over-Provisioned Resources: Leverage AWS Trusted Advisor for recommendations.

‚Ä¢ Modernization: Modernize applications with serverless technologies like AWS Lambda and Aurora Serverless.

## Build a Culture of Cost Responsibility

Technology won‚Äôt optimize costs on its own‚Äîyou need a culture that supports it. Cloud cost management should be part of daily operations, not a side project.

Start by assigning clear ownership of cloud budgets at the team or project level, and equip teams with tools like AWS Organizations, SCPs, and AWS Service Catalog to enforce standards and prevent overspending. Empower developers and engineers to treat cloud resources like real business expenses by providing training and real-time cloud cost monitoring dashboards.

Collaboration is key. Finance, engineering, and operations teams should work together to regularly review spending, celebrate wins, and address inefficiencies. Fostering a culture of cost responsibility isn‚Äôt about cutting corners‚Äîit‚Äôs about ensuring your cloud billing reflects value, not waste, and freeing up resources to drive innovation.

# Portfolio+ : Accelerating the Pace of Innovation Through Cloud Transformation

## Introduction

When you‚Äôre a company committed to the cloud transformation journey, one of the best ways to set yourself up for success is to find a strong partner.

Portfolio+ is a financial technology company offering a trusted core banking platform that includes applications for loans, mortgages, term deposits, investments, and retail banking, as well as an API for open banking integrations and the future of open finance.

Having provided financial services solutions to banks and financial institutions for over 30 years, Portfolio+ knew that in order to meet the rapidly changing needs of the industry, it needed to address critical technology quickly.

With its new CTO Steven Thomas on the hunt for a different solution and a new partner to help with the company‚Äôs digital transformation, OpsGuru rose to the challenge.

We sat down with Steven to discuss how the two companies are working together to drive results and the factors that have contributed to the strong partnership and achievements towards Portfolio+‚Äôs cloud transformation vision.

## Portfolio+ Partners with OpsGuru for Cloud Transformation

Portfolio+ had received extensive feedback that it needed to move towards cloud transformation in order to help its customers prepare for the future of banking in Canada.

While Portfolio+ was working with a vendor to manage its previous cloud environments, gaps were becoming apparent. It had multiple environments in AWS; however, the organization didn‚Äôt have direct visibility into those accounts.

Ultimately, the strategy of having third-party vendors wasn‚Äôt working. Portfolio+ wanted a solution that provided more oversight and increased control of the hosting platform.

In addition, Portfolio+ was challenged with excessive lead times in provisioning new customer environments under its current vendor‚Äîsomething that impacted customer experience and stressed operations. The company knew that with disciplined engineering, these tasks could be largely automated to reduce timelines by orders of magnitude.

If Portfolio+ could find a better solution, it could also reduce some of the costs it was incurring with its previous vendor. As the engagement was with a managed environment reseller, costs were opaque and high despite the lean service delivery model.

Since the Portfolio+ team had previously discussed potential solutions with OpsGuru, the company decided to reach out to see how they could help.

## A Journey of Incremental Improvements

With many providers offering the same type of tech solutions, Portfolio+ was also looking for what the people on the OpsGuru team could bring to the table.

Agility would be critical to the project‚Äôs success, so Portfolio+ wanted to ensure a high level of engagement‚Äîwhich OpsGuru delivered in spades. Ultimately, Portfolio+ was confident it could trust OpsGuru to act as a true extension of its team.

Portfolio+ worked closely with the OpsGuru team to clearly define the statement of work and the vision of what Portfolio+ wanted to achieve, which included:

‚Ä¢ Gaining visibility into the cloud environment.

‚Ä¢ Assuming ownership of the provisioning process.

‚Ä¢ Leveraging automation to reduce lead time for provisioning customers.

‚Ä¢ Modernizing core practices and infrastructure.

‚Ä¢ Providing a self-service platform to developers to minimize toil for the operations team.

Additionally, Portfolio+ also expressed a strong preference for incremental improvements versus ‚Äúbig wins‚Äù with a focus on establishing a trusted partnership. The approach focused on exercising existing tools. This would allow them to figure out what was working and implement solutions addressing any gaps.

From there, the two organizations could work together to design and build a new automated system for rolling out infrastructure.

## OpsGuru Empowers Portfolio+ with Iterative Transformation

Through its collaboration with OpsGuru, Portfolio+ has achieved several wins that have helped propel the organization toward its goals for digital transformation.

\#1. Iterative Approach and Multifaceted Improvements

Portfolio+ embraced an iterative approach to its transformation journey, focusing on application automation, environment provisioning, and security. OpsGuru helped the company successfully modernize its existing SIEM tooling with Lacework, a risk and compliance tool. The iterative process addressed the company‚Äôs specific needs and helped optimize its operations.

\#2. Self-Sufficiency and Continual Support

One of the project‚Äôs primary objectives was to achieve self-sufficiency. While OpsGuru provided invaluable expertise, the goal was to empower Portfolio+ to drive its operations.

OpsGuru‚Äôs Customer Reliability Engineering (CRE) program was pivotal. The CRE program ensured continual support and proactive infrastructure monitoring, allowing Portfolio+ to focus on its core business without being burdened by operational complexities.

\#3. IT Systems Migration and Modernization

OpsGuru and Portfolio+ embarked on a comprehensive migration of its IT systems. By leveraging OpsGuru‚Äôs MAP assessment program, Portfolio+ gained valuable insights into the feasibility and costs of migrating its legacy infrastructure to the cloud.

This step allowed Portfolio+ to make informed decisions about system migration and modernization. While this aspect is still a work in progress, it represents a crucial step toward future optimization.

\#4. Agile and Responsive Collaboration

OpsGuru demonstrated an agile and responsive collaboration model. The Engineering on Demand (EoD) approach and two-week sprints enabled quick pivots and adjustments to accommodate new priorities, such as onboarding a significant customer.

The ability to adapt swiftly to evolving circumstances ensured that Portfolio+ remained on track and responsive to market demands.

\#5. Enhanced Security and Compliance

Security and compliance are paramount in the financial industry, and Portfolio+ recognized the importance of maintaining a robust posture. The move to Lacework provided transparency into their security posture.

By promptly and proactively addressing compliance concerns, Portfolio+ gained peace of mind and enhanced its credibility with customers.

One achievement, in particular, came just months into the project. With all OpsGuru‚Äôs work to shore up the automation process, including rewriting much of the Infrastructure as Code (IaC), the first new customer could come on board.

While the two teams had worked together to create the new approach, they had anticipated some bugs would need to be worked out. However, trialing the new process enabled them to improve the provisioning process significantly.

In nine short months, Portfolio+ could not only sever ties with its previous vendor, but it could also pull everything back in-house.

With OpsGuru‚Äôs assistance in cloud transformation, Portfolio+ can now rely on its expert team for ongoing cloud administration and architecture.

## Achieving Cloud Transformation Through Partnership

Portfolio+‚Äôs work with OpsGuru over the past year illustrates the importance of finding partners aligned with your vision and establishing a trusted relationship.

Partnerships like the one that Portfolio+ established with OpsGuru foster efficiency and enable both parties to overcome challenges together. Regularly evaluating the partnership‚Äôs alignment with project goals is crucial, ensuring the roadmap and project charters remain in sync.

Also, AWS was a critical ally in this partnership. It plays an active role in holding partners accountable and facilitating new relationships. The financial support provided by AWS through its funding programs has been instrumental in driving the initiatives undertaken by Portfolio+ and OpsGuru.

Leveraging the resources and support offered by AWS actively contributes to the success of these types of transformational projects.

These fundamental principles and lessons can guide organizations embarking on their cloud transformation journeys, empowering them to succeed and drive meaningful changes in their operations and customer experiences.

As Portfolio+ continues to work closely with OpsGuru, it is poised for further growth and success in cloud banking and the evolving landscape of IT operations.

# OpsGuru Achieves Amazon API Gateway Service Delivery Designation

## Introduction

‚Ä¢ AWS

OpsGuru is proud to announce that it has achieved the Amazon API Gateway Service Delivery designation, demonstrating our expertise in leveraging APIs at any scale to enhance our customers‚Äô security, scalability, and efficiency.

The API Gateway Service Delivery Program distinguishes AWS Partners who have demonstrated deep technical knowledge, extensive experience, and a consistent track record in deploying Amazon API Gateway services to customers.

Amazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. It acts as a ‚Äúfront door‚Äù for applications to access data, business logic, or functionality from backend services.

Being a part of this elite group of AWS Partners highlights our proficiency in AWS services and opens up unique collaborative opportunities with AWS so we can streamline API management, bolster security, and improve performance and scalability for our valued customers.

In addition to the Amazon API Gateway Service Delivery designation, OpsGuru also holds AWS Lambda Service Delivery, AWS Resilience Services Competency, AWS DevOps Competency, AWS Migration Competency, AWS SaaS Competency, Microsoft Workloads Competency, AWS Networking Competency, Amazon EC2 for Windows Server Delivery, Amazon EC2 for Windows Server Delivery and the Amazon EKS Delivery.

Contact us today to learn more about how OpsGuru can build responsive, event-driven systems to enhance your business outcomes.

## 

‚Ä¢ AWS

# OpsGuru Achieves the AWS Lambda Service Delivery Designation

## Introduction

‚Ä¢ AWS

OpsGuru is proud to announce that it has achieved the Amazon Web Services (AWS) Service Delivery designation for AWS Lambda.

AWS Lambda is a groundbreaking, event-driven, serverless computing platform that allows developers to run code without the need to manage or provision servers, handling the computing resources automatically.

Achieving the AWS Lambda designation demonstrates OpsGuru‚Äôs deep expertise and experience in developing and deploying scalable, event-driven, serverless applications using AWS Lambda. This recognition also further strengthens our position as a leader in AWS solutions and sets the stage for further innovations and success stories.

Through AWS Lambda, we‚Äôve driven exceptional, transformative solutions for our customers, showcasing our ability to create responsive, event-driven systems that adapt to real-time business needs, significantly reducing operational overhead and enhancing overall performance.

‚ÄúWe are proud to receive this recognition that is a testament to our team‚Äôs unwavering commitment and deep expertise in leveraging AWS Lambda to develop scalable, serverless, and event-driven solutions. Through AWS Lambda, we empower our customers with applications that are highly efficient, allowing them to focus on innovation and growth while we handle the complexities of server management and scalability.‚Äù

Alan Williamson | Vice President, Engineering @ OpsGuru

In addition to the AWS Lambda Service Delivery, OpsGuru also holds the AWS Resilience Services Competency, AWS DevOps Competency, AWS Migration Competency, AWS SaaS Competency, Microsoft Workloads Competency, AWS Networking Competency, Amazon EC2 for Windows Server Delivery, Amazon EC2 for Windows Server Delivery and the Amazon EKS Delivery.

Contact us today to learn more about how OpsGuru can build responsive, event-driven systems to enhance your business outcomes.

## 

‚Ä¢ AWS

# 5 Key Steps for a Successful AWS Cloud Migration

## Introduction

Did you know that migrating to Amazon Web Services (AWS) results in an average 62% increase in infrastructure management efficiency? (IDC) Or that organizations who migrate to AWS save an average of 31% in infrastructure costs? (IDC) With impressive statistics like these, it‚Äôs no wonder that organizations across Canada are migrating and modernizing their infrastructures, databases and applications on AWS.

However, cloud migration can come with challenges and many organizations fall victim to common pitfalls along the way. In order to improve your chances of a successful migration project, there are a few key areas in which organizational alignment is essential, especially for large-scale migrations.

If you‚Äôre considering a migration to AWS, take a look at the following steps which briefly outline the areas that require organizational alignment during the AWS migration process. At the end of this guide, you will learn more about how OpsGuru can help you overcome these challenges to de-risk and accelerate your migration process.

## 1\. Know Your Estate

The first step of your migration process involves developing a workload inventory that includes all the virtual machines and servers to be considered for migration. Without a clear understanding of the scope of the migration, you‚Äôre likely to underestimate your migration budget and timeline.

OpsGuru Activated Solution: Workload Inventory

## 2\. Understand Your Licensed Liabilities

The next challenge is understanding the licensing costs or operating limitations of the third-party software (such as Oracle, Windows, SAP, and OpenText) governing your system or applications. Licenses and commercial agreements aren‚Äôt always straightforward but a failure to understand them can cost your organization time and money.

OpsGuru Activated Solution: Optimization and License Assessment (OLA)

## 3\. Architect for Where You Are and Where You Want to Be

When engineering your cloud architecture, it‚Äôs crucial to engineer a right-size solution that is tailored to your current business needs and future business goals. Overengineering your solution can be unnecessarily costly for your organization.

OpsGuru Activated Solution: Migration Readiness Assessment and High-Level Planning

## 4\. Understand the True Cost of Ownership

Next, understanding the true cost of your migration based on your expected utilization of resources is important to help you budget accordingly. This might include the following costs in addition to your cloud invoices:

‚Ä¢ Compute and storage costs

‚Ä¢ Network costs

‚Ä¢ Labour costs

‚Ä¢ Support costs

‚Ä¢ Operating system costs

Without a clear understanding of the true cost of ownership, cost optimization can be difficult and cost overruns are a risk.

OpsGuru Activated Solution: Total Cost of Ownership Analysis / Right Pricing

## 5\. Execute the Migration Strategy

Finally, it‚Äôs time to execute your AWS cloud migration strategy. In this step, it‚Äôs important to align your migration strategy with your business to maintain momentum throughout the migration; continuing alignment and making the necessary investments in foundations. A migration strategy that doesn‚Äôt account for these items is likely to fail, due to misalignment of your team‚Äôs capabilities and operational practices.

OpsGuru Activated Solution: Migration Acceleration Program (MAP) ‚Äì Assess, Mobilize, Migrate & Modernize

**How to Overcome the Challenges of a Successful Migration**

Cloud adoption comes with many benefits including higher scalability, flexibility and business agility. It can also help you boost innovation and drive your organizational transformation to meet changing demands.

Although organizational alignment is a critical first step of your AWS migration journey, it‚Äôs also essential to have an AWS Consulting Partner by your side. As you‚Äôve seen in the previous sections, each of these cloud migration steps comes with its own challenges. That‚Äôs why having a partner to help you throughout your cloud migration journey is key.

As the AWS 2021 Canada Consulting Partner of the Year, OpsGuru has experience with multiple successful migrations and we‚Äôre proud to offer proven solutions to help you leverage best practices and take full advantage of the AWS environment.

In our new ebook, Building the Business Case for AWS Migration, we provide valuable insights on how you can leverage OpsGuru‚Äôs activated solutions to meet the challenges of successful migration and unlock the power of AWS for your business. Take the first step towards a successful migration journey and download the ebook today.

# OpsGuru Awarded AWS Canada Consulting Partner of the Year

## Introduction

‚Ä¢ AWS

Today, OpsGuru has received the 2021 Canada AWS Consulting Partner of the Year Award from Amazon Web Services, Inc (AWS). This award is given to a high-performing AWS Consulting Partner who has delivered consistently well throughout 2020, helping customers drive innovation and build solutions on the AWS Cloud.

Every year, AWS recognizes a handful of AWS Partners in the Amazon Partner Network (APN) whose business models have embraced specialization and collaboration and continue to evolve and thrive on the AWS Cloud. Earning this award is a testament to OpsGuru‚Äôs advanced AWS expertise.

As a highly distinguished AWS Advanced Consulting Partner, OpsGuru provides cloud consulting services and specializes in architecting and deploying solutions using cloud-native technologies. Year over year, OpsGuru has grown its team of cloud-certified professionals by 306% and revenues by 400%, along with helping hundreds of customers leverage the AWS platform.

‚ÄúWe are proud to be awarded this AWS Partner Award along with the other extraordinary winners and industry leaders who help businesses across the world drive innovation on AWS,‚Äù said Anton Mishel, CEO at OpsGuru. ‚ÄúAt OpsGuru, we are committed to continuing the advancement of our AWS competencies and capabilities to help our customers in Canada and across the world unlock the power of AWS, develop critical solutions to their challenges, and thrive on the AWS Cloud.‚Äù

What This Means for OpsGuru Customers

‚Äì The 2021 Canada AWS Consulting Partner of the Year Award demonstrates OpsGuru‚Äôs commitment to the APN Program and AWS customers.

‚Äì Prospective customers can ensure they are choosing an industry leader with proven advanced expertise in the AWS platform and cloud-native technologies.

About OpsGuru

OpsGuru is a cloud consulting partner that delivers cloud adoption, application modernization, Kubernetes enablement, cloud security, and data analysis services to customers across the world. OpsGuru has empowered hundreds of customers to successfully adopt cloud technologies and achieve success on cloud platforms.

Contact us to learn more about how OpsGuru can help you start your digital transformation\!

# OpsGuru Awarded AWS Canada‚Äôs Migration & Modernization Partner of the Year

## Introduction

‚Ä¢ AWS

Cloud consulting company, OpsGuru has received the AWS Canada Migration & Modernization Partner of the Year Award at the AWS Toronto Partner Summit 2022\. Every year, AWS recognizes high-performing partners in the AWS Partner Network (APN) who have demonstrated outstanding results, helping customers drive innovation and build solutions on the AWS cloud.

This award establishes OpsGuru as the Canadian leader in AWS migrations and modernization and recognizes the company‚Äôs success in helping hundreds of customers accelerate cloud initiatives and incite digital transformation, achieving cost savings, staff productivity, and operational and business agility.

‚ÄúWe are honoured to be recognized for our proven expertise in helping customers meet the challenges of cloud programs, de-risk the migration process, and navigate their modernization journey. As always, our team is committed to accelerating cloud-native technology adoption by leveraging the agility, breadth of services, and innovation afforded by the exceptional AWS platform,‚Äù said Alan Williamson, VP of Engineering at OpsGuru.

As a leader in navigating the AWS Migration Acceleration Program, OpsGuru helps customers accelerate their cloud journey while leveraging a streamlined process and obtaining incentives which significantly increase the likelihood of a successful AWS program.

‚ÄúBeyond technical implementations, OpsGuru delivers consulting on critical transformation topics such as organizational change management, upskilling, training and adoption of agile methodologies,‚Äù said Mency Woo, VP of Enterprise Transformation at OpsGuru. ‚ÄúExecutives are looking for strategic guidance and learnings from trusted advisors. OpsGuru is the perfect partner for organizations looking to transform through cloud technologies.‚Äù

In addition to the 2022 Migration & Modernization Partner of the Year award, OpsGuru received the 2021 Canadian AWS Consulting Partner of the Year award and was recently recognized as Canada‚Äôs leader in AWS cloud solutions, with world-class AWS Premier Partner Status.

Contact us today to learn more about how OpsGuru can help you on your digital transformation journey.

For more information, please read our press release.

## 

‚Ä¢ AWS

# AWS Transfer Family Service Delivery Launch Partner

## Introduction

Cloud consulting company, OpsGuru announced today that it has been recognized as an Amazon Web Service (AWS) Transfer Family Service Delivery Launch Partner. This achievement recognizes that OpsGuru follows best practices and has proven success in helping customers build sophisticated business-to-business (B2B) file exchange solutions with Transfer Family while leveraging Amazon Simple Storage Service (Amazon S3) and Amazon Elastic File System (Amazon EFS) to securely store data and the rich set of data analytics capabilities from AWS.

Achieving the AWS Transfer Family Service Delivery designation differentiates OpsGuru as an AWS Partner Network (APN) member that provides specialized technical proficiency and proven customer success to deliver cloud-native managed file transfer and B2B file exchange solutions using Transfer Family. Transfer Family‚Äôs purpose is to enable customers to fully leverage B2B data generated externally from customers‚Äô core information systems and to expand their data analytics pipelines. To receive the designation, AWS Partners must possess deep AWS experience and deliver solutions seamlessly on AWS.

‚ÄúOpsGuru continues to differentiate our AWS capabilities by receiving the new AWS Service Delivery designation for Transfer Family,‚Äù said Dave Lindon, General Manager at OpsGuru. ‚ÄúOur team is committed to accelerating cloud-native technology adoption with the market-leading AWS platform. OpsGuru and AWS customers can leverage Transfer Family to connect valuable data between disparate systems and organizations.‚Äù

In addition to the Transfer Family Service Delivery, OpsGuru holds the AWS Networking Competency, DevOps Competency, the AWS Migration Competency, AWS SaaS Competency and the Amazon EC2 for Windows Server Delivery. As a result, OpsGuru provides customers with guidance in specific solution areas like Networking, Big Data, DevOps, Migration, and AWS IoT; in vertical markets such as Financial Services, Healthcare, Life Sciences, Government, and Digital Media; and with enterprise business applications like Microsoft Workloads and SAP.

## What This Means for OpsGuru Customers

‚Ä¢ The AWS Transfer Family Delivery Program validates OpsGuru‚Äôs capabilities across the Transfer Family product suite.

‚Ä¢ AWS Transfer Family services help OpsGuru enable customers to fully leverage B2B data generated outside of customers‚Äô core information systems to expand their data analytics pipelines.

Contact us today to learn more about how OpsGuru can help you on your digital transformation journey.

For more information, please read our press release.

# Building a SaaS Offering on AWS

## Introduction

A number of factors, including multi-tenancy, high availability and cost-effectiveness, need to be considered for a successful SaaS product design. While each SaaS offering is unique, there are common design patterns that can be adopted.

In Building SaaS Offering on AWS whitepaper, the OpsGuru team covers the key attributes of a SaaS product, running applications on Amazon Elastic Kubernetes Service (EKS), multi-tenancy on datastores and pricing according to use and how to measure tenant consumption.

Interested in building a SaaS offering to join the next wave of the SaaS evolution? Download the whitepaper to learn more.

# Pitfalls to Avoid in Cloud Adoption: Ineffective KPIs

## Introduction

‚Ä¢ Advisory

‚Ä¢ Cloud Adoption

Welcome to part four of our five part series, Major Pitfalls to Avoid During Cloud Adoption.

Cloud adoption provides many benefits but before you embark on your journey, it is critical to evaluate your goals and ensure that KPIs align with those goals.

**Pitfall: Not Defining KPIs that Align with Cloud Adoption Goals**

Many organizations track the number of applications migrated to the cloud as a key performance indicator during an application migration project. However, is moving all targeted applications the real goal of cloud adoption?

Application migration is the journey to different cloud adoption goals of cost efficiency, high availability, rapid delivery, and application stability, which are the most effective KPIs to be measured.

## Solution to Pitfall 4: Define KPIs that Align with Cloud Adoption Goals

Cloud adoption is an investment. To justify and track the investment, it is important to illustrate the goals of cloud adoption with trackers in the form of relevant KPIs. For example, many organizations adopt the cloud to support high availability, then relevant KPIs will be the up-time of the application, the mean time to recovery of the application after an outage and the amount of unplanned downtime. The KPIs not only track the progress of cloud adoption, but they are also indicators of whether further investments should be made after the applications are migrated to the cloud.

On the other end of the spectrum, it is tempting to track too many KPIs, especially when so many of them represent benefits to adopting the cloud. However, too many KPIs may result in analysis paralysis on what features to invest in to maximize improvement across the maximum number of KPIs. That may lead to the pitfall of the perfect platform again. It is more effective by tracking KPI against the main bottleneck (the goal); after improvement, reexamine the system and identify the next bottleneck such that the teams can see momentum through the progressing KPI, and be continuously engaged in growing and maturing the cloud platform.

## For more of the series, check out:

Pitfalls to Avoid in Cloud Adoption: Focusing Solely on Applications

Pitfalls to Avoid in Cloud Adoption: Perfection as the Enemy of Progress

Pitfalls to Avoid in Cloud Adoption: A Siloed Approach

Pitfalls to Avoid in Cloud Adoption: Ineffective KPIs

Pitfalls to Avoid in Cloud Adoption: Spiralling Out Cloud Costs

## 

‚Ä¢ Advisory

‚Ä¢ Cloud Adoption

# Pitfalls to Avoid in Cloud Adoption: Spiralling Out Cloud Costs

## Introduction

‚Ä¢ Advisory

Welcome to part five of our five part series, Major Pitfalls to Avoid During Cloud Adoption.

We have offered a five part series on the major pitfalls to avoid during your cloud adoption journey, and our suggested solution to each pitfall. Today we‚Äôll cover one of the most hard hitting risks ‚Äì spiraling cloud costs. It is crucial to understand how cost modelling works when transitioning to cloud, where you may benefit from efficiencies, and how to leverage and manage organizational budgets.

## Pitfall 5: Spiraling Out Cloud Cost

One of the major differences between on-premises depreciable hardware and cloud platform is how costs are charged. Most of the on-premises are upfront costs that can be predictably depreciated through their lifetime, while cloud services are primarily based on the pay-as-you-go model and, therefore, can fluctuate significantly based on usage. Regardless of the amount of care in estimating cloud costs, it is rare for cloud costs to align well with projections without tremendous effort.

## Solution to Pitfall 5: Allocate Costs Based on Workload and Ownership

Cloud services on a pay-as-you-go model is one challenge, but an even more significant challenge is that each service is charged differently. As each workflow tends to be composed of different cloud services, it takes too much effort to minutely account for different cost parameters and how they roll up to the final bill.

This explains why any cost measurement will be based on allocation. On the cloud, allocation is indicated by metadata in the form of tags. Most cloud adopters are aware of tags, but the real work is in defining how the tags should be tailored for the organization.

At OpsGuru, we argue that the most important tags are the ones that align with budget allocations based on lines of business and P\&L owners. It automatically integrates well with existing account practices. Because most resources resource tens of tags, engineering teams then start to include a multitude of tags. The effort is applaudable but difficult to maintain. We, therefore, recommend lower-level tags to be based on workloads ‚Äì the fewer the tags, the better.

One other challenge about tags will be shared resources or consumption that is difficult to account for. Admittedly for such consumption, approximation is the best way to go by. A dual approach of initial IT budget plus a gratuity-based model (e.g., workload load costs $100 per month, a gratuity of 10% is automatically charged back to the workload owner to cover shared services cost) will avoid surprises.

Cloud costs can be complex for newcomers. This is where collaboration with your company‚Äôs finance and accounting team, who are the experts in allocation and budgeting, will be most handy. It is yet another indicator that cloud adoption is not only a technology exercise but an opportunity for cross-team collaboration.

## Further Reading

For more of the series, check out:

Pitfalls to Avoid in Cloud Adoption: Focusing Solely on Applications

Pitfalls to Avoid in Cloud Adoption: Perfection as the Enemy of Progress

Pitfalls to Avoid in Cloud Adoption: A Siloed Approach

Pitfalls to Avoid in Cloud Adoption: Ineffective KPIs

Pitfalls to Avoid in Cloud Adoption: Spiralling Out Cloud Costs

# OpsGuru and Alida Partner to Migrate Data to AWS Cloud for Scalability and Faster Performance

## Introduction

‚Ä¢ Customer Success

January 6, 2025

## Background

Alida, a leader in experience management, needed to modernize an application running on legacy architecture to meet its customers‚Äô growing data demands. They worked with OpsGuru to replatform the application and migrate the data to AWS Cloud using containerization and microservices. The solution improved scalability and performance, with 24 times faster access to data. The partnership model through AWS helped Alida meet its technical and business goals, and innovate faster.

## The Challenge

For Alida, a global software-as-a-service (SaaS) firm that provides a customer experience management (CXM) and customer insights platform to many of the world‚Äôs largest companies, scalability is a must.

Legacy technology worked well when Alida was in its growth phase. However, Alida‚Äôs customers were flourishing, too. Its customers, Fortune 500 companies and many of the world‚Äôs tech, banking, retail, and media giants, required exponentially more data in and out of Alida‚Äôs Total Experience Management (TXM) platform as they ran more customer surveys and gathered more feedback.

Alida‚Äôs internal testing demonstrated that its architecture was nearing its limits in terms of scalability and performance. ‚ÄúIt was set up for a different era, different technologies, a different business, really,‚Äù says Joseph Finneran, VP Engineering at Alida. The company needed a solution that quickly scaled to meet the enormous data analytics needs of its customers, with speed, stability, and simplicity.

AWS Plays Matchmaker for Data Migration

Alida took part in an AWS program to incentivize migration from legacy systems to AWS Cloud using trusted AWS partners. AWS recommended cloud consulting partner, OpsGuru, for the job. ‚ÄúWe were excited for the opportunity to help Alida,‚Äù says Simon Villiard, Cloud Software Architect at OpsGuru, and leader of the organization‚Äôs cloud-native development practice.

In February 2022, the companies brought their teams together to discuss needs and to determine system requirements. OpsGuru quickly familiarized itself with Alida‚Äôs pain points and Villiard created a road map that focused on where OpsGuru could add value to Alida‚Äôs infrastructure.

Before entering the partnership, Alida was impressed with the skillset and capabilities of the OpsGuru team. It valued OpsGuru‚Äôs vast prior experience helping Independent Software Vendors (ISVs) and SaaS customers modernize, scale, and drive greater efficiency on AWS. However, like many ISVs, Alida was wary about working with an external provider. Would they make a good fit? How quickly would they get up to speed?

Those concerns faded as OpsGuru‚Äôs breadth of experience came to the fore. ‚ÄúA few hours here and there explaining things,‚Äù says Scott Velez, Senior Architect at Alida, ‚Äúand OpsGuru hit the ground running.‚Äù

## Our Solution

Replatforming to AWS for performance and scalability

The project centered on an application called ‚ÄúCommunity‚Äù that Alida ran on a Microsoft Windows server and a Microsoft SQL Server database. Alida customers used the application to get feedback from users of their products and survey results to generate rich insights.

Alida wanted to improve performance and scalability by modernizing the application and replatforming to AWS Cloud. The Microsoft SQL Server could not scale out horizontally to meet growing customer needs. To address the issue, it needed to move its workloads to a new database. The teams decided to update the database technology, deploy microservices to access the data, and run it on the AWS Aurora Postgres database. OpsGuru accessed AWS Database Freedom for migration advice and support and used License Mobility to manage licensing across the migration.

The data migration was central to the replatforming and OpsGuru was instrumental to that task. The strategy was to rewrite much of the old system in Node.js, containerize it in Alida‚Äôs cloud-based Kubernetes platform, and target the Postgres database. The teams considered a range of data migration tools. In the end, OpsGuru wrote a customized data synchronization code to deploy the application in AWS Cloud.

Migration in just four days, with 24 times faster data access

The data migration moved over 800 million records from SQL Server into the Postgres database. Alida needed 3.5 weeks to complete the same process with its previous data synchronization tool. The new tool cut down the migration time to just four days.

Once the data was in the Postgres database, the teams performed a data shard to separate each customer in its system and achieve better indexing and performance. Now, when new customers join the system, Alida can take advantage of the containers written by OpsGuru and the horizontal scaling that Kubernetes provides, and spin up additional containers quickly and efficiently to handle the load.

The migration markedly improved speed and performance. Customers can access data 24 times faster in Community using the new microservices with APIs.

Alida now has access to on-demand compute capacity on AWS to serve its market of growing customers, at a far lower cost than the licensing fees for Microsoft Server SQL.

With cloud architecture support from OpsGuru, Alida migrated from an inefficient and non-scalable legacy system to a fast-performing, scalable, modern application stack in AWS Cloud in just four months. Alida‚Äôs customers needed to see that its technology worked quickly, effectively, and was exceedingly stable. ‚ÄúAWS allows us to do that,‚Äù says Finneran.

## The Result

A partnership model for the future

Building on this project, Alida will continue to invest in its platform on AWS Cloud to gain scalability through containerization and microservices, and extensibility through APIs and open architecture.

The company is focused on staying ahead of the market and integrating technology, including natural language processing and Artificial Intelligence (AI). As Riaz Raihan, President, Products & Engineering, at Alida says, ‚ÄúTechnical architecture needs constant upkeep. You‚Äôre never done. You‚Äôre constantly upgrading it, making sure it‚Äôs the best and latest.‚Äù

That‚Äôs where partnerships are important. The collaboration with OpsGuru opened the door to technological innovation at Alida much faster than the company could have achieved on its own. It showed that the match-making model works. ‚ÄúIt‚Äôs very valuable to bring in trusted consultants to work with us,‚Äù says Finneran.

The matchmaking model, that brought AWS partners together, was a success. It expedited the project, created a better overall solution, transferred valuable skillsets, and delivered functionality to Alida customers much faster.

Says Raihan, ‚ÄúIt‚Äôs been a wonderful partnership and I‚Äôm sure it will evolve and grow.‚Äù

# How Alida is Leading Research Innovation with OpsGuru‚Äôs GenAI Guidance

## Introduction

‚Ä¢ Customer Success

January 6, 2025

## Background

The accelerating use of Generative AI (GenAI) is creating many new opportunities to improve business processes of all kinds. One of these opportunities is how to interpret product feedback from thousands of customers at a time, providing immediate feedback and actionable insights for global brand leaders.

The challenge of accurately interpreting feedback is critical for Alida, a global leader in experience management and community-oriented research for Fortune 500 clients. Alida has built a reputation for high-quality community feedback with incredible response rates (as high as 30%). To do this, they employ several survey methods and analysis techniques, including sentiment analysis and topic summarization.

Traditional methods, while useful, fall short in terms of speed, accuracy, and labour intensity. The advent of GenAI presented a possible solution to improve their service offerings. Alida was curious to see if AI could work in this field to produce accurate results better than those from current methods yet concerned about how to proceed while protecting the data privacy of its clients and their customers‚Äô data. That‚Äôs where OpsGuru came in.

## Innovation Through Partnership

OpsGuru is a Premier Amazon Web Services (AWS) Cloud Consulting Partner, with over 500 successfully implemented projects focused on cloud assessments, acceleration services, and productivity solutions. With a growing emphasis on AI, particularly GenAI, OpsGuru was a perfect fit to help Alida, having worked with them on several other cloud projects with glowing results since 2022\.

OpsGuru was a natural choice to lead this project, having previously guided Alida through an AWS Cloud Data Migration project. A key part of the overall solution is early use of AWS‚Äôs Bedrock platform, a managed service that allows customers to leverage GenAI models from various providers via API without needing dedicated hardware or specific contracts with 3rd party vendors.

## The Challenge

Interpreting Uniquely Human Experiences

The challenge centers around how market researchers perform qualitative analysis, and how difficult it is to do. Quantitative analysis is easy to perform, as the questions are structured into yes/no queries or numerical ranking (one to five). These types of results are easy for researchers to tabulate and summarize using statistical analysis software.

But qualitative analysis, while incredibly valuable, is much harder to do. Qualitative analysis involves the use of open-ended questions where responses can be sentences long and include multiple topics, concepts, and sentiments.

Researchers analyze data by closely examining responses to summarize topics, meanings, and sentiments, a process known as coding (distinct from programming). This tedious task demands language-savvy individuals with quick analytical skills and cultural knowledge. Alida‚Äôs research, sometimes involving tens of thousands of respondents, can take weeks to analyze.

While software exists to analyze open-ended responses, it only looks for keywords. It can‚Äôt pull sentiment or meaning out of sentences and has a hard time summarizing the data. Even when software is used, people still have to analyze the results the software provides.

GenAI seemed to provide a potential solution to this, but a further challenge is that Alida‚Äôs global clients operate in highly complex regulatory environments where the protection of their customer‚Äôs data is of paramount importance. There are risks associated with using AI tools, including potential data leakage, i.e., ingestion of user data to further train AI models, which some GenAI models do. Thus, any solution requires doing the job safely and ethically.

## Our Solution

OpsGuru Recommended AWS Bedrock and Anthropic Claude

At Alida‚Äôs request, OpsGuru developed the GenAI proof of concepts to evaluate efficacy, reliability, speed, and risks for sentiment analysis and topic summarization. The desired outcomes were to see if the tests could unearth nuances in patterns and sentiments with unstructured data and whether the results could equal or surpass human analysis and output, without putting data at risk.

OpsGuru and Alida chose Amazon Bedrock, in collaboration with AWS, for its comprehensive benefits: it‚Äôs a managed service offering access to high-performance AI models via an API for easy integration, with robust privacy and data security features. Its data storage in certified secure centers, with regional segmentation, supports global data privacy needs, underlining Alida‚Äôs commitment to data protection.

The selected model, Anthropic Claude, is a constitutional, ethics-focused model that uses principles for output evaluation, bypassing human bias. Designed for complex reasoning and technical tasks, it surpasses ChatGPT with abilities to process uploaded files, handle more words, and access post-2021 data.

For Alida‚Äôs purposes, Anthropic Claude was the best model to use and was chosen after OpsGuru‚Äôs Clear Path Forward for Generative AI. A readiness assessment, which included a two-week technical deep-dive, was conducted and a thorough analysis of Alida‚Äôs goals and technical and business challenges. This led to the recommendation of the type of AI model to use, as well as the proof-of-concept plan.

OpsGuru‚Äôs experience with AI models was vital in helping tune the solution specifically for superior performance in topic reduction and summarization.

## The Result

Greater Accuracy, Faster Results

Since the proof of concepts have been running, Alida has realized incredibly positive results so far. The results were compared using a test bed of highly anonymized data with human analysis, and computer-based keyword analysis.

What the results have shown is a vast improvement in both sentiment analysis and topic summarization accuracy when compared to Alida‚Äôs previous attempts using computer keyword analysis and superior results to existing human-based efforts.

There are immediate tangible benefits for their clients, where automated reports are instantly usable, without requiring additional human analysis to interpret the results.

And the GenAI process is fast. It can process 10,000 responses in a few minutes and can process up to 100,000 responses in just a few hours. ‚ÄúIt‚Äôs really a quantum leap forward in terms of performing sentiment analysis and topic reduction at scale,‚Äù said Finneran.

Lessons Learned: Worth Experimenting with AI

Overall, this was an experiment worth trying for Alida. ‚ÄúWith OpsGuru‚Äôs help, we‚Äôve learned a lot about just how capable AI is going to be in market research analysis. And our early experiments have given us a leg up on the competition,‚Äù Finneran remarked. ‚ÄúThis shows us that this can be done in a way that produces results and is safe with our client‚Äôs data.‚Äù

Future Directions: New AI Features Quarterly

Alida‚Äôs AI ambitions are not slowing down, with a goal to deliver one new AI-based feature per quarter. Some of their next steps will be to integrate real-time translations into the responses so they can pull in customer experience insights from around the world for their global clients. With OpsGuru‚Äôs AI expertise and guidance, Alida can innovate faster.

Conclusion

The early success shows how working with OpsGuru on AI projects can help clients solve tough problems and progress in a time of fast tech growth. As Joseph Finneran aptly put it, ‚ÄúInnovate or die. That‚Äôs the way forward.‚Äù With its bold foray into GenAI, Alida, supported by OpsGuru and Amazon Bedrock, is not just surviving‚Äîit‚Äôs thriving, setting new standards for the industry, and opening up new possibilities for the future of experience management research.

# HIPAA Compliant Genomics Platform with Emedgene

## Introduction

‚Ä¢ Customer Success

January 6, 2025

## Background

Emedgene is on a mission to unlock genomic insights for the world‚Äôs health organizations by providing data-driven AI solutions for genomic diagnostics and discovery at scale. They work with leading healthcare and life sciences organizations to power precision medicine programs, enable high throughput diagnostics, and accelerate research pace.

## The Challenge

The continuous innovation of the genomics intelligence platform is heavily dependent on efficient data search and analytics capabilities.

To facilitate that, Emedgene deployed a self-managed Elasticsearch cluster on a number of virtual machines running on the cloud.

As the company rapidly grew, reliability and scalability became top of mind priorities. Consequently, the Emedgene team decided to adopt AWS to support the demand for growth. The managed AWS Elasticsearch services drastically reduce the operational overhead, enabling Emedgene to take advantage of the diverse set of AWS services to deploy to multiple regions closer to healthcare providers.

However, the Emedgene team soon realized that setting up on AWS, properly following the best practices of AWS while satisfying the requirements of HIPAA was a complex exercise. While it was important for the production to migrate to AWS rapidly because of the costs of operations, reliability and performance, it was equally important for the workload to be migrated with security best practices and compliance in mind.

## Our Solution

OpsGuru was engaged partly because of the team‚Äôs demonstrable expertise on Elasticsearch, but more importantly, because of the wide-range experience in AWS adoption through the OpsGuru Cloud Launchpad.

The OpsGuru Cloud Launchpad is a templated cloud onboarding solution written in open-sourced Hashicorp Terraform. It sets up the AWS accounts in alignment with security best practices. With the OpsGuru Cloud Launchpad, Emedgene immediately acquires full audit capabilities. In addition to the technical capabilities that were enabled, Emedgene acquired network security and protection from external access.

Other than migrating the Elasticsearch workload to AWS Elasticsearch services, the OpsGuru team helped Emedgene adopt other relational data storage and services to enhance the system, including the adoption of Amazon ECS and Amazon ECS on Amazon Fargate for the different types of workloads.

While the migration was a resounding success, OpsGuru continued to assist Emedgene by advising on cost consolidation through the strategic use of spot instances and adoption of innovative storage solutions.

Both of these solutions led to significant cost savings of about 20% towards Emedgene‚Äôs operating costs.

## The Result

By migrating the workload to AWS, Emedgene could continue building its core capabilities leveraging Elasticsearch using AWS Elasticsearch services without needing to continuously bear the management overhead.

Because of the OpsGuru Cloud Launchpad, Emedgene had gained a HIPAA-compliant AWS foundation that enabled them to continuously build new features and extend their customized AWS solutions for patient care globally.

## 

‚Ä¢ Customer Success

# Kubernetes Development Environment for ML in Amazon EKS

## Background

This client is a company that leverages AI to generate photos for use in the fashion industry. They unite a team of experts with the perfect combination of skills to revolutionize fashion visuals. With vast experience in computer vision, AI, media, and usable enterprise products, they are committed to transforming the fashion industry through the use of synthetic media.

## The Challenge

The client faced several challenges in developing a more efficient and cost-effective development environment for ML. First, their previous solution using GPU instances in GCP was too expensive and slow. Second, they needed a solution that would enable their developers to carry out the development process and training experiments in a more streamlined and efficient way.

## Our Solution

To address these challenges, OpsGuru created a development environment for ML using EKS and GPU instances on Amazon Web Services (AWS). This provided the client with a faster and more efficient development environment than they previously had. The developers were able to SSH into the pods to carry out the development process and could use Kubernetes jobs to carry out training experiments.

OpsGuru used Terraform to create the environment, which helped to automate the process and ensure consistency across the entire environment. This also allowed OpsGuru to deploy the environment quickly and efficiently while minimizing the risk of errors.

OpsGuru proposed building an ML development environment using Terraform to automate the environment creation process, as well as EKS and GPU instances on AWS to provide the necessary infrastructure for training ML models.

To implement the solution, OpsGuru followed the following steps:

Step 1: Use Terraform to automate environment creation

OpsGuru used Terraform to automate the process of creating the ML development environment on AWS. This allowed the client to create and manage the environment in a more efficient and cost-effective way.

Step 2: Set up the EKS cluster

OpsGuru set up an EKS cluster using a combination of managed AWS services and open-source tools. This provided a scalable and reliable platform for running ML workloads.

Step 3: Configure GPU instances

To accelerate the training process and reduce costs, OpsGuru configured GPU instances to be used in the development environment. Thus allowing the client‚Äôs development team to train models more quickly and efficiently.

Step 4: Set up a development environment

OpsGuru set up the development environment, including pods and Kubernetes jobs, to enable the development team to carry out ML experiments. The development environment was designed to be flexible and scalable, allowing the team to run multiple experiments in parallel.

## The Result

The project was a success, and the company‚Äôs CEO expressed satisfaction with the results, stating, ‚ÄúI really enjoyed working together and very happy with the results‚Äù. By leveraging OpsGuru‚Äôs expertise in cloud-native technologies and extensive experience with the AWS platform, the company was able to achieve its goal of developing an efficient and cost-effective development environment for ML including:

‚Ä¢ A cost-effective and scalable environment by leveraging Terraform automation and utilizing GPU instances on the AWS platform.

‚Ä¢ Faster model training times, thus allowing them to iterate more quickly on their AI models.

‚Ä¢ A scalable and flexible development environment that could accommodate multiple experiments in parallel.

‚Ä¢ The new environment has enabled their developers to work in a more streamlined and efficient manner while avoiding the high costs and slow performance they had experienced in the past.

As a result of this project, the client was able to train its AI models more quickly and cost-effectively, thus gaining a competitive advantage in the fashion industry.

# Connecting Women to Essential Healthcare with Nivi

## Introduction

‚Ä¢ Customer Success

January 6, 2025

## Background

Nivi empowers women to connect with reproductive healthcare providers in ways that are immediate, personal, and always accessible from multiple geographic locations. When a user contacts ‚ÄúaskNivi‚Äù, the Nivi platform automatically identifies her intent and routes her to the best provider.

## The Challenge

As Nivi‚Äôs customer base grew, Nivi faced the challenges to grow the infrastructure as fast as the customer base.

System availability, workflow reliability and data security were the fundamental requirements of the system, and manually creating and supporting the resources would not suffice as a solution for the long term. Given the highly positive inception of their service, Nivi was looking at quickly expanding the service to different geographical regions. Before they could do so, they needed a solution that would allow the team to easily create and maintain the necessary AWS resources, and have a delivery mechanism that could systematically deliver and update the services as new features were being rolled out.

## Our Solution

Nivi engaged OpsGuru for the AWS expertise, architecture and implementation of the next-generation architecture.

Given the key issues were scalability, reliability and ability to service users in different regions efficiently, OpsGuru has focused on these key challenges and devised a solution accordingly through working closely with key stakeholders at Nivi.

Based on the AWS well-architected framework, the new architecture was designed rolled out in Frankfurt (eu-central-1) and Mumbai (ap-south-1) regions of AWS. The serverless architecture ‚Äî as the microservices were deployed using multiple deployments of AWS Lambda functions ‚Äî was chosen because of its scalability, cost-effectiveness and ease of deployments. The flexibility and minimal management overhead are especially suitable to an agile start-up team like Nivi.

The features of the Nivi platform were delivered in about 100 AWS Lambda functions written in Node.js. An Amazon API Gateway was set up in each region to handle the requests directed by Amazon Route53 based on geographical locations, while Amazon SQS acted as the messaging tier coordinating actions across the different AWS Lambda functions with minimal coupling. The data of the platform was stored in Amazon RDS and Amazon DynamoDB. Amazon SNS was used to handle external notifications, while sessions were stored in AWS Elasticache Redis. Meanwhile, Amazon CloudFront was deployed to reduce the load time of static content.

Infrastructure as code was also an indispensable part of the solution, OpsGuru delivered the codified infrastructure using Terraform. The code-base could serve as a baseline for new workloads and expansion to other new regions on AWS in the future. The OpsGuru team also rolled out the Serverless Framework to increase developer productivity, as with the framework the developers could deploy and test the serverless functions conveniently.

To continue the momentum of innovations and fast delivery, OpsGuru architects also provided comprehensive training/coaching for cloud-native development best practices. This enabled the Nivi developers to reliably and speedily deploy new features into production.

Because the system was composed of hundreds of microservices (deployed AWS Lambda), an application delivery pipeline that would deploy and update each of the microservices as needed was necessary. OpsGuru identified the criticality of a robust CI/CD system to support Nivi‚Äôs goal and created a set of easily replicable CI/CD pipelines on CircleCI to support Lambda application delivery

On top of that, as Nivi‚Äôs core goal was to continuously and reliably empower women and make healthcare accessible, it was essential for the system to run operational best practices and security in mind. As such, OpsGuru designed bespoke AWS IAM roles that aligned with the principle of minimal access for each of the AWS Lambda functions and the Nivi team, while AWS CloudWatch metrics, alerts and dashboards were used extensively throughout the system to track system health in order to optimise the end-user experience.

The code-base could serve as a baseline for new workloads and expansion to other new regions on AWS in the future.

## The Result

The Nivi team was able to successfully roll out their services to additional geographic locations while providing a performant and reliable experience.

The ‚ÄúAskNivi‚Äù platform was able to scale as the company‚Äôs customer base significantly expanded.

With the number of new regions closer to the user base, Nivi.io was able to grow the customer base more effectively. Existing customers also became more engaged due to improved service performance.

# Automating & Optimizing Best Candidate Hiring Platforms with Perfect

## Introduction

‚Ä¢ Customer Success

January 6, 2025

## Background

Perfect is developing a cutting-edge AI and Big Data solution that is targeting the recruitment market. They are a team of seasoned professionals both in technology and the human resources space, coming from industry-leading companies, and backed by top investors from the local ecosystem.

Their solution allows the end-user to instantly connect with the next-best-hire through features that find candidates who are most likely to join the company, and use contextual data to start meaningful conversations that stand out from the noise.

## The Challenge

Perfect had already adopted Databricks as a primary platform but was limited by a centralized environment that did not allow for the isolation of production and non-production environments or the implementation of user segregation. Additionally, the lack of a declarative approach made it difficult to effectively manage the overall configuration.

Perfect was seeking to optimize and standardize its Databricks environment on AWS through the implementation of best practices for both AWS and Databricks infrastructure resources. This included the establishment of consistent workload environments with no configuration snowflakes, a single source of truth for the desired environment configuration in code, and the ability to declaratively manage Databricks jobs and clusters. The goal was to enable fully automated testing and promotion of features/changes across multiple workload environments, as well as strict access controls between environments and fully automated data replication from production to staging environments.

Additionally, Perfect aimed to improve observability through comprehensive logging and monitoring and to better control and manage overall compute and platform costs.

## Our Solution

Perfect engaged OpsGuru, an AWS Premier Tier Services Partner, due to the team‚Äôs extensive AWS and Databricks experience and a proven track record with complex workload migrations. With OpsGuru‚Äôs help, Perfect was able to improve its Databricks configuration by fulfilling all of the identified requirements.

OpsGuru identified 4 resource layers that needed to be implemented, in order to achieve efficient scaling and workload isolation requirements:

1\. Infrastructure Layer

‚Ä¢ AWS Resources:

‚Ä¢ VPC

‚Ä¢ Bastion hosts

‚Ä¢ Security (IAM, CMK)

‚Ä¢ Storage (S3)

‚Ä¢ DBMS (RDS)

2\. Bridge Layer

‚Ä¢ Database MWS Configuration

‚Ä¢ Credentials

‚Ä¢ Storage Configuration

‚Ä¢ Network

‚Ä¢ Databricks Workspace(s)

3\. Configuration Layer

‚Ä¢ Resources within the Databricks Workspace

‚Ä¢ Cluster Policies

‚Ä¢ Instance Pools

‚Ä¢ Libraries

‚Ä¢ Secrets

‚Ä¢ User roles/groups/permissions

‚Ä¢ DBFS Mounts

4\. Application Layer

‚Ä¢ DBX managed application-specific resources (e.g., Jobs, tables, repos)

All of the layers are managed declaratively in a fully automated manner through Infrastructure as Code (IaC) and CI tools.

Multiple Environments

Different workload environments (e.g., development, staging, production) are managed through multiple instances of the above-defined layers. This approach allows complete isolation and reduces the potential attack surface as the environments have no dependencies on each other and are completely unaware of each other.

IaC allows consistent environment configuration across the whole ecosystem with no manual, snowflake environments.

## The Result

OpsGuru worked alongside the Perfect engineering team to implement the above-presented architecture. This process ensured that Perfect was able to rapidly deploy and promote workloads across fully isolated Databricks environments (workspaces).

‚Ä¢ Environment Isolation

Multiple Databricks workspaces allowed engineering teams to frictionlessly manage and promote workloads and their artifacts through different stages.

‚Ä¢ Resource layers isolation

Through four identified resource layers, Opsguru helped Perfect to define the separation of concerns across various layers of the architecture, without introducing additional overhead to the engineering teams.

‚Ä¢ Declarative architecture

With the effective implementation of the IaC through Terraform with Databricks and AWS providers, Opsguru allowed Perfect to have a single source of truth for the complete architecture, from Databricks to AWS, without any manual steps or snowflake behaviour as before.

‚Ä¢ Continuous integration and deployment

Implementation of the IaC allowed Perfect to treat the complete infrastructure the same way as application code, providing them the possibility to implement a fully automated CI/CD process integrated into the existing Git workflows with GitHub Actions.

OpsGuru then assisted the Perfect team in their migration to the newly built Databricks platform by providing the data, configuration migration plan, and execution.

After the completion of the project, OpsGuru provided comprehensive training sessions for the Perfect team as well as documentation and operational playbooks for the newly designed systems. The training and documentation included the operation of the Databricks configuration and environment deployment, AWS configuration, and deployment, among other topics.

# OpsGuru helps design, implement and operate a multi-regional platform on AWS

## Introduction

‚Ä¢ Customer Success

January 6, 2025

## Background

The Vancouver-based company under consideration is a well-established mining company with a strong financial foundation. Focusing on gold production, the company manages a diverse portfolio of mines across the Americas. The organization is constantly exploring ways to produce gold in a efficient, sustainable and sociably responsible manner.

## The Challenge

The company pursues a high growth and high yield strategy, which is sustained by close oversight on asset transactions, material costs and operational expenditure. Such sensitive business data is stored in the different ERP systems across the different regions. It is essential to keep its mission-critical ERP systems secure and available to promote the company‚Äôs continued success. Meanwhile, because of the ERP systems are set up and maintained by regional teams using different technologies, sharing a uniform architecture and standardized operating procedures is extremely challenging.

## Our Solution

OpsGuru was engaged to design, implement and operate a multi-regional platform on AWS, which would host mission-critical ERP systems and potentially other assets and support workloads to be operated using a standardized playbook.

The platform was developed using infrastructure as code (Terraform), such that new regions to support multi-region resilience could be easily added. However, migrating the ERP systems to AWS on the newly created platform was only the first step; it was more even important to ensure system availability through data backup, system monitoring, rapid responses. A full resilience workflow was deployed leveraging AWS Backup, Amazon CloudWatch, Amazon Lambda. All the workflows were executed in internal network contained within AWS VPC. DevOps best practices were also followed by setting up isolated environments emulating production, such that the full workflow could be tested before production adoption.

## The Result

As a result of the engagement, OpsGuru enabled the company to migrate already two of the largest ERP systems onto AWS. The resilience procedures have been tested for both of the systems. The runbooks and records of the resilience tests were instrumental for the company to pass third-party independent review, such that the company remained in good standing. The streamlined workflow also grew confidence in the different teams of the company. New workloads are currently being elaborated to support further AWS adoption, and to take advantage of the resilient and cost-effective designs.

## 

‚Ä¢ Customer Success

# SOC2 Compliant AWS Platform with Rival Technologies

## Introduction

‚Ä¢ Customer Success

January 6, 2025

## Background

Rival Technologies (Rival) develops chat, voice and video solutions that enable global brands to understand the accurate attitudes, opinions and preferences of their target audience.

## The Challenge

As a growing startup, Rival wanted to increase their agility and resiliency while still meeting compliance needs.

The company was especially focused on automating its AWS infrastructure to ensure fast and repeatable scaling. The company was on a tight deadline but still wanted to ensure that the right services and best practices where implemented to establish a future-proof infrastructure.

## Our Solution

The Rival Technologies platform can now scale up and down to closely align with actual end-user demand, in multiple geographic regions.

Automation and enhance network segmentation allow for better redundancy and assurance. OpsGuru provided a better way for Rival‚Äôs agile DevOps team to practice change management.

At the end of the engagement, OpsGuru provided documentation and training sessions to ensure a seamless handoff.

At the end of the engagement, OpsGuru provided documentation and training sessions to ensure a seamless handoff.

## The Result

In OpsGuru, Rival Technologies found exactly the right partner to design and implement the solution they needed.

The cloud onboarding solution OpsGuru architected allowed Rival to enhance their security and availability capabilities and better meet their compliance needs.

To enhance the technical implementation, OpsGuru conducted several end-user sessions, remaining efficient and cost-effective, and then provided multiple rounds of architectural enhancements as part of the infrastructure rollout.

## 

‚Ä¢ Customer Success

# Scaling Travel Insurance Platforms with Setoo

## Introduction

‚Ä¢ Customer Success

January 6, 2025

## Background

Setoo‚Äôs award-winning insurance-as-a-service platform empowers e-businesses (such as online travel agents, accommodations, ticket sellers, e-commerce) to independently build and distribute personalized, claims-free insurance products. The products are integrated directly into the customer journey to provide a seamless purchasing experience.

## The Challenge

Setoo had identified a business opportunity by streamlining the insurance purchase process.

While a sound business concept was essential, business success also depended on strong execution. Setoo recognised the need to engage trustworthy and dependable partners to bring the concept into fruition.

As Setoo identified Amazon Web Services to be the ideal cloud provider because of the rich features and the maturity of the platform, Setoo needed a partner who could rapidly define and rollout an architecture following cloud best practices and provide the thought leadership necessary on the journey to adopt cloud-native designs. To set it up for success and hyper-growth, Setoo wanted a partner who could take ownership of the infrastructure and build it based on evolving needs, such that the Setoo team could focus on growing the Setoo core products and continuing building its distinct advantages.

## Our Solution

In OpsGuru, Setoo saw the qualities they were looking for in developing the Setoo platform.

OpsGuru identified a number of critical features Setoo required, including rapid delivery and update of systems to deliver features, extensibility in the design such that new features could be easily and quickly added, scalability of the system to service the growing demands, and cost-efficiency to sustain the system while the team expanded to different markets. End-to-end security was indispensable to the workload too.

## Cloud Launchpad ‚Äì Extensibility, Security and Compliance Readiness

To efficiently deliver a baseline and facilitate team adoption of the AWS platform, OpsGuru deployed the Cloud Launchpad, which featured:

‚Ä¢ Account isolation per environment.

‚Ä¢ Full AWS API level audit leveraging AWS Cloudtrail shipped to separate Security Escrow account used by security analysts.

‚Ä¢ Certificates automatically managed & renewed by AWS Certificate Manager.

‚Ä¢ Full encryption at transit and rest leveraging KMS and TLS.

‚Ä¢ VPC isolation per environment, with peering configured with shared VPC used to host the shared workloads such as the centralized CI/CD on Jenkins and container repository using Amazon Elastic Container Registry.

The Cloud Launchpad was not only a kickstart toolkit but also a solid foundation that included vulnerability scanning and auditing capabilities. With the foundation in place, Setoo earned the peace of mind knowing that they could focus on further enhancing their platform instead of having to worry about future security features and compliance requirement retrofitting.

## Cloud-Native Application Design

While OpsGuru delivered Cloud Launchpad to create an infrastructure baseline for the Setoo services, it was also important for OpsGuru to support the Setoo team at creating a solution that would take advantage of the design.

Microservices pattern was advocated as a cloud-native design because the modularity of architecture made delivery, testing and release of specific features faster and less challenging, the design pattern also made scaling individual stages of workloads as required by the customers a lot more rapidly and cost-efficient. To that effect, OpsGuru delivered a Spring Boot template, which incorporated best practices for connectivity with databases, queuing systems, configuration and secrets management. By leveraging the template, the Setoo development team was able to confidently deliver new features following a framework that had already been tested and proven working. Multitenancy design was also incorporated in the template, as multi-tenant architecture offered the most efficient use of the infrastructure and standardised access patterns.

An automated CI/CD workflow would make the delivery of the microservices a sustainable process, especially as the number of features and microservices grew. OpsGuru delivered a full-fledged CI/CD system to empower the team with a mechanism to introduce and update the services. Service delivery was not the only goal of the CI/CD system; as sufficient testing needed to run before rolling upgraded services out to all users, OpsGuru also included canary deployment and UI (Selenium) testing in the CI/CD workflows.

## Holistic System Elasticity, High Availability and Resiliency

A cloud platform supporting insurance as a service must consider elasticity, high availability and resiliency.

Service scalability and resilience was achieved by running all stateless applications on AWS Elastic Container Service (ECS) spanning three availability zones for maximum redundancy. Cost-optimized auto-scaling policies leveraging spot instances were implemented for maximum cost efficiency ‚Äì these helped with supporting workload spikes during high traffic hours while still remaining cost-efficient.

For proactive observability, OpsGuru provided a comprehensive solution which included metrics dashboards and log analysis leveraging AWS CloudWatch as well as distributed application tracing leveraging Zipkin.

The cloud infrastructure delivered by OpsGuru also supported multi-tenancy, as a multi-tenant system could minimise management overhead and utilise the cloud resources more optimally. Armed with the metrics and proactive monitoring, Setoo had the capability to scale the multi-tenanted solutions ahead to ensure no visible impact on end-users.

The initial architecture aside, the entire system was not signed off until OpsGuru and Setoo were able to perform multiple rounds of load tests to ensure performance criteria of elasticity, high availability and resiliency were satisfied.

## Data Insights and Continuous Growth

To support continuous growth, Setoo needed to have insights on the engagement with the system evolved.

After examining the data collected by the Setoo platform, OpsGuru created a workflow to help the team readily report on the data. As the data source was primarily in the Amazon RDS (MySQL) database, AWS Glue ETL processes to retrieve the relevant data and store it in S3 in data-partitioned Parquet format. AWS Glue crawler processes were then run on the S3 data to populate the AWS Glue data catalogs, such that Amazon Athena was deployed to execute specific queries to obtain business insights. Finally, Amazon Quicksight was also set up to connect to AWS Athena, such that charts and dashboards were readily available to make the business insights more accessible.

This foundation enabled Setoo to build a secure system that included vulnerability scanning, auditing capabilities and machine-learning-based threat detection.

## The Result

OpsGuru collaborated with Setoo to create a SaaS platform. A fully compliant and enterprise-grade cloud environment was delivered on day one.

Given the automation of their infrastructure deployments, operational risk had been significantly reduced. All infrastructure and application changes were fully tested and validated in identically configured environments before being promoted into production. Developer velocity was also increased as a result of the automation, which was instrumental in maintaining Setoo‚Äôs edge in a competitive market space. Their highly available deployment allowed Setoo to maintain customer satisfaction by making sure customers were always served, even under unexpected circumstances.

‚Ä¢ To fully enable the team to use the platform, OpsGuru created Spring Boot templates for the teams to create new microservices more conveniently. A CI/CD pipeline was delivered as well to take advantage of the speed and limitless expandability of the cloud.

‚Ä¢ Furthermore, the solution featured easy business data processing. With the data pipeline established to retrieve business insights from the Amazon RDS MySQL data source, near real-time analytics, were readily available through the charts and dashboards on Amazon Quicksight. The Setoo team now could easily analyse the engagement data to design more features feeding into the innovation in the insurance-as-a-service space.

‚Ä¢ By providing ample staff training and handover documentation, OpsGuru ensured that Setoo is empowered to continue the enhancement of their platforms and to continue to evolve faster than their competitors.

# Data Sovereignty in Canada: Implications for Cloud Services

## Introduction

‚Ä¢ Compliance

‚Ä¢ Data Sovereignty

Today, we‚Äôre diving into an important topic‚Äîdata sovereignty. As a company deeply involved in professional cloud services, understanding how data sovereignty influences our operations, our clients, and the cloud ecosystem is key.

## What is Data Sovereignty?

In a nutshell, data sovereignty refers to the jurisdictional rules that govern data based on its location. You might be wondering why there are data centers scattered across the globe. Different nations have distinct laws for data protection, privacy, and security. Additionally, legal mechanisms such as subpoenas can further complicate matters, as they can compel organizations to disclose or transfer data in specific jurisdictions.

## Its Importance in IT Security

Data sovereignty is crucial for two main reasons. Firstly, regulatory compliance with regional laws is non-negotiable, and falling short can carry both legal and reputational risks. Secondly, jurisdictions differ in their security measures, so adhering to these local standards is vital for a seamless operation.

## PIPEDA

The Personal Information Protection and Electronic Documents Act (PIPEDA) is the Canadian federal privacy law for private-sector organizations. This legislation broadly governs how companies handle the personal information of Canadian citizens, setting standards for data collection, processing, transfer and storage. Businesses are held responsible for ensuring robust data protection measures. Within the healthcare industry, organizations in Ontario are additionally governed by the Personal Health Information Protection Act (PHIPA). While PIPEDA sets the framework for personal information, PHIPA safeguards Personal Health Information (PHI). Compared to the broad coverage of PIPEDA, PHIPA offers a targeted framework for healthcare professionals and closely aligns with the Health Insurance Portability and Accountability Act of the United States (HIPAA) in terms of safeguarding PHI.

There is no single federal law that applies to data in Canada. PIPEDA applies to private sector organizations across Canada (except Quebec, Alberta, and British Columbia), and the Privacy Act applies to government organizations. While neither law mandates organizations to keep their sensitive data in Canada, the Directive on Service and Digital by the Canadian government says keeping computing facilities within the border should be considered the first choice.

Quebec, Alberta, and British Columbia each have provincial laws that resemble but are not identical to PIPEDA. Quebec‚Äôs legislation requires organizations to conduct a privacy assessment if data is sent outside Quebec. British Columbia has stipulations requiring public bodies to store personal information inside Canada. In Alberta, the Personal Information Protection Act (PIPA) applies primarily to commercial activities and includes special provisions for non-profit organizations and professional regulatory bodies. While each province has its nuances, the underlying aim across all is to protect personal information effectively.

To avoid legal hassles and the complexity involved in processing data across national borders, cloud service providers have started setting up their own data centers in Canada. This move is particularly important for healthcare and insurance organizations that are required to store personally identifiable information, including Electronic Health Records (EHR), now that Canadian regulations will be followed.

## The Cloud Service Ecosystem and Data Sovereignty

Our alignment with industry-leading cloud service providers equips us with the tools and responsibilities to manage data sovereignty effectively. For example, the choice of data center locations‚Äîdetermined by the Region selected at most public cloud providers‚Äîallows us to align with client needs concerning jurisdictional requirements. On top of that, customers can extend cloud infrastructure and services into on-premise environments using services such as AWS Outposts, Azure Stack, and Google Anthos. This allows you to keep sensitive data within specific jurisdictions while benefiting from cloud flexibility and scale. It‚Äôs a powerful option for organizations subject to strict data residency laws.

## Navigating the Challenges

The road to full compliance isn‚Äôt without its bumps. Multi-Region intricacies add layers of complexity that necessitate a well-crafted strategy. Similarly, the dynamic legal landscape means that laws are ever-changing; keeping up is an ongoing process. Moreover, the rapid evolution of technology solutions adds another layer of complexity to maintaining compliance.

## Best Practices

When it comes to best practices, data localization is key. Store data close to its point of use to minimize jurisdictional complications. Regular audits play a vital role by ensuring data handling and storage practices are in line with regional laws. Furthermore, keeping the team educated on current laws and regulations related to data sovereignty is essential.

## Wrapping Up

As professionals in the cloud services sector, understanding data sovereignty is more than ticking off legal boxes; it‚Äôs about delivering secure and ethical services to our clients. With the available resources, it‚Äôs our collective responsibility to stay educated and vigilant.

# Does Your Organization Need a Service Mesh? (Part 1\)

## Introduction

‚ÄúService Mesh‚Äù may be one of the most hyped buzzwords in the Cloud Native space, but what is a Service Mesh and how do you know if your team needs one? These are obvious questions with non-obvious answers. Unless, of course, you‚Äôve had the chance to work through the good, bad, and the ugly of service mesh adoption on multiple projects and observe where organizations struggle, where they succeed, and whether the payoff was worth the pain. Fortunately, OpsGuru has had the opportunity to help clients through these service mesh adoptions. In this blog series, I‚Äôll share some high-level insights in hopes of bringing you closer to considering service mesh adoption for your organization.

## Why Are Service Meshes So Popular?

At the highest level, a service mesh is a set of utilities that abstract most of the complexities of communication between applications. These complexities, highlighted in the Fallacies of Distributed Computing, draw attention to specific and probable failure modes that are introduced to a system when it is distributed across a network. These eight fallacies still ring true today and provide an excellent perspective for describing the problems that a service mesh solves:

The Eight Fallacies of Distributed Computing:

‚Ä¢ The network is reliable

‚Ä¢ Latency is zero

‚Ä¢ Bandwidth is infinite

‚Ä¢ The network is secure

‚Ä¢ Topology doesn‚Äôt change

‚Ä¢ There is one administrator

‚Ä¢ Transport cost is zero

‚Ä¢ The network is homogeneous

Modern applications are typically built with a combination of microservices and third-party services, distributed across failure domains while being subject to tighter regulations. The increasing pace of innovation for products leaves developers and operators less time to plan ahead.

Fortunately, there are organizations that have shared their experience in resolving these complex issues. Thanks to initiatives and open source solutions from companies like Lyft, Google, IBM, HashiCorp and others, we now have a number of solutions to these problems at scale. Collectively, these solutions all fall into the category of Service Mesh.

Service meshes will continue to be a topic of discussion due to the complexities in distributed computing. These problems have been exacerbated by modern software engineering practices enabled by cloud-native platforms. What we‚Äôre seeing today is the first round of accessible, robust solutions to these problems. While there are some obvious rough edges to most offerings, it‚Äôs important to recognize them as implementation details rather than fundamental flaws in the service mesh paradigm.

In the next few weeks, we‚Äôll do a deeper dive in the following topics:

‚Ä¢ What does a Service Mesh Do?

‚Ä¢ How does a Service Mesh Work?

‚Ä¢ Do you need a Service Mesh?

Are you intrigued by service mesh and wondering if your team needs one? Let us know. We‚Äôd love to help\!

# From AI Agents to Agentic AI: An Exploration into Autonomous Systems

## Introduction

‚Ä¢ Data and AI

‚Ä¢ Generative AI

‚Ä¢ Innovation

It‚Äôs hard to miss the growing momentum behind Agentic AI. It‚Äôs rapidly changing the landscape of artificial intelligence, promising a future where machines can act with unprecedented autonomy and intelligence. This blog post will explore the core concepts of Agentic AI, uncovering its potential to transform industries and redefine our relationship with technology.

Key Takeaways

‚Ä¢ Agentic AI represents a shift from reactive tools to autonomous systems that plan, act, and adapt to achieve goals.

‚Ä¢ It often involves coordinating multiple specialized AI agents to tackle complex, real-world tasks.

‚Ä¢ Agentic AI is already being applied across healthcare, energy, and finance industries to enable smarter, more proactive systems.

## What is Agentic AI?

Agentic AI refers to AI systems designed to act autonomously and achieve specific goals with minimal human intervention. Unlike traditional AI that passively responds to inputs, Agentic AI can actively plan, take actions, and adapt to new information. Think of it as a highly capable digital assistant that can learn, reason, and make decisions to achieve objectives.

These systems can:

‚Ä¢ Perceive: Gather and interpret information from various sources.

‚Ä¢ Reason: Process information to understand context and make informed decisions.

‚Ä¢ Set Goals: Define objectives based on predefined parameters or user inputs.

‚Ä¢ Make Decisions: Evaluate options and choose the best course of action.

‚Ä¢ Execute: Interact with systems or provide responses.

‚Ä¢ Learn and Adapt: Refine strategies over time based on feedback and experience.

In short, Agentic AI represents a significant leap forward‚Äîshifting from reactive tools to proactive, goal-driven systems capable of independently navigating complex tasks.

## Agentic AI vs. Generative AI

Agentic AI and Generative AI have distinct purposes. Generative AI excels at creating new content, like text, images, music, or code. It‚Äôs the creative powerhouse of AI.

Agentic AI, on the other hand, is action-oriented. It goes beyond content creation to empower systems that can act autonomously in the real world, analyze situations, strategize, and take action to achieve goals.

The core distinction is this: Generative AI creates, while Agentic AI acts. Generative AI responds to prompts; Agentic AI takes initiative. This proactivity allows for a more dynamic and efficient human-machine interaction, where AI can anticipate needs and take action without waiting for instructions.

## Agentic AI vs. AI Agents

‚ÄúAgentic AI‚Äù and ‚ÄúAI agents‚Äù are often used interchangeably, and understandably so\! They both refer to AI systems that can act autonomously to achieve goals. However, there‚Äôs a subtle difference in how we can understand them.

Think of ‚ÄúAI agent‚Äù as the broader term. It encompasses any AI system that can perceive its environment, make decisions, and take actions to achieve a goal. This could be a simple chatbot that answers basic questions or a complex system that manages an entire supply chain.

Agentic AI often involves coordinating multiple AI agents to achieve a common goal. Think of it like a team of specialists working on a complex project. Each agent has expertise and responsibilities but collaborates and communicates to achieve a shared objective.

This coordination can involve:

‚Ä¢ Task delegation: Assigning specific tasks to the most suitable agents.

‚Ä¢ Information sharing: Exchanging data and knowledge between agents.

‚Ä¢ Synchronization: Ensuring that agents‚Äô actions are aligned and timed correctly.

‚Ä¢ Conflict resolution: Managing competing goals or priorities between agents.

This multi-agent approach allows Agentic AI systems to tackle complex, real-world problems beyond the capabilities of any single agent. By coordinating the actions of multiple agents, Agentic AI can create more robust, adaptable, and intelligent solutions.

## Applications of Agentic AI

Let‚Äôs walk through a few real-world examples to make this more tangible.

## Healthcare

AI is already making a difference in healthcare. An AI-powered diagnostic tool, for instance, can analyze medical images, helping doctors identify potential diseases more accurately. This specialized agent excels at image analysis and is crucial in early detection.

Imagine a network of these AI agents working together to provide even more comprehensive patient care. Such a system might include:

‚Ä¢ An agent proactively schedules appointments and manages medication reminders, ensuring you stay on top of your health.

‚Ä¢ Another agent continuously monitors your vital signs, alerting your healthcare team to any concerning changes, even outside of the hospital.

‚Ä¢ A third agent analyzes your health data to recommend personalized treatment plans, tailoring your care to your needs.

‚Ä¢ A friendly AI agent can answer your questions, guide you through the healthcare system, and provide personalized support.

This coordinated system of AI agents demonstrates the potential of Agentic AI in healthcare ‚Äì delivering proactive, patient-centric care through collaboration and intelligent action.

## Energy

AI is crucial for ensuring the safe and efficient operation of oil and gas pipelines. An AI agent can be a vigilant leak detection system, analyzing sensor data to identify anomalies and pinpoint potential leaks in real-time. This agent specializes in rapid response, preventing environmental damage and ensuring safety.

Meanwhile, Agentic AI can be a multi-agent system where several AI agents collaborate to optimize pipeline management, where:

‚Ä¢ An agent monitors pipeline integrity, predicting potential failures before they occur, enabling proactive maintenance and preventing costly downtime.

‚Ä¢ Another agent optimizes pipeline flow, adjusting pumping rates and valve settings to maximize throughput and minimize energy consumption.

‚Ä¢ A third agent automates accounting processes, tracking ownership and volume for accurate billing and revenue management.

‚Ä¢ An agent ensures regulatory compliance and monitors adherence to safety and environmental regulations.

‚Ä¢ An agent communicates with stakeholders, providing real-time updates on pipeline operations and any potential disruptions.

This coordinated system of AI agents, orchestrated by Agentic AI, creates a more efficient, reliable, and transparent midstream operation.

## Finance

In the financial industry, AI is transforming customer service. A simple AI chatbot can handle basic inquiries, providing instant answers to common questions and freeing up human agents for more complex issues.

But think about a team of AI agents working together to provide a more personalized and proactive customer service experience:

‚Ä¢ An agent analyzes customer interactions to identify those needing financial guidance, offering personalized advice and resources.

‚Ä¢ Another agent monitors accounts for unusual activity, proactively alerting customers to potential fraud and guiding them through protective measures.

‚Ä¢ A third agent anticipates customer needs based on their financial history, offering relevant products and services like loan consolidation or investment recommendations.

‚Ä¢ An agent provides personalized financial guidance, answers complex questions and offers tailored advice.

‚Ä¢ An agent proactively informs customers about benefits and rewards programs they may be eligible for.

This coordinated system, orchestrated by Agentic AI, creates a more proactive, personalized, and empathetic customer service experience, building stronger customer relationships and improving overall satisfaction.

## AWS Bedrock Agent and Bedrock Flow: Your Gateway to Agentic AI

While the field of Agentic AI is still evolving, AWS offers powerful tools to start exploring its potential.

AWS Bedrock Agent enables you to create AI agents that can perform complex tasks autonomously. You can define their specific capabilities, connect them to foundation models, and equip them with tools to interact with data sources, APIs, and other systems. Each agent can specialize in functions like data analysis, content generation, or customer service.

A notable feature of Bedrock Agent is its multi-agent orchestration capability. This allows you to create systems in which a primary agent coordinates the work of specialized agents, distributing tasks based on their expertise and consolidating their outputs. This approach helps tackle complex workflows that benefit from multiple specialized perspectives.

AWS Bedrock Flow complements these agent capabilities by providing a visual, low-code environment for designing and managing the interactions between different components of your AI system. With Bedrock Flow, you can define precise sequences of operations, conditionally branch between different processes, and control how information flows between agents and other services.

Together, these services allow you to build sophisticated AI systems that can reason through problems, access relevant information, and take appropriate actions with minimal human intervention‚Äîall while maintaining a structured, manageable architecture as your system scales in complexity.

# Supercharging Customer Experiences Using GenAI and Knowledge Bases For Amazon Bedrock

## Introduction

‚Ä¢ AWS

‚Ä¢ Generative AI

There‚Äôs plenty of data available online that shows how critical customer service is for business success. A whopping 95% of consumers say that after-sales service is essential for brand loyalty, and 60% of consumers report having deserted a brand and switching to a competitor because of poor customer service. At the same time, cost controls and a clear return on investment are top of mind for everyone.

In these challenging circumstances, enterprises have been relying on chatbots more than ever to provide customers with answers to common questions and reduce the workload of overly strained contact centres.

Unfortunately, these chatbots are either easy or incredibly hard to implement without a middle ground. In the former case, they provide subpar-quality responses that only frustrate customers. In the latter case, they require extraordinary technical investments and specialized, hard-to-find expertise to simply retrain and maintain the bot‚Äôs quality.

At AWS re:Invent 2023, AWS announced the new Knowledge Bases for Amazon Bedrock service, which promises to change the game when building customer service products with Generative AI (GenAI), eliminating the need for model re-training by leveraging Retrieval Augmented Generation (RAG). For customer service chatbots, Knowledge Bases can act as an external repository for business context. AI models can provide responses that are informative, precise, and contextually enriched based on the vast data stores companies maintain.

Here, we‚Äôll explore a chatbot for a coffee chain that leverages GenAI to effectively respond to customer inquiries. By integrating customer survey data into a Bedrock Knowledge Base, businesses can empower their Generative AI systems to engage in more meaningful dialogues. For instance, when a customer inquires about a product feature or expresses a concern, the AI can retrieve specific feedback from the knowledge base to provide a tailored response. This action demonstrates an understanding of the customer‚Äôs issues and conveys that their feedback is valued and considered.

## Building a context-enriched chatbot is now just a few steps:

First, we upload the desired context documents to an S3 bucket. The Bedrock Knowledge Bases platform is adept at handling a diverse array of content types, ranging from Frequently Asked Questions (FAQs), product manuals, and customer service transcripts to more structured formats such as spreadsheets containing product information or CSV files cataloging customer feedback. Whether the data is presented as text, Markdown, HTML for web-based content, PDFs, Word documents, Excel files, or CSVs, Bedrock Knowledge Bases facilitates their automatic ingestion.

Next, we create a Knowledge Base with a vector store‚Äîa specialized database designed to organize and retrieve data based on content similarity. OpenSearch Serverless is the default vector store for its ease of use and scalability, although alternative options can also be utilized to fit different needs.

During the ingestion process, documents are broken down into smaller, digestible pieces called chunks. These chunks are then converted into embeddings, which are numerical representations that capture the essence and context of the text, making it understandable for the AI. These embeddings are then stored in the vector store. As the knowledge base evolves, it continues to automatically incorporate and index new content, ensuring that the AI‚Äôs responses remain dynamic and up-to-date.

Finally, we can connect our chatbot client to the pre-built RAG infrastructure with a single call to the new RetrieveAndGenerate API. Knowledge bases will convert user queries into embeddings, do a similarity search with the knowledge base to find relevant context, and trigger a response generation from a foundation model.

What used to be a complex workflow requiring many custom-built integrations is now as simple as adding business data to S3 and calling an API\! The customer service chatbot now knows of frequently asked questions and past customer surveys, which it uses to provide precise, personalized customer responses.

## Conclusion

By leveraging the new Knowledge Bases for Bedrock feature, you can employ advanced GenAI techniques to provide a superior customer experience in a fraction of the time. By integrating context and customer survey data into these knowledge bases, companies can create AI systems that not only generate responses but do so with an informed understanding of customer sentiment.

These AI systems bring an additional layer of sophistication to these interactions. Language models are equipped to gracefully handle scenarios where information may be incomplete. They excel in formulating probing questions that can extract more nuanced details, enriching the conversation and gathering the necessary context to provide accurate responses. The result is a superior experience provided to your customer by an AI that doesn‚Äôt just speak but engages in a conversation rooted in their needs.

The application of GenAI also plays a crucial role in the iterative improvement of AI models through reinforcement learning from human feedback. As AI systems interact with users and process their feedback, they can detect patterns and preferences. This information can be used to refine the AI‚Äôs decision-making processes, leading to more accurate and user-aligned responses over time.

As we continue to push the boundaries of what‚Äôs possible with Generative AI, Bedrock‚Äôs Knowledge Bases stand as a testament to the potential of AI to not only process information but to comprehend and act upon the wealth of human feedback.

In this context, OpsGuru‚Äôs expertise can help your business unlock its innovation potential to enhance customer responsiveness, and power new services and business models using Data & AI. If you want to explore how your business can unlock its innovation potential, get in touch with us.

# Generative AI for Business: How Amazon Q Business Delivers 10x ROI

## Introduction

‚Ä¢ Data and AI

‚Ä¢ Generative AI

‚Ä¢ Innovation

Information is power, but accessing that power can be a challenge when it‚Äôs locked away in silos and scattered across outdated systems. Organizations are under increasing pressure to do more with less in 2025‚Äôs economic climate. However, many still struggle with fragmented knowledge systems, leading to wasted time, missed opportunities, and hindered innovation.

Enter Amazon Q Business‚Äîa generative AI-powered assistant designed to reshape how enterprises access and leverage their information. Businesses can leverage Amazon Q Business to unify information access, improve organization and discoverability, and ensure robust security.

So, what does this mean for your business? Simply put, it means less time spent searching and more money saved. But before adopting any new technology, it‚Äôs essential to understand the financial impact. In this post, we‚Äôll break down the costs and benefits of Amazon Q Business to help you make a confident, informed decision.

## The Hidden Cost of Productivity Lost Due to Information Access

The average knowledge worker spends up to 30% of their time searching for information across disjointed systems. At an annual average wage of $50,000, that 30% translates to $15,000 in lost productivity per employee. The financial impact becomes immediately evident when you scale that across your organization.

While some searching is inevitable, excessive time hunting for information hinders productivity, stifles innovation, and ultimately impacts your bottom line. Fortunately, AI offers a powerful solution.

## What is Amazon Q Business and Why Does It Matter?

Amazon Q Business is a generative AI tool designed specifically for enterprises. It connects your organization‚Äôs data sources‚Äîsuch as SharePoint, Salesforce, and Amazon S3‚Äîand provides users with contextual, permission-aware answers in a secure, conversational interface.

‚ÄúTo boost productivity with GenAI, you have to get people to consistently use GenAI tools. In the 2024 Gartner Digital Worker Survey, digital workers who used everyday AI tools or applications for work purposes at least once a week saved an average of 3.6 hours per week.‚Äù

‚ÄîGartner, Pacing Yourself in the AI Races: 2024 IT Symposium/Xpo Keynote Insights

Unlike traditional enterprise search tools, Amazon Q Business leverages large language models (LLMs), retrieval-augmented generation (RAG), and other deep learning techniques to deliver relevant, understandable, and immediately actionable results.

The result? Less time digging, more time doing.

## 4 Key Features of Amazon Q Business

From secure access to real-time answers and automation, Amazon Q Business is designed to improve productivity and simplify how teams interact with their data.

1\. Secure, Role-Based Access: Ensures that users only see information they‚Äôre authorized to access.

2\. Real-Time Answers: Uses advanced text-based AI and neural networks to generate real-time relevant responses.

3\. Scalable Integrations: Connects easily to a wide range of enterprise data repositories.

4\. Application Automation: Power users can build and publish AI-driven apps to automate repetitive workflows.

Whether you‚Äôre looking to improve knowledge sharing, automate everyday tasks, or simply make better use of your existing training data and data sets, Amazon Q Business is built to scale with your needs.

## How to Estimate the Cost of Amazon Q Business

To understand the potential savings Amazon Q Business can deliver, it‚Äôs important to look at the full picture:

1\. The cost of user subscription

2\. The cost of Indexing

3\. The cost of engineering and human management

4\. Any other AWS costs

Below, we summarize the four key cost components directly affecting recurring savings with Amazon Q Business.

## User Subscription

Amazon Q Business offers two subscription tiers to cater to different needs and budgets:

1\. Amazon Q Business Lite ($3 per user per month): This affordable option provides the core features you need for secure enterprise information search. It allows your team to:

2\. Amazon Q Business Pro ($20 per user per month): This offers advanced features like application publishing and Amazon QuickSight integration for power users.

1\. Securely connect to your company‚Äôs knowledge base and data sources.

2\. Get permission-aware answers in a conversational interface.

3\. Quickly sign on with their existing enterprise login.

Amazon Q Business Lite will be the perfect solution for enhancing knowledge sharing and productivity for many organizations. It‚Äôs a cost-effective way to empower your teams with quick and secure access to the information they need.

## Indexing Cost

Amazon Q Business leverages retrieval-augmented generation (RAG) to index your documents and provide accurate, up-to-date answers. We recommend Amazon Kendra GenAI Enterprise Edition for enterprise-scale deployments, security, and control over your knowledge base.

## Engineering and Human Management Costs

Document management requires ongoing effort, and tasks like prioritizing documents and managing metadata will likely require some dedicated human expertise, even with automation. To account for this, let‚Äôs budget for an engineer partially dedicated to these tasks, with an estimated cost of roughly $100,000 per year for every 500 users.

## Other AWS Costs

The cost of essential services like CloudWatch and CloudTrail for monitoring and logging, such that the overall solution follows the AWS best practices. These costs are estimated to be around 5% of your subscription and Kendra costs.

## How to Estimate the ROI of Deploying Amazon Q to Facilitate Information Search

Based on our previous cost examples, the estimated total cost of ownership (TCO) ranges from $101,000 to $206,000 per year for 100 to 500 users, assuming each user accesses 500 documents and makes 200 daily queries.

To assess the potential recurring savings, we‚Äôve made some conservative estimates:

1\. Productivity Recovery: Limited to 10%, although research suggests potential losses of up to 30% due to inefficient search.

2\. Average User Salary: $50,000 per year, assuming the cost to maintain and manage the Amazon Q Business system is $100,000 per year per engineer.

Based on these assumptions, the chart below illustrates the potential productivity gains and cost savings you could achieve by implementing Amazon Q Business.

For 100 users, a 10% productivity recovery yields $500,000 in savings. At a total cost of ownership (TCO) of roughly $101,000 annually, that‚Äôs a 4x return on investment.

Scale to 500 users and the ROI can reach as high as 23x, depending on document volume and query frequency.

## Understanding Total Cost of Ownership and Engineering Costs with Amazon Q Business

It‚Äôs important to remember that engineering costs play a role in the overall cost-benefit analysis. As your user base grows, the need for dedicated engineering support increases, impacting the overall savings. For example, with 600 users, the recurring savings are lower than with 500 users, even with the same usage parameters. This highlights the importance of considering the full TCO, including engineering costs, when evaluating the potential benefits of Amazon Q Business.

## The Strategic Value Beyond Cost Savings

While the cost savings alone are compelling, the long-term value of Amazon Q Business goes beyond dollars and cents:

## Better Decision-Making

Access to accurate, contextual information enables teams to make smarter decisions faster. When your workforce is equipped with real-time, permission-aware answers, they can confidently move from analysis to action.

## Accelerated Innovation

By automating routine tasks and reducing friction in knowledge access, Amazon Q Business frees up time for creative thinking, strategic planning, and innovation. In short, your teams stop working on autopilot and start driving meaningful change.

## Turning Information Access into Measurable ROI

It‚Äôs natural to wonder if these numbers are too good to be true. While the potential savings from Amazon Q Business are significant, every organization is different. The impact of inefficient search and the costs and benefits of implementing Amazon Q Business will vary depending on your circumstances.

However, even with these nuances, the potential upside remains compelling. We‚Äôre still talking about the order of magnitude difference between the potential cost savings and the investment in Amazon Q Business.

At OpsGuru, we help organizations evaluate, implement, and optimize AWS-native solutions like Amazon Q Business. We bring deep expertise in generative AI work, fine-tuning large models, and aligning technology with business goals.

Let‚Äôs talk. Whether exploring AI for the first time or looking to scale your use of generative tools, we can help you build a roadmap that delivers real results.

# GitOps \- The Growing Landscape of Tools

## Introduction

In an earlier post we have discussed the advent of GitOps, what it is and why it has become so popular. In this post we are going to take a look into a few GitOps offerings. While the first GitOps product has been Weaveworks Cloud in 2017 (if memory serves me right), the landscape has since evolved. While GitOps originally is conceived for Kubernetes deployment, the term has expanded to usage beyond Kubernetes, including deployment onto other platforms and creation of infrastructure. It appears that GitOps is now a term that is interpreted literally: operations driven from Git.

A number of the products have been open-core i.e. core functionalities are open-sourced, while full suite of functionalities are only available based on subscriptions of commercial products. A few examples belonging to this category are Weaveworks Cloud with open-source Flux, CloudBees CI/CD with open-sourced CloudBees Jenkins X distro. A few of them are extended offerings from established products e.g. GitLab with Auto DevOps, GitHub Actions. Meanwhile, a number of open-sourced projects serve as archetype of what a GitOps pipeline should look like.

The survey below by no means captures all the services currently available; it is a sketch of a few offerings. We have explored these projects in particular because they are representatives of the landscape and many are well-known brands. We certainly look forward to other upcoming services that enrich the landscape solving the git-driven CI/CD challenge.

## GitLab Auto DevOps

If GitOps is about using Git as the source of truth of everything, including deployment and configurations, GitLab is certainly strongly behind the message. GitLab is promoting using Git as the enterprise collaboration tool because the mechanism is well-known and well-trusted.

Starting at GitLab 11.0, GitLab has introduced Auto DevOps, which is a suite of 12 features responsible for the range of build, test, static code analytics, dynamic application security testing, dependency scanning, license compliance, container scanning, deployment and monitoring. Though purported to work for infrastructure as well, the GitLab Auto DevOps offering is conceived for containerized applications. Since GitLab 12, Auto DevOps pipeline can be triggered automatically upon detection of the presence of the Dockerfile. The Auto DevOps suite incorporates a number of open-source tools, including Clair for container scanning, Helm for deployment packaging and templating and Prometheus for monitoring.

GitLab Auto DevOps work on Google Kubernetes Engine (GKE) and Amazon Elastic Kubernetes Service (EKS). It also has support for AWS Lambda and Knative running on either GKE and EKS. The full feature set of GitLab Auto DevOps is only available in the GitLab Gold package at $99 per month per user, which includes 50,000 pipeline minutes per month on shared runners.

## CloudBees CI/CD powered by Jenkins X

First announced in early December 2019, CloudBees CI/CD powered by Jenkins X is a hosted solution provided by CloudBees. Jenkins X is used to drive the CI/CD features of the product, while the pipeline coordination mechanism is based on Tekton. Both Jenkins X and Tekton are projects of the Cloud Delivery Foundation, providing continuous delivery features that are vendor-neutral and cloud-native.

Jenkins X is designed to operate on Kubernetes from the ground-up. Jenkins X attempts to provide a platform experience of Kubernetes. User should not be concerned be concerned about the underlying cloud provider (the managed Kubernetes from AWS, Google Cloud, Azure are supported, along with OpenShift, Pivotal, even Minikube and Minishift), it also supports spinning up preview environments as a test before code merges. Jenkins X has a command-line tool ‚Äújx‚Äù that can instigate multiple API calls to Kubernetes and cloud providers (it reminds of kubectl). A number of addon integration are possible, including Prometheus for monitoring, Istio for service mesh, Ambassador for API gateway and Anchore Engine for container security validation.

However, Jenkins X in CloudBees CI/CD powered by Jenkins X is based on the CloudBees distro of Jenkins X. The specific version of Jenkins is managed by CloudBees and has monthly release cadences. At time of writing, CloudBees Jenkins X distribution only supports GKE. It leverages heavily the concept of build pack, whereby the user will need to define the Dockerfile, Jenkinsfile and helm charts dedicated to the particular application. Build packs are organised based on physical locations of files in the GitHub repository. While the ‚Äújx‚Äù command-line tool to install Jenkins X functionality into GKE clusters, how addons are supported by the distro and how future addons are going to be made available is not yet well understood.

CloudBees CI/CD has highlighted however a number of features extraneous to the generic Jenkins X distribution. One such feature is the Hashicorp Vault integration, which is noteworthy because secrets management is often the Achilles heel for CI/CD systems, where hard-coded secrets are often stored internally and exposed as environment variables to be accessed during the pipeline actions. Because the CloudBees offering usages Tekton, the Kubernetes operator for pipeline, the Jenkins server component is no longer needed, making pipeline triggering for deployment to even other Kubernetes clusters, much simpler. In fact, automated environment promotion of a build is one of the features CloudBees advertises. The CloudBees CI/CD also promises a rich and intuitive UI for better user experience.

There are a number of unknown at the time of writing because CloudBees CI/CD powered by Jenkins X is still in preview. Feature-wise, it has potential to be like OpenShift with its own ecosystem of support of many tools; it appears that it can be integrated with other pieces of tools (e.g. Clair or Anchore Engine that is supported natively in Jenkins X). We look forward to more information ‚Äî including pricing, support and introduction of new services ‚Äî and actually testing the service out.

## Weaveworks Weave Cloud

When Weaveworks first promoted the term GitOps, it was in concert with the advent of the Weave Cloud service. The core technology of Weave Cloud CI/CD ‚Äî Flux ‚Äî has become a CNCF sandbox project in August 2019\. Flux is also a Kubernetes operator, it has the advantage of not needing to grant extraneous access to a CI system because Weave Cloud adheres to the essence of using Git as the single source of truth and collaboration workspace.

Unlike later comers such as GitLab Auto DevOps and CloudBees CI/CD powered by Jenkins X, Weave Cloud focuses on the delivery and monitoring capability for Kubernetes clusters. It assumes the container is already built and stored somewhere accessible ‚Äî the tutorials Weave Cloud use Travis CI for build and quay.io for container registry needs.

Because of the Flux CD origin, ‚Äúfluxctl‚Äù is the command-line tool that can be used to manage a workload, which is defined by the version of the image used in a pod (thus can be a deployment, daemonset, stateset and cronjob) and the targeted cluster. Deployment on Weave Cloud is triggered based on changes in the monitored container repository. By monitoring the container repository instead of the Git directly, it implies that Weave Cloud only expects the Kubernetes manifests to be made available in the Git repository instead of actual application source code. The Flux operator deployed onto Kubernetes clusters will monitor for the manifests on Git and compare it against the previous deployments stored in memcached deployed on the cluster, and deployment is triggered if a difference is discerned and when deployments policies allows.

Weave Cloud also assumes the manifest files are directly stored in the Git repository pointing to the versioned container image i.e. Weave Cloud does not support templating engines like Helm to dynamically resolve the manifests. Weave Cloud effectively takes a simplistic yet intuitive approach to Kubernetes deployment. It can be argued that the product is more similar to Spinnaker than a full development platform. Weave Cloud pricing is based on monthly node-seconds, a formula that addresses the ephemeral nature of nodes while supporting the traditional duration-based usage.

## GitHub Actions

Made generally available in November 2019, GitHub Actions is another new-comer into the GitOps landscape. It is easy to compare it against GitLab Auto DevOps, though at the initial release, GitHub actions appears to be a pipeline management tool with optional runners. Users can configure the pipeline based on specific needs or use third-party provided actions to perform specific needs, be it for container validation, testing, deployment, than a diverse platform with many features to support actions in the software delivery life cycles. Given the availability of third-party actions, it is expected that there will be more managed actions, including more in-depth Kubernetes actions, to make GitHub actions more feature-rich and powerful.

GitHub Actions focuses on convenience and easy adaptability. Arguably its simplicity and intuitiveness (as in not that different from other hosted CI/CD offering like CircleCI and TravisCI) can be attractive to its faithful followers who are deeply entrenched in the GitHub ecosystem. GitHub Actions are free for public repositories; for private repositories, the charge is based on minutes, storage or data transfer and associated overage charge after free tier is exhausted.

The OpsGuru team has done a deep-dive on setting up GitHub actions. Stay tuned for the post\!

## Argo CD

It is not a hosted service provided by a vendor (though there are independent vendors providing hosted support), Argo CD is arguably the most popular open-source project implementing GItOps pipeline. Argo CD needs to be mentioned in this post because it is the vendor-neutral archetype that demonstrates what a GitOps pipeline would look like. The continuous delivery mechanism is deployed as an operator onto a Kubernetes cluster, making pipelines a first-class resource in a Kubernetes cluster. The Argo CD Kubernetes controller than continuously monitors running applications and compares the current and desired state as stated in the Git repository. As is expected of any continuous delivery system, it provides a web UI that provides a real-time view of application activity. It supports multiple config management and templating tools including Kustomize, Helm, Jsonnet, and naturally the plain YAML.

## GitOps Engine

We have discussed earlier Weaveworks Cloud, which is based on the open-sourced Flux project and Argo CD above. Flux CD and Argo CD are the open-sourced standard for GitOps pipelines; hence it is very interesting how the two projects are joining forces for a collaboration that attempts to get the best of both worlds. The FAQ of the GitOps Engine project is especially interesting on how implementers of each of the projects can look forward to as a result of the joining of forces. Hopefully we can see a new and comprehensive standard emerging from this collaboration.

## Summary

The landscape of services providing Kubernetes-specific CI/CD pipelines is increasingly rich and competitive. With Kubernetes becoming the winner amongst container orchestrators, the user and platform experience becomes the gap that needs to be addressed to support Kubernetes‚Äô continuous lead. Multiple vendors and projects are providing solutions based on their own interpretation to the needs. When I started off writing this piece, I was aiming to address GitOps as a concept, but it is really exciting to see the different interpretations. I look forward to a write-up on as the landscape evolves.

GitOps is a particular way to implement CI/CD, which is essentially in any manageable deployments. How do you configure your CI/CD? Does GitOps make sense to you? We‚Äôd love to hear from you. In our final post, we will look into how to create a pipeline using GitHub Actions.

# GitOps \- Why is it Relevant Now?

## Introduction

Recently there seems to be a lot of talk about GitOps. This impression is certainly reinforced by the sessions and booths during KubeCon San Diego late 2019\. Regardless of the discipline or services, GitOps was the keyword that was constantly repeated. Despite a very branded beginning of the word (Weaveworks, with full credits, has done a great job promoting the concept), it seems to have quickly become a standard. As OpsGuru spends a lot of time working with clients implementing Day2Ops, including managing CI/CD and processes enabling cloud-native workloads, GitOps is such a relevant topic that it needs a deeper look.

## What is GitOps?

When Weaveworks first promoted the term GitOps, it was largely pointing to using Git as the source of truth for Kubernetes manifests and resource configurations. An automated pipeline is set up to run through the validation, test and deployment actions. As a result of the setup, the pipeline is triggered whenever the triggering event (e.g. pull request issued, successful merge to the production branch) is fired. The pre-configured workflows ‚Äî including necessary tests and validations ‚Äî generally lead to API calls to the targeted Kubernetes clusters, resulting in, for example, rollout of new versions of applications and services, creation of a new namespace to support a new tenant. The key is that no manual intervention is necessary.

Because Git is the standard option to store source code (including codified configurations), and any DevOps practices will mandate automated pipelines, the GitOps is the logical marriage of source code and automation pipeline. In fact, GitOps is only a slight extension from the infrastructure as code paradigm. After all, the next logical stage for codified infrastructure is a standardised workflow, and what more can be standardised than a predefined automated workflow?

One implicit requirement for GitOps, however, is declarative infrastructure. Declarative code is superior to procedural code because it is idempotent. Because declarative code only specifies desired state of the system post-application, the same desired state is valid regardless of the current state of the system. Kubernetes resources are declarative by nature, infrastructure-as-code tools such as Terraform and CloudFormation are also declaration-driven. Declarative infrastructure is popular because delegating the just-in-time action plan to the controllers of the respective resources is a lot more scalable and manageable. The alternative of determining all the applicable decision flows and execution paths of complex systems is simply not viable.

Figure 1: A typical GitOps pipeline: Git triggers the workflow, resulting in ‚Äúkubectl apply‚Äù at a Kubernetes cluster

## Why GitOps now?

The premise of GitOps is not different from the drivers of CI/CD, namely

‚Ä¢ Need for speed: one of the differentiation for Docker and containers is speed, especially when compared to bare-metal servers and virtual machines. The advantage of speedy deployments requires however a process that is equally speedy. The need for speed goes hand in hand with the need for transparency ‚Äî transparent code leads to quicker knowledge transfer and therefore increases confidence and quality.

‚Ä¢ Need for repeatability: one of the primary drivers of using version control is repeatability ‚Äî that the same software/configuration can be regenerated using the same code. As declarative (and implicitly idempotent) infrastructure and configuration is the expected norm, the code and instructions of the same version are expected to bring up the exact infrastructure, regardless of when the code is executed.

‚Ä¢ Need for a single source of truth: this is where Git shines over other version control mechanisms. What can be more convenient than using a version control system that allows developers to collaborate while keeping track of all the divergences and correlations? Git is accessible, it builds on a storage mechanism that is well-known and well-adopted by the majority of developers. Until another version control mechanism is introduced (blockchain\!\!), Git is the binary-based version control that is as good as cemented for most purposes.

‚Ä¢ Need for reliability with auditability: having Kubernetes manifests stored in a source code is the first step to reliability, but a proven and reliable mechanism to verify, apply and track the changes is also critical as part of the insurance to the credibility of the process. A standardized workflow that applies the changes is a lot more credible than an engineer being able to get the change and apply manually using ‚Äúkubectl.‚Äù

As such, why is it suddenly so popular? Arguably, it is because of the current features of the Kubernetes ecosystem.

The popularity of Kubernetes-targeted, opinionated CI/CD solutions from different vendors is not only an indicator of the popularity of Kubernetes, but it is also a signal of the need to introduce a platform-as-a-service experience that is thus far largely lacking in Kubernetes.

While Kubernetes grows in its feature set, Kubernetes user experience in the core project is still an area that lags behind the platform. The kubernetes/dashboard project has been attempting to fill the gap. However, a UI that assumes users to have relatively intimate knowledge of the container orchestration platform is still a high entry barrier to most developers whose primary concern is the administration of individual workloads instead of the cluster. Kubernetes is the primary choice of container orchestration, but it does not fulfil the user requirements on looking for a platform-as-a-service experience.

OpenShift arguably is the first product that identifies the need to address the platform-as-a-service gap for the Kubernetes experience. However, while RedHat continuously and heavily contributes to upstream Kubernetes, one cannot help but wonder about the potential divergence between OpenShift and Kubernetes. The slew of products emerging in the market aims to provide a packaged platform-as-a-service experience, including CI/CD, container registry, container scanning services. They aim to strongly reinforce the native Kubernetes platform by providing an experience that abstracts from the complexities of the platform.

In the next two weeks, we are going to explore a number of services available that answer the GitOps needs and compare the features and promises of a number of GitOps hosted services. We are also going to deep-dive into one such offering and examine how it can be incorporated to simplify some CI/CD pipeline setup.

Are you interested in learning how OpsGuru implements CI/CD, Day2Ops and how GitOps aligns with Day2Ops in a solution that works for you? We‚Äôd love to discuss a workflow that is customised to your needs\!

info@opsguru.com

# Is Your Organization Ready for a Service Mesh? (Service Mesh Part 3\)

## Introduction

By now, we hope you‚Äôve had the chance to read our Service Mesh blog series. In our previous posts, we shared how Service Meshes has the potential to cure many of your microservice pains by enabling canary deployments, ubiquitous monitoring, circuit breaking, and a bunch of other useful features. In fact, it can be tempting to put a few lower priority efforts on hold just to roll out a proof of concept in a non-prod cluster. Perhaps you have a new greenfield project that is begging for a service mesh to solve all the communication problems you had in the legacy system, and the timely rollout of a service mesh can kickstart the next wave of new features that your organization desperately needs.

Despite being one of the most powerful concepts to come out of the Cloud Native space, service meshes should be adopted with care. They are an operational subsystem that can take down your entire data plane (read: cause a catastrophic outage of your entire system) if not managed correctly. Furthermore, the location of Service Meshes on the data plane makes them tempting attack vectors and therefore high-maintenance in terms of keeping up-to-date with the latest releases and security patches. The purpose of this post is to provide simple criteria to help decide if your organization is prepared for the operational overhead of moving to a service mesh.

Across the OpsGuru team, we have assisted several organizations in adopting service mesh technology. Over the course of these projects, we have identified 3 DevOps capabilities that are essential to the success of service mesh adoption:

1\. Continuous Deployment of Infrastructure

2\. Independently Testable Microservices

3\. Well Defined SLOs, SLAs, and SLIs

In the sections below, we‚Äôll elaborate on these capabilities and why they are critical to adopting a service mesh.

## Continuous Deployment of Infrastructure

CI/CD is a well-known cornerstone of DevOps practices when it comes to application code. However, most organizations still struggle with introducing DevOps practices to their infrastructure. Infrequent changes, inherent statefulness, and the high cost of disruption are all factors that have stunted the maturity of infrastructure change automation in most organizations. Even when the components change infrequently or have a limited impact on other components, slowly but surely, deployed environments succumb to configuration drift, and automated processes are shelved and tagged for ‚Äúrefactoring‚Äù at some later date. Even when the drift is sub-optimal and error-prone, the drift is generally still manageable ‚Äì that is till Service Mesh is introduced.

The nature of a service mesh means that it can be:

‚Ä¢ an attack vector for each microservice

‚Ä¢ an attack vector for your control plane

‚Ä¢ a point of failure for each microservice

‚Ä¢ a single point of failure for your entire system (within a failure domain)

Furthermore, given the newness of the space and competitive market, all service mesh products are undergoing aggressive development. In practice, this means you can expect a fast release cadence and high frequency of critical patches (due to CVE disclosures as well as regressions) from whichever service mesh you adopt.

Therefore, before adopting a service mesh. Your teams must have the ability to:

‚Ä¢ Rapidly deploy a new version of the mesh to a non-production environment

‚Ä¢ Adequately test the mesh for regressions across all microservices

‚Ä¢ Upgrade production environments to a new version of the mesh without unplanned downtime

Note, there is no one-size-fits-all approach for managing these deployments. It can be achieved via in-place upgrades, blue-green deployments, or phased rollouts. What matters is that the processes are fully automated, reproducible, reliable, and testable.

## Full Stack Visibility

While a deficiency in infrastructure automation, like upstream vulnerabilities, can bring productivity to a halt, inadequate visibility can magnify simple application bugs into release-blocking time sinks. In this sense, visibility goes beyond simply setting up your favourite logging and monitoring systems. It also encompasses application and infrastructure design practices that enable a targeted analysis of each microservice‚Äôs behaviour independent of the service mesh and underlying cloud infrastructure.

Obviously, proper visibility is a benefit even without a service mesh. Modern applications are typically built with some combination of managed cloud services, third-party services, and proprietary microservices. Troubleshooting a misbehaving transaction requires identifying which subset of components was involved, reasoning about the role each one played, and then hypothesizing and testing until a root cause is identified. Without adequate visibility, this process is challenging at the best of times. However, adding a service mesh into the mix compounds required effort due to its relatively unique position in the stack. Few, if any, third party components are more closely integrated with application processes than service mesh proxies. While there are real benefits to encapsulating communication logic in a local proxy, the practice makes it hard to distinguish between application behaviour and proxy behaviour, especially when it comes to troubleshooting performance degradation and communication errors.

One of the confounding factors with visibility is that most service meshes promise rich monitoring and tracing functionality out of the box. This often lulls teams into believing they can ignore all visibility concerns and let the service mesh take care of it. The unavoidable truth, however, is that the service mesh provides only one perspective on system health ‚Äì as seen from the local proxies. For example, if the mesh control plane or proxies are disrupted, latency metrics may become unavailable for some workloads, or worse, they may become skewed due to time spent within proxy buffers, making them irrelevant to the application state. While this still provides sufficient signal to determine overall system health, it increases noise in root cause analysis. In other words, a service mesh does significantly increase the observability of your system, but it also adds a new failure mode. (It often requires external tooling to efficiently diagnose and debug. This particular concern varies quite a bit by implementation. Linkerd, for example; provides excellent tooling for visibility out of the box ‚Äì but the general case still stands.)

In order to prevent your teams from losing all momentum in troubleshooting integration issues, it is critical that they have logs and metrics, including the Four Golden Signals (Latency, Traffic, Errors and Saturation) readily available for each:

‚Ä¢ microservice workload instance (excluding the local proxy)

‚Ä¢ service mesh proxy instance

‚Ä¢ service mesh control plane component

The availability of similar signals for cloud and third-party components goes without saying, but is typically provided out of the box and therefore less likely to be overlooked. These signals should be exposed via a system that is external and independent of all service mesh components.

## Well Defined SLOs (Service Level Objectives)

Full-stack visibility provides the required insight into how each component is behaving but does not provide insight into how a microservice, or system as a whole, should behave. Defining desired behaviour is key to controlling the amount of effort as well as the implementation costs of any system. As soon as a service mesh is introduced, these controls quickly become essential as the complexity of a system increases.

SLOs are an efficient and measurable mechanism for describing desired behaviour. Typically they focus on the availability of a component, which includes some upper bound on latency. For example: ‚ÄúMicroservice A will return a non-5XX status code in under 200ms, 99.95% of the time‚Äù. The process of defining SLOs is beyond the scope of this post but once again, Google‚Äôs SRE book is a great place to start.

Adopting a service mesh increases the necessity of SLOs for several reasons:

‚Ä¢ Guides the judicious use of service mesh features

‚Ä¢ Prevents unnecessary engineering costs due to over-tuning

‚Ä¢ Provides testable contracts between microservices

‚Ä¢ Encourages cost-benefit discussions around service mesh features

First and foremost, adopting a service mesh means adopting an abundance of mechanisms for controlling the performance and availability of your microservices and system as a whole. Without a fixed objective, performance-minded teams can easily fall into the trap of over-utilizing features like circuit breakers and rate limiters. This increased demand adds new failure modes and complexity to the system and drives up the overall effort to stabilize and maintain it. On the other hand, SLOs also help identify which service mesh features should be seen as a necessary complexity for achieving desired behaviour.

Well-defined SLOs also simplify the development of new services that depend on existing microservices. The contracts represented by those SLOs can be used to reason about the maximum achievable performance of the new service and, in turn, the SLOs it can support. Furthermore, the act of defining SLOs themselves drives conversations around the costs associated with achieving them. Costs will always increase super-linearly with improvements to availability and latency. These costs apply to underlying infrastructure as well as to service mesh components and features. Oftentimes, stateful features (like rate limiting) are much more costly in terms of compute resources than simple traffic management features. At any significant scale, understanding these costs in relation to the behaviour they support is critical.

## In Summary

Service meshes are extremely helpful solutions to very real problems; but, like most technologies, they have their trade-offs. For organizations that haven‚Äôt reached a certain level of operational maturity, those trade-offs could be catastrophic. In this post, we‚Äôve laid out key criteria you can use to evaluate your team‚Äôs operational maturity. If your teams already have the ability to safely deploy infrastructure to live systems, quickly collect key metrics when troubleshooting, and understand the behaviour of each microservice, introducing a service mesh will not be a win. On the other hand, if they fall short in one or more of those areas, short term efforts are probably better spent improving operational maturity. Wherever you land, hopefully, this post helps save some time and stress during the evaluation process.

# Karpenter \- A New Way to Manage Kubernetes Node Groups

## Introduction

One of the most common discussions that happen when adopting Kubernetes is around autoscaling. You can autoscale your workloads horizontally or vertically, but the main challenge has always been the nodes.

The hypervisor doesn‚Äôt have visibility into what the container is actually consuming in a virtual machine, nor is it aware of the workload resource requirements, and without that information the cloud provider can‚Äôt reliably handle the node autoscaling. The solution was to let something that does have that information handle it, and so we have the Cluster Autoscaler.

The Cluster Autoscaler automatically adjusts the size of an autoscaling group (ASG), when a pod failed to run in the cluster due to insufficient resources, or when nodes in the cluster are underutilized for a set period of time, and their pods can fit into other existing nodes.

Looking at the above description, it seems like the Cluster Autoscaler is just fine, and in most cases it is, but what if you need a new type of node that isn‚Äôt available yet in your cluster‚Äôs nodegroups?

Most organizations will have their clusters deployed using some kind of infrastructure as code tool like Terraform or AWS Cloudformation, which means that updates to this codebase will be necessary when changing the node groups. Configuring details and restrictions of these node groups is not always a straightforward process either.

New nodes can also take a while to be available to Kubernetes, and once they are available you might still run into racing conditions scheduling pods into these nodes.

Recently, AWS released Karpenter to address these issues and bring a more native approach to managing your cluster nodes.

Let‚Äôs take a look at how both solutions work, current pros and cons.

## Cluster Autoscaler and Karpenter

How does the Cluster Autoscaler work?

‚Ä¢ We deploy a workload to the cluster

‚Ä¢ Kubernetes scheduler could not find a node that will fit our pod

‚Ä¢ Pod is marked as Pending and Unschedulable

‚Ä¢ Cluster Autoscaler looks for pods in a Pending state

‚Ä¢ It increases the ASG desired count if the pending pods do not fit in the current nodes

‚Ä¢ The ASG creates a new instance

‚Ä¢ Instance joins the cluster

‚Ä¢ Kubernetes scheduler finds the new node and, if the pod fits in it, assigns the pod to it

## Cluster Autoscaler and Karpenter

So the Cluster Autoscaler doesn‚Äôt really deal with the nodes themselves, it just adjusts the AWS ASG and lets AWS take care of everything else on the infrastructure side, and relies on the Kubernetes scheduler to assign the pod to a node.

While this works, it can introduce a number of failure modes, like a racing condition having a pod being assigned to your new node before your old pod, triggering the whole loop again and leaving your pod pending for a longer period.

What about Karpenter?

Karpenter does not manipulate ASGs, it handles the instances directly. Instead of creating code to deploy a new node group, then target your workload to that group, you just deploy your workload, and Karpenter will create an EC2 instance that matches your constraints, if it has a matching Provisioner. A Provisioner in Karpenter is a manifest that describes a node group. You can have multiple Provisioners for different needs, just like node groups.

Ok, if its like node groups, what is the advantage? The catch is in the way that Karpenter works. Let‚Äôs do the same exercise we did for the Cluster Autoscaler, but now with Karpenter.

‚Ä¢ We deploy a workload to the cluster

‚Ä¢ Kubernetes scheduler could not find a node that will fit our pod

‚Ä¢ Pod is marked as Pending and Unschedulable

‚Ä¢ Karpenter evaluates the resources and constraints of the Unschedulable pods against the available Provisioners and creates matching EC2 instances

‚Ä¢ Instance(s) joins the cluster

‚Ä¢ Karpenter immediately binds the pods to the new node(s) without waiting for the Kubernetes scheduler

## Cluster Autoscaler and Karpenter

Just by not relying on ASGs and handling the nodes itself, it cuts on the time needed to provision a new node, as it doesn‚Äôt need to wait for the ASG to respond to a change in its sizing, it can request a new instance in seconds.

In our tests, a pending pod got a node created for it in 2 seconds, and was running in about 1 minute in average, versus 2 to 5 minutes with the Cluster Autoscaler.

The possible racing condition we talked about before, is not possible in this model as the pods are immediately assigned to the new nodes.

Other interesting things the Provisioner can do is setting a ttl for empty nodes, so a node that has no pods, other than DaemonSet pods, is terminated when the ttl is reached.

It can also ensure nodes are current by enforcing a ttl for the nodes in general, meaning a node is recycled once the ttl is reached.

Ok\! So Karpenter is great, let‚Äôs dump the Cluster Autoscaler\! Not so fast\! There is one feature that Karpenter is missing from Cluster Autoscaler, which is rebalancing nodes, the later can drain a node when its utilization falls under a certain threshold and its pods fit in other nodes.

## Talk is Cheap\! Show me the demo\!

Let‚Äôs get this running\! We‚Äôre following the getting started guide from karpenter.sh with a couple twists.

At the time this post was written Karpenter 0.5.2 was the latest version available.

First the good old warning for all demo code.

WARNING\! This code is for use in testing only, broad permissions are given to Kar

# Elevating Women in Tech: Insights from OpsGuru's Mency Woo

## Introduction

In the tech industry, where women often face significant barriers and remain underrepresented in leadership roles, OpsGuru is proud to be an organization that challenges this norm, embracing the principles of equality, equity, and inclusion. We‚Äôre excited to spotlight Mency Woo, OpsGuru‚Äôs co-founder and VP of Enterprise Transformation, who provides invaluable guidance for women aspiring to excel in the cloud sector.

Mency is a visionary leader, dedicated to guiding companies through technological adoption and harnessing the cloud‚Äôs limitless potential to foster business growth. In this short interview with Mency, we explore her journey in tech, the obstacles she‚Äôs navigated as a woman in the field, and the wisdom she imparts to those just starting out.

## What inspired you to pursue a career in the tech industry?

Growing up, one of my favourite toys was Lego. The ability to assemble the pieces together for different purposes fascinated me. My interest in combining things to build solutions naturally steered me toward a career in technology.

## What challenges have you faced as a woman in tech, and how have you overcome them?

In the early stages of my career, I was overly concerned about avoiding mistakes, leading to hesitation and missed opportunities. Although I can‚Äôt claim to have overcome these fears entirely, my awareness has significantly improved. Nowadays, I concentrate on interpreting signals and adjusting my approach as needed, essentially embracing an agile methodology in my work.

## What advice would you give to women just starting out in tech?

My advice to women embarking on their tech careers is not to aim for perfection. You were hired for the value you bring, not because you are flawless. Many of us experience imposter syndrome to varying degrees, yet it‚Äôs crucial not to undervalue our capacity to learn and adapt. Become your most formidable advocate and relentlessly push forward\!

## What skills do you believe are crucial for success in the tech industry?

In the tech world, two big skills stand out for anyone looking to do well: understanding how different parts of a system work together and always being ready to learn new things. During my time working in professional services, I‚Äôve met lots of smart people who are great at solving specific problems. But when you‚Äôre dealing with bigger, more complex issues‚Äîlike figuring out how to adopt cloud technology or keeping up with fast changes in tech‚Äîit‚Äôs really important to see how everything connects. That‚Äôs what makes the difference in solving tough problems and coming up with new ideas.

On the other hand, always learning new stuff is super important too, especially in tech where things change all the time. Just think, a couple of years ago, artificial intelligence (AI) was just a small topic for most people. Now, it‚Äôs a huge deal, and we‚Äôre even talking about artificial general intelligence. That shows just how fast things move and why we need to keep up by learning all the time.

## Are there any specific qualifications or certifications that have significantly helped your career?

It goes without saying that, in my field of cloud computing, certifications are crucial. They serve as gateways to opportunities, but true expertise and mastery are honed through tackling real-world challenges.

## How important has mentorship been in your career?

As for mentorship, it has been invaluable to my career. Without a doubt, the trajectory of my professional life would have been markedly different without the guidance and examples set by my mentors. They‚Äôve played a pivotal role in showing me the importance of continual learning and development and have steered me through several critical decisions.

## Read More Inspiring Stories from Trailblazing Women in the Cloud Sector

If you want to read more insights and expert guidance from some more incredible female leaders in the tech sector, check out our blog, Voices of Women in Tech: Highlighting Women in Cloud. Discover their empowering stories and invaluable advice to fuel your journey in technology.

And, if you‚Äôre looking to get into or expand your expertise in the cloud industry, check out our current job openings at OpsGuru.

## 

# OpsGuru‚Äôs Mency Woo Inducted into CDN Women in the Channel Hall of Fame

## Introduction

The annual Canadian Women in the IT Channel Recognition Luncheon recognizes women for their prominent roles in the channel and their profound impact on the tech sector. We are thrilled to share that OpsGuru‚Äôs Mency Woo was inducted into the Channel Daily News (CDN) Women in the Channel Hall of Fame at this year‚Äôs event.

Congratulations from the OpsGuru team on being recognized by CDN for your contributions to the IT channel\!

One of the biggest hurdles in the tech industry is the lack of representation of women in leadership roles and CDN highlights an elite subset of female executives whose insight and leadership within their respective fields leads to channel success.

Mency Woo, VP, Enterprise Transformation at OpsGuru, provides thought leadership to companies looking to adopt technologies and services from Amazon Web Services, Microsoft Azure or Google Cloud. Mency helps business executives make technology accessible and to harness the cloud‚Äôs potential to accelerate business growth.

‚ÄúMency‚Äôs common sense combined with her technical and implementation experience enables her to present the most affordable and suitable solutions that solve real business problems. This makes her a trusted advisor at Equinox Gold.‚Äù said Miller Dussan, VP of IT at Equinox Gold.

After over a decade of working with cloud computing, Mency cofounded OpsGuru in 2017\. Through roles spanning Consulting, Engineering, Product Management, Marketing and Partner Management, Mency helped establish OpsGuru as the preeminent Canadian multi-cloud consulting partner.

‚ÄúI‚Äôm grateful to my mentors for the opportunities they have afforded me on my journey from Engineer to Executive. It is important that I now pay it forward, and I want to ensure that others receive similar guidance from me. Technology careers, including advancement and fulfillment, should be accessible to all genders and backgrounds.‚Äù said Mency.

Are you interested in working with leading-edge technologies? OpsGuru is always looking for highly skilled engineers and architects. Visit our careers page to find the role that‚Äôs right for you.

For more information, please read our press release.

## 

# Mining and Metals Digital Transformation Report 2021

## Introduction

The Mining industry is evolving faster than ever ‚Äì cloud technology is a critical component of that process.

The Mining & Metals Digital Transformation Report 2021 provides a comprehensive overview of the key benefits and obstacles to operationalizing cloud technology in mining and exploration.

Download the report and learn how mining companies are taking advantage of cloud technologies to evolve and innovate.

# OpenSource Data Lake for the Hybrid Cloud \- Part 1

## Introduction

Data lakes have become the de-facto standard for Enterprises and Corporations looking to harness value and take advantage of their existing data. Ultimately, businesses do not care whether they‚Äôre running their workloads on public, private or hybrid cloud. They just want to make sure that they do not miss out on the opportunities their data offers.

Through our experience we‚Äôre convinced that when it comes to deploying data lakes, the public cloud is by far the cheapest option for deploying most of the data lake solutions.

The public cloud offers:

‚Ä¢ Independently scaling of storage and compute capacity (don‚Äôt run servers when they‚Äôre not needed)

‚Ä¢ Rapid innovation cycles providing virtually infinite scale spin up and spin down capabilities to test theories as fast as they are thought of

‚Ä¢ Simple storage for both relational and non relational data

‚Ä¢ Pay as you go flexibility

‚Ä¢ Spot or preemptive compute capacity offering significant savings for batch analytics spikes

In this series of blogs, we will share some of OpsGuru‚Äôs experience building Open Source data lakes on public and hybrid clouds. We will also provide some detailed architecture examples of the most critical components for any data lake.

## Why should I care about an Open Source Data Lake?

Two common questions we hear from our customers: ‚ÄúWhat value do Open Source solutions provide?‚Äù and ‚ÄúWhy should I investigate building my own solution when my cloud provider can build one for me?‚Äù

First, let‚Äôs analyse in detail what it is that you‚Äôre really deciding when choosing cloud vendor solutions versus your own:

1\) The True Total Cost of Ownership:

We will conduct a basic cost analysis of an ingestion pipeline running on AWS. We assume that we are evaluating Apache Kafka as an alternative to Kinesis Streams.

## Kinesis Streams

Assuming we have a single stream that is used primarily for data ingestion:

In order to ingest 50,000 messages per second with 24h of data retention, an annual cost of $52,400 is expected. This number doubles to $104,800 if your Kinesis stream is storing data with increased retention (up to a week).

## Open Source:

Now, let‚Äôs look at an alternative, Kafka and Zookeeper running on top of Amazon EC2 or Kubernetes utilizing Stateful Sets.

A few assumptions are made here of course, compression (such as: ‚Äògzip‚Äô) is being used and we are replicating data (replication factor of 3\) for increased redundancy:

‚Ä¢ 3 x m3.medium machines with 10GB io1 EBS volumes for Zookeeper

‚Ä¢ 3 x m5.2xlarge machines with 5TB st1 EBS volumes for Kafka

The cost of the infrastructure that is confidently able to handle similar workloads on demand, will be $1,840 per month which runs an annual cost of $22,080 per year. This excludes any other potential instance savings. Even if you factor-in some engineering effort the annual cost of your Open Source solution will be considerably cheaper than a vendor based solution. This is especially evident when you extrapolate out over longer terms:

As shown above, we have factored in an initial expenses of approximately $60,000 for deploying an Open Source solution either through your own staff or by using a consultant, like OpsGuru. In this example, the break-even point happens within the first year of operation. Overall Kinesis is not cheaper than the Open Source example, and offers you less flexibility while locking your services into a specific cloud vendor. If you factor data growth of your streaming solution ‚Äì the break-even point arrives even sooner.

## Managed Kafka:

AWS recently released a managed Kafka solution, Amazon MSK, with similar configurations of instances as per our previous examples. This can be a much more appealing option, however it‚Äôs important to also conduct a cost benefit example for Amazon MSK. Depending on your configuration, our modelling shows that the calculated costs of Amazon MSK can be almost twice as expensive as the Open Source alternative.

An Amazon MSK deployment with almost identical configurations (3x kafka.m5.2xlarge brokers with similar storage) can cost $3,350 a month or $40,200 a year. While this is already almost double the cost of the Open Source alternative, it‚Äôs also important to note some rather large feature gaps when utilizing the service. Amazon MSK requires that you deploy your Kafka cluster across a minimum of 3 availability zones. While this is great for redundancy, there is a rather large hidden transfer cost when you factor in your application consuming data from brokers across availability zones. When you factor the $0.01/GB charge for data transfers you will find a pipeline processing 50,000 messages per second (of an average size of 10KB) will transfer \~500GB of data each day across availability zones. This adds an additional \~$4,562 per month and gives us a more accurate cost estimate of $90,000 to $100,000 per year.

As your streaming throughput and retention requirements increase, vendor solutions quickly become less appealing. This can be well summarised as a rent vs buy argument, just like if you were looking for a place to live. We have seen time and time again, that an Open Source solution becomes cheaper and more appealing as volumes grow and time goes by.

\* Reference: AWS cost calculator.

## Vendor lock-in

Another key discussion point for an approach to deploying a data lake is vendor lock-in.

The main question we believe you need to ask yourself is: Given our business requirements to remain flexible across multiple cloud providers. Will our teams be able to migrate our services from vendor A to vendor B, without a significant engineering effort, at relatively low cost, and in a timely manner?

Obviously, there are some services that are so tightly integrated into the vendor‚Äôs platform that it is almost never worth trying to implement an Open Source alternative. Object stores like Amazon S3 and Google Cloud Storage are great examples. These global scale systems are very simple in an operational sense and in some cases even compatible with each others APIs. Because of this, we recommend utilizing a hybrid approach where some base primitives are consumed from a cloud vendor (Kubernetes, Blob Storage, etc.) but you still maintain the flexibility Open Source systems provide.

Developing using proprietary SDKs can be a significant sunk cost. Ultimately if another cloud provider offers a significant saving on compute, you want to be able to chase that opportunity without worrying about portability.

Thankfully, there‚Äôs also a good answer to this. Kubernetes has become the industry standard method for orchestrating technology and is heralded because of its portability. If we base ourselves in Kubernetes, one deployment script can be utilised to deploy to AWS, GCP, Azure or on premises with very little extra effort.

By approaching the vendor lock-in problem pragmatically, we combine the power of ‚Äúthe cloud‚Äù with the portability to take your services anywhere. This sets up your business for long term success.

## Increased performance, visibility and customization:

Another reason to investigate your own customized solution for services such as Kafka is increased performance.

We work with clients both large and small who want to squeeze every last drop of performance out existing compute resources. By allowing flexible instance types, storage and opening up kernel parameters we have seen significant increases of throughput without costing any more. Being able to attach to and configure the runtime layer (for example JVM) also affords benefits for performance tuning.

Our experience shows that vendor provided visibility services are rather basic and lacking. By running your own Open Source solution you are free to utilize whatever monitoring solution you use, or to run our preferred Open Source system ‚Äì Prometheus. With dedicated exporters you can export any relevant service metric and gain better insights into the service internals for a much lower cost.

## Security and compliance requirements:

Strict security and compliance requirements are another reason for choosing to deploy an Open Source data lake.

Some regulations require strict data locality enforcement and/or no internet access (even in an encrypted form). Many of the systems deployed by cloud vendors do not allow for this type of deployment.

Additionally, there are cases where customer managed encryption keys or HSM keys are required. These are not always supported by a specific vendor service and may require downgrade functionality if used. By deploying an Open Source solution you are in control of encryption at rest and in transit which allows you to configure strict security policies when used.

We have deployed Open Source solutions in some of the most secure and regulated sectors on the planet. Many companies will simply not be able to use cloud vendor services which do not meet their security requirements.

## Limited feature sets and regional availability:

Cloud managed services may not always support the features offered by the Open Source software they are based on. If we take the AWS managed Kafka service as an example, at the time of writing, the following could be considered key blockers for your workloads:

‚Ä¢ No in-place rolling upgrades, cluster migration required to upgrade software

‚Ä¢ Limited broker version support (1.1.1 and 2.1 only, at the time of writing)

‚Ä¢ No custom 3rd party jars such as data balancers or metrics exporters

‚Ä¢ No hosted schema registry

‚Ä¢ Inter-zone network transfer costs caused by clients consuming and producing from brokers

Additionally, even within the same cloud vendor, often services are regionally available. Cloud vendors prioritize feature releases to their largest or most strategic regions. As an example, Amazon MSK is not available in the Canadian region meaning if your deployments required Canadian data locality, you will need to investigate your own solution there.

It‚Äôs important to note that it took Amazon MQ (a managed message queue) more than a year an a half to be available in the Canadian region after the initial launch of the service.

Now that we have established what might drive an organisation to adopt an Open Source data lake, we will talk about some of the design patterns and considerations to deploying them on the cloud.

We expect this series of blog posts will cover the end to end lifecycle of deploying and managing data lakes.

If you have any questions or queries, feel free to reach out to info@opsguru.com to discuss further.

# OpenSource Data Lake for the Hybrid Cloud \- Part 2: Designing an OSS DataLake

## Introduction

In part 1 of this series, we answered the question of WHY open source components are often an attractive option when building a data lake of any significant size. In this second installment, we describe HOW to cost-effectively build a data lake out of open source components. We will share common architectural patterns as well as critical implementation details for the key components.

## Designing an Open Source Data Lake

A typical data lake‚Äôs logical flow is comprised of these functional blocks:

‚Ä¢ Data Sources

‚Ä¢ Data Ingestion

‚Ä¢ Storage Tier

‚Ä¢ Data Processing & Enrichment

‚Ä¢ Data Analysis Exploration

In this context, the data sources are generally streams or collections of raw, event-driven data (e.g. logs, clicks, IoT telemetry, transactions). A key characteristic of these data sources is that the data is far from clean ‚Äì often due to constraints of time or compute power during collection. Noise in this data usually consists of duplicate or incomplete records with redundant or erroneous fields.

Given one or more data sources, we consume the raw data via an ingestion phase. The ingestion mechanism is most often implemented as one or more distributed message queues with a lightweight computational component responsible for initial data sanitization and persistence. In order to build an efficient, scalable, and coherent data lake, we strongly recommend a clear distinction between simple data sanitization and more complex enrichment tasks. One rule of thumb is that sanitization tasks should only require data from a single source and within a reasonable sliding window.

For example, a deduplication task that is bounded to only consider keys from events that are received within 60 seconds of each other from the same data source would be a typical sanitization task. On the other hand, a task that aggregates data from multiple data sources and/or across a relatively long time span (e.g. the last 24 hours) would probably be better suited for the batch analytics enrichment phase (which we will talk about below).

Once data has been ingested and sanitized, it is persisted into an object store/distributed file system to ensure resilience from any subsequent component failure. The data is normally written in a columnar format such as Parquet or ORC and is usually compressed via fast protocol such as Snappy for maximum storage efficiency and query performance.

When new data is written into the storage tier, a data catalog (which hosts the schema and the underlying metadata) can be dynamically updated using a serverless function crawler. The execution of such data crawler is normally event driven (arrival of new file into a specific location on an object store). Data stores are normally integrated with the data catalog in order to infer the underlying schema to make the data queryable.

The data usually lands in a dedicated location (or a ‚Äúzone‚Äù) dedicated to the golden data. The data is called golden for a reason: it is still raw, semi-structured or unprocessed and it is your business logic‚Äôs primary ‚Äúsource of truth‚Äù. From here on the data is ready to be further enriched by the subsequent data pipelines.

During the enrichment process, the data is further modified and distilled according to the business logic. It is eventually stored in a structured format in one of the data stores (e.g. a document store, an RDBMS or an object store) that may be dedicated to on-line serving, BI analytics, data warehousing or model training.

Lastly, the analysis and data exploration is where the data consumption occurs. This is where the distilled data is transformed into business insights through visualizations, BI dashboards, reports and views. It is also a source of ML predictions, the outcome of which helps drive better business decisions.

## Platform Components

A hybrid cloud data lake architecture requires a reliable and unified core abstraction layer that will allow us to deploy, coordinate, and run our workloads without being constrained by vendor API‚Äôs and resource primitives. Kubernetes is a great tool for this job since it allows us to efficiently deploy, orchestrate and run various data lake services and workloads in a reliable and cost efficient manner while exposing a unified API whether it is running on-premise or on any public or private cloud. We will dive deeper into the specifics of Kubernetes implementation details in a future post.

From a platform perspective, the foundation layer is where we deploy Kubernetes or equivalent thereof. The same foundation can be used to handle workloads beyond the data lake. A future-proof foundation layer incorporates cloud vendor best practices (functional and organizational account segregation, logging and auditing, minimal access design, vulnerability scanning and reporting, network architecture, IAM architecture etc) in order to achieve the necessary levels of security and compliance.

Above the foundation layer, there are two additional layers ‚Äì the data lake and the data value derivation layers.

These two layers are mainly responsible for the core business logic as well as data platform pipelines. While there are many ways to host these two layers, Kubernetes is once again a good option because of its flexibility to support different workloads both stateless and stateful.

The data lake layer, typically includes all the necessary services that are responsible for ingestion (Kafka, Kafka Connect), filtering, enrichment and processing (Flink and Spark), workflow management (Airflow) as well as data stores such as distributed file-systems (HDFS) as well as RDBMS and NoSQL databases.

The uppermost layer, data value derivation, is essentially the ‚Äúconsumer‚Äù layer and includes components such as visualisation tools for BI insights, ad-hoc data exploration via data-science notebooks (Jupyter). Another important process that takes place on this layer is ML model training leveraging data sets residing on the data-lake.

It is important to mention that an integral part of every production grade data lake is the full adoption of common DevOps best practices such as infrastructure as code, observability, audit and security. These play a critical role in the solution and should be applied on every single layer in order to enable the necessary level of compliance, security and operational excellence.

‚Äî

Now, let‚Äôs deep dive further into the data lake architecture and review some of the core technologies involved in the process of ingestion, filtering, processing and storing our data. A good guiding principle for choosing open source solutions for any of the data lake stages is to look for a track record of wide industry adoption, comprehensive documentation and, of course, extensive community support.

A Kafka cluster will receive the raw and unfiltered messages and will function as the data lake ingestion tier with its reliable message persistence and ability to support very high message throughput in a robust way. The cluster typically contains several topics for raw, processed (for stream processing) and dead letter (for malformed messages). For maximum security the brokers endpoints can terminate SSL, while encryption is turned on in the persistence volumes.

From that point, a Flink job consumes the messages from Kafka‚Äôs raw data topic and performs the required filtering and, when needed, initial enrichment. The data is then produced back to Kafka (into a separate topic dedicated to filtered/enriched data). In the event of failure, or when business logic changes, these messages can be replayed from the beginning of the log since they are persisted in Kafka. This is very common in streaming pipelines.

Meanwhile, any malformed, illegal messages are written by Flink into the dead letter topic for further analysis.

Using a Kafka Connect fleet backed by storage connectors, we are then able to persist the data into the relevant data store backends such as a golden zone on HDFS. In the event of a traffic spike, the Kafka Connect deployment can easily scale out to support a higher degree of parallelism resulting in higher ingestion throughput:

While writing into HDFS from Kafka Connect, it is usually a good idea to perform a content (topic) and date-based partitioning for query efficiency (less data to scan meaning less IO), for example:

hdfs://datalake-vol/golden/topic\_name/2019/09/01/01/datafoo.snappy.parquet

hdfs://datalake-vol/golden/topic\_name/2019/09/01/02/databar.snappy.parquet

Once the data was written to HDFS, a periodically scheduled serverless function (such as OpenWhisk or Knative) updates the metastore (which contains the metadata and schema settings) with the updated structure of the schema, so that it can be queried via SQL-like interfaces, such as Hive or Presto:

For the subsequent data flows and ETL coordination we can leverage Apache Airflow, which allows users to launch multi-step data pipelines using a simple Python object Directed Acyclic Graph (DAG). A user can define dependencies, programmatically construct complex workflows, and monitor scheduled jobs in an expressive UI.

Airflow can also potentially handle the data pipeline for all things external to the cloud provider (e.g. pulling in records from an external API and storing in the persistence tier).

Being orchestrated by Airflow via dedicated operator plugin, Spark can then periodically further enrich the raw filtered data according to the business logic and prepare the data for consumption and exploration by your data scientists, business analysts and BI teams.

The data science team will be able to leverage JupyterHub to serve the Jupyter Notebooks, therefore enable effective multi-user, collaborative notebook interfaces with your data leveraging the Spark execution engine to perform aggregations and analysis.

The team can also leverage frameworks such as Kubeflow as a production-grade ML model training leveraging the scalability of Kubernetes. The resultant machine learning models can later be fed back into the serving layer.

Gluing all the pieces of the puzzle, the final architecture will look something like this:

## Operational excellence

We‚Äôve already mentioned that DevOps and DevSecOps principles are core components of every data lake and should never be overlooked. With great power comes great responsibility, especially when your business has structured and unstructured data now residing in one place.

One of the recommended approaches is to allow access only to specific services (via appropriate IAM service roles) and block any direct user access so that data cannot be manually altered by your team members. Also, a full audit with relevant trail services is essential for monitoring and safeguarding the data.

Data encryption is another important mechanism to protect your data. Data encryption at rest can be done by using KMS services to encrypt your persistent volumes for stateful sets and object store, while data encryption in transit can be achieved by using certificates on all UI‚Äôs as well as services such as Kafka, ElasticSearch endpoints.

We recommend a serverless scanner for resources that aren‚Äôt complaint with your policies, such that it is easy to discover issues as such untagged resources, non restrictive security groups.

We discourage any manual, ad-hoc deployments for any component of the data lake; every change should originate in a version control and go through a series of CI tests (regression, smoke tests etc) before getting deployed into the production data lake environment.

## Cloud Native Data Lake ‚Äì Epilogue

In this series of blog posts we‚Äôve demonstrated the rationale and the architectural design of an open source data lake. As in most cases in IT, the choice of whether to adopt or not to adopt such an approach is not always obvious and can be dictated by a wide array of business and compliance requirements, budget and time constraints.

It is important to understand that the real cost benefit is usually observed when the solution is deployed at scale, since there is an initial investment in a platform that is made to support this flexible model of operation (as we have demonstrated in part 1).

Going with a cloud native data lake platform (whether it is hybrid or fully cloud native solution) is clearly a growing trend in the industry given the sheer amount of benefits this model offers. It has a high level of flexibility and protects against increasing lock-in. In the next installment we are going to drill down into a Kubernetes abstraction that enables hybrid data lake implementation.

Written by: Paul Podolny

# OpsGuru Achieves AWS DevOps Competency

## Introduction

‚Ä¢ AWS

‚Ä¢ DevOps

OpsGuru, Kubernetes and Data Analytics Experts empowering businesses to accelerate Cloud-Native adoption, announced today that it has achieved Amazon Web Services (AWS) DevOps Competency status. This designation recognizes that OpsGuru provides deep expertise to help customers implement continuous integration and continuous delivery practices or helps them automate infrastructure provisioning and management with configuration management tools on AWS.

Achieving the AWS DevOps Competency differentiates OpsGuru as an AWS Partner Network (APN) member that provides specialized demonstrated technical proficiency and proven customer success with a specific focus on Infrastructure as Code, Continuous Integration & Continuous Delivery and Performance. To receive the designation, APN Partners must possess deep AWS expertise and deliver solutions seamlessly on AWS.

‚ÄúOpsGuru is proud to achieve AWS DevOps Competency status,‚Äù said Anton Mishel, CEO of OpsGuru. ‚ÄúOur team is dedicated to helping companies achieve their technology goals by leveraging the agility, breadth of services, and pace of innovation that AWS provides. We have seen many companies start rapidly on the cloud, but soon run into the roadblock of cloud operations. OpsGuru has been awarded the DevOps competency because we have successfully helped clients find the right recipe for sustainable cloud success through infrastructure as code and different tiers of application and infrastructure CI/CD. Automation is an essential component for businesses to truly realize scalability, security and elasticity of running workloads on the cloud.‚Äù

AWS is enabling scalable, flexible, and cost-effective solutions from startups to global enterprises. To support the seamless integration and deployment of these solutions, AWS established the AWS Competency Program to help customers identify Consulting and Technology APN Partners with deep industry experience and expertise.

In addition to the AWS DevOps Competency, OpsGuru holds the AWS Migration Competency, and the AWS SaaS Competency, which further supports customers looking to modernize their solutions and ultimately provide SaaS procurement of their existing or new technology. OpsGuru combines deep migration and modernization experience with the Public Sector, Immersion Day and AWS EC2 for Microsoft Windows Server service delivery statuses to ensure that businesses can take full advantage of the cloud.

## 

‚Ä¢ AWS

‚Ä¢ DevOps

# OpsGuru Achieves AWS Generative AI Competency

## Introduction

‚Ä¢ AWS

‚Ä¢ Generative AI

OpsGuru is proud to announce that we have achieved the AWS Generative AI Competency, further solidifying its position as an AWS Premier Partner and a leader in Generative AI solutions. This competency illustrates our organization‚Äôs technical expertise, dedication to innovation, and infrastructure, which are pivotal for implementing Generative AI technologies for customers.

‚ÄúWe are proud to receive this competency, demonstrating OpsGuru‚Äôs expertise in leveraging AWS Generative AI tools to craft innovative solutions for our valued customers,‚Äù said Ryan Smyth, President & CEO of OpsGuru. ‚ÄúOur team is dedicated to leveraging AWS to implement generative AI solutions that drive business growth and ensure success in a competitive market.‚Äù

Generative AI has revealed a world of opportunities. Even if you are just beginning your journey, our team can help you make strategic decisions on applying AI technology to drive your business outcomes and eliminate the risk of misguided investment decisions. OpsGuru can customize, build and scale innovations using advanced AWS Generative AI technologies, including Amazon Bedrock, Amazon SageMaker Jumpstart, Amazon CodeWhisperer, Amazon Q, AWS Trainium, and AWS Inferentia.

Additionally, OpsGuru offers various Generative AI services, including the GenAI ideator workshop, GenAI Clear Path Forward featuring technical design and project roadmaps, and the GenAI Virtual Team, which allows for the rapid implementation of AI solutions.

Generative AI can provide business solutions such as enhanced data analysis and visualization, intelligent workflow automation, AI-driven content curation, and automated reporting.

Your business can also leverage Generative AI to revolutionize your customer experience through advanced search capabilities, improve customer support with intuitive Gen AI-based Chatbots, and cross language barriers by adjusting to the customer‚Äôs preferred language.

Whether just thinking about a product roadmap, requiring assistance proving out the technology and outcomes, or transitioning from a proof of concept to a robust and scalable, production-grade implementation, we will guide you every step of the way.

Contact us today to learn more about how OpsGuru can leverage Generative AI solutions to enhance your business outcomes.

## 

‚Ä¢ AWS

‚Ä¢ Generative AI

# OpsGuru announced as an AWS Premier Tier Service Partner, becoming the only Canadian-Based AWS Premier Partner

## Introduction

‚Ä¢ AWS

OpsGuru recognized as Canada‚Äôs leader in AWS cloud solutions, with world-class AWS Premier Partner Status

OpsGuru, the leading Canadian cloud consulting organization, today announced it has become the only Canadian-based partner in the Amazon Web Services (AWS) Partner Network (APN) to achieve the AWS Premier Tier Services Partner Status ‚Äì a bespoke recognition awarded to select world-class partners who have demonstrated their technical expertise with AWS-validated solutions and leadership in their market.

Headquartered in Vancouver with offices in Toronto, Richmond Hill and Saint John, OpsGuru is a highly distinguished Amazon Web Services Partner. This latest distinction further sets OpsGuru apart and recognizes the organization as Canada‚Äôs leading cloud consulting partner.

‚ÄúAs the only Canadian-based partner to hold AWS premier partner status, we are tremendously proud of our team for reaching this milestone,‚Äù says Dave Lindon, General Manager of OpsGuru. ‚ÄúThis recognition reflects hundreds of successful cloud transformation journeys with great Canadian customers such as Equinox Gold and Trimac. Like AWS, OpsGuru‚Äôs steadfast customer obsession makes the AWS partnership entirely natural.‚Äù

To become an AWS Premier Tier Services Partner, companies must complete a rigorous approval process through accreditations and certifications, demonstrate a long-term investment in their work with AWS, and have extensive expertise in deploying customer solutions on AWS. In addition, AWS Premier Tier Services Partners must have a strong team of AWS-trained and certified technical consultants and deep expertise in project management and professional services.

‚ÄúOur vision is to be the leading and most trusted end-to-end multi-cloud service provider, empowering our customers across Canada by enabling their digital transformation,‚Äù says John Witte, CEO of Carbon60, OpsGuru‚Äôs parent company. ‚ÄúAchieving the AWS premier partner status not only recognizes our leadership in the Canadian market but also demonstrates our success in scaling impactful cloud solutions tailored for the needs of every customer.‚Äù

With over 240 employees, Carbon60 and OpsGuru continue to write a Canadian success story by accelerating growth to support hundreds of customers to transform their businesses using cloud technologies.

Contact us today to learn more about how OpsGuru can help you on your digital transformation journey.

For more information, please read our press release.

## 

‚Ä¢ AWS

# OpsGuru an AWS SaaS Competency Launch Partner

*Today OpsGuru announces that we have achieved the Amazon Web Services (AWS) SaaS Competency. We are also honoured to be the only Canadian Launch Partner of the AWS SaaS Competency.*

## Introduction

‚Ä¢ AWS

Today OpsGuru announces that we have achieved the Amazon Web Services (AWS) SaaS Competency. We are also honoured to be the only Canadian Launch Partner of the AWS SaaS CompetencyAchieving an AWS Competency designation is not an easy feat. For the past few months, OpsGuru has worked with AWS through a rigorous validation process. The detailed process required OpsGuru to prove our expertise in designing architectures and deployments for multi-tenant, secure and scalable SaaS platforms on AWS. The process also required customer validation and review, obsessing over details of our previous work with our customers to ensure quality. One of the key differentiators of the AWS SaaS Competency is its focus on multi-tenancy. While the AWS Well-Architected Framework lays out clear guidance for establishing a strong foundation for any AWS workload, multi-tenancy adds a new dimension of complexity. The requirements for operational excellence, security, reliability, performance and cost optimization become that much more uncompromising as the expectations for customer experience on SaaS offerings are constantly being raised. While OpsGuru prides itself on Big Data and Kubernetes expertise, we believe that such cloud-native technologies only show real value when the cloud foundation is strong and sound. As such, OpsGuru has invested heavily in the , because we believe that a strong foundation is the only way to a secure, scalable, dependable and cost-effective SaaS offering. Through the last few years, OpsGuru has worked with (ISV‚Äôs) to adopt, expand and optimize on AWS. As a result, we have developed deep insights into the exact questions ISVs should ask when deploying and optimizing a SaaS solution on AWS. This allows us to lead our customers to tackle the highest risks early in the design process by identifying performance bottlenecks, cost inefficiencies, operational gaps, and vulnerable security practices well before they could find their way into the product. Despite AWS providing almost infinite scaling potential, SaaS applications regularly push the boundaries of AWS and require deep thought to fortify and ensure platforms can meet growing peak demands. As COVID-19 continues to usher in a new economic landscape, SaaS has rapidly become the preferred software delivery mechanism. CIOs place deep trust in SaaS organizations to operate their entire stack and this trust requires SaaS vendors to target the highest levels of availability. Trust is indeed easy to lose and hard to regain. To celebrate the achievement of the AWS SaaS competency, OpsGuru will be releasing a whitepaper distilling some key lessons learned entitled ‚ÄúBuilding a SaaS Offering on AWS‚Äù. This whitepaper will share a number of our best practices learned and highlight factors that your team should consider when delivering a multi-tenanted solution. The paper will be released next week, but you can secure one of the first copies by submitting your contact information . Interested in learning more about what the AWS SaaS Competency is? Are you curious about how OpsGuru can help your organization build a SaaS offering in tune with the drastically changing commercial environments and customer behaviour? Please reach out to us at .Cloud Launchpad a number of independent software providers here info@opsguru.com

## 

‚Ä¢ AWS

# OpsGuru and House of Brick partner for confident and cost-effective Oracle migrations to AWS

## Introduction

‚Ä¢ AWS

‚Ä¢ House of Brick

OpsGuru, a leading cloud modernization consultancy, announces their partnership with House of Brick, a leading specialist in optimizing software licensing costs, to help customers manage the risk of migrating and modernizing operationally critical Oracle databases on AWS.

While many organizations have come to rely on Oracle‚Äôs resilience and stability to run core business processes, the recent dramatic upswing in consumer online activity can drive significant operational costs. Dave Lindon, General Manager at OpsGuru, says, ‚ÄúAs more consumers go online, companies need the capability and technology to support that growth.‚Äù

The partnership helps Oracle customers migrate Oracle-based workloads confidently and kick-start database modernization in the cloud. Lindon says, ‚ÄúCompanies are looking to shave margins any way possible. Amazon Web Services lets you harness the latest processor technology much faster than many on-premises environments, which will help customers see significantly more bang for the buck.‚Äù

‚ÄúHouse of Brick welcomes partnering with OpsGuru, particularly given their strength in customer migrations to AWS,‚Äù states Bob Lindquist, VP of Global Partner Development at House of Brick Technologies. ‚ÄúWith our partnership, customers will experience end-to-end cloud solutions and support options, whether for first-time AWS migrations out of a data center to business-critical system migrations of Oracle.‚Äù

Migrating Oracle workloads to AWS can cut costs and provide customers with the scalability they need to keep up with growth. In addition, it can be the first step towards complete database modernization on the AWS platform.

Lindon says that while organizations appreciate the benefits of an AWS migration, doing it in-house can be disruptive, particularly for organizations with little or no experience in complex cloud migrations. ‚ÄúWe‚Äôve facilitated nearly 500 cloud migrations, mostly to AWS, and we‚Äôve learned a lot from that,‚Äù he says. ‚ÄúWe‚Äôre able to navigate the Oracle journey to AWS better than anyone else.‚Äù The key to success is OpsGuru‚Äôs ability to understand unique customer requirements to develop a relevant solution. ‚ÄúWe don‚Äôt carbon-copy one migration for another customer,‚Äù he says.

Migrating to AWS enables organizations to outsource data centre maintenance and administration, take advantage of new software development practices and leverage the entire suite of AWS technologies and services, such as its data analytics suite. Lindon says, ‚ÄúBy not being beholden to your data centre size, you open everything up for your organization.‚Äù

OpsGuru uses several AWS services for migration and modernization, such as the Partner Opportunity Acceleration Program and the Database Freedom program to ensure quick, successful migrations.

House of Brick and OpsGuru have already worked together to enable many successful AWS migrations. House of Brick helps customers ensure licensing compliance, prepare for audits, and avoid costly penalties that can result from them. Lindon says, ‚ÄúYou want to make sure that when you migrate, your licensing costs do not drastically increase, and you remain compliant. House of Brick are experts at this.‚Äù

‚ÄúAt House of Brick, we help customers ensure a compliant and performant migration of Oracle to AWS. HoB‚Äôs Oracle license advisory services and our OpsCompass license management tools complement OpsGuru‚Äôs AWS migration consulting options, enabling Oracle customers to migrate both their database platform and licenses in the most cost-effective and highly resilient manner,‚Äù says Lindquist.

The two companies recently migrated Oracle workloads for an auction house experiencing a sustained spike in online activity. ‚ÄúThe company‚Äôs database and technology costs were getting out of hand,‚Äù Lindon says. ‚ÄúIt needed a platform to accommodate growth and be able to bring new products to the market, which wasn‚Äôt possible before its migration.‚Äù He says that the migration gave the company immediate access to the cost and performance benefits of running Oracle workloads in the cloud, with the ability to modernize with an open-source database going forward. ‚ÄúThey took the ‚Äòbest of both worlds‚Äô approach,‚Äù he says.

## About OpsGuru

OpsGuru and the 2021 Canadian AWS Consulting Partner of the Year, specializes in cloud adoption, application modernization, Kubernetes enablement, managed cloud operations, cloud security and data analytics services. The company provides customers with guidance for solutions like networking, big data, DevOps, migration and IoT for markets such as financial services, retail, mining, public sector, SaaS, and digital media.

## About House of Brick

House of Brick couples innovative software solutions with world-class consulting expertise to solve the industry‚Äôs most complex cloud migration and operational challenges. We automatically monitor our customers‚Äô cloud environments for cost overruns, security vulnerabilities, compliance variances, and operational inefficiencies to alert them to issues before they become problems. With nearly 25 years of proven success, House of Brick focuses on optimizing Oracle and Microsoft workloads running in AWS, Azure and VMware public and private cloud environments. To learn more, visit

Contact us today to learn more about how OpsGuru can help you migrate your Oracle database to AWS.

For more information, please read our press release.

# OpsGuru's ‚ÄúBuilding a SaaS Offering on AWS‚Äù has been Released\!

## Introduction

A few weeks ago, we announced the exciting news that OpsGuru has been awarded the inaugurating AWS SaaS Competency. While we are proud to be the only Canadian-based AWS Partner, we are grateful for the number of clients who have worked with us and who have been instrumental to our growth as an AWS Partner.

We have spent the last few weeks working on the whitepaper ‚ÄúBuilding a SaaS Offering on AWS‚Äù. Celebrating the AWS SaaS Competency is one of the drivers, but our main consideration has been to share our experience and thinking process of designing multi-tenant SaaS offerings with the community. The pace of digital transformation has quickened ‚Äî we have all witnessed in recent weeks how business transactions have completely changed. As the world is gradually recovering from the unprecedented shockwave of COVID-19, SaaS adoption is picking up pace. In many cases, there is no turning back from a multi-tenant subscription-based software-as-a-service model.

A number of factors, including multi-tenancy, high availability and cost-effectiveness, need to be considered for a successful SaaS product design. While each SaaS offering is unique, there are common design patterns that can be adopted. In the whitepaper, we have particularly focused on deploying applications on Amazon Elastic Kubernetes Service (EKS) with a number of data stores, because Kubernetes is a much-trusted container orchestrator and aligns well to the needs of multi-tenanted workload deployments. It‚Äôs feature-richness, fast responses to metrics and effectiveness in sharing resources, make it a good choice for designing a cost-effective solution.

To access the whitepaper, please go to the link. We‚Äôd love to hear your feedback on the content and about your journey building a SaaS solution. Should you have any questions, please simply reach out. info@opsguru.com

## 

‚Ä¢ 

# Securing Kubernetes secrets: How to efficiently secure access to etcd and protect your secrets

## Securing Kubernetes secrets: How to efficiently secure access to etcd and protect your secrets

Etcd is a distributed, consistent and highly-available key value store used as the Kubernetes backing store for all cluster data, making it a core component of every K8s deployment.

Due to its central role etcd may contain sensitive information related to access of the deployed services and their associated components, such as database credentials, CA keys, LDAP logins credentials it is a premium target for malicious attacks.

Historically, in traditional, non-containerised environments, this data was NOT stored in such a centralised manner as credentials were usually under an ownership of a specific team that was responsible for maintaining a certain component of the stack: the DB access credentials, for example, were known only to the DBA team, CA keys have been in the hands of few selected System Administrators etc.

With K8s, the required approach is notably different as credentials are now kept within a single central place (etcd), which, if not properly hardened, can lead to serious security breaches as the attacker may now create fake certificates, access databases and applications.

Managing and hardening your secrets becomes even more critical with tools such as Helm and Tiller; these tools allow you to install (or redeploy) an entire K8s based datacenter within minutes and they constantly interact with etcd.

The Center for Internet Security (CIS) came up with this publicly available document providing guidance on how to properly harden and secure your Kubernetes cluster.

The only single recommendation CIS provides regarding hardening etcd is using TLS:

## Encrypting Secret Data at Rest

Starting with K8s 1.7 (and etcd v3) you can encrypt resources inside etcd using several different algorithms. At the very least, you should encrypt all your secrets. It is especially true if you are using Helm as a lot of Helm charts require LDAP or DB credentials to be directly made available in the ConfigMaps.

The encryption follows a very simple rule:

encrypt using the first provider defineddecrypt after locating a functional provider at checking each provider in the order the providers are defined

To implement the full workflow, it is necessary to add the experimental-encryption-provider-config flag to the apiserver

Define the EncryptionConfig config file (place the content in /etc/kubernetes/pki/encryption-config.yaml)

Within the file, the resources.resources field is an array of Kubernetes resource names that should be encrypted. The providers array is an ordered list of the possible encryption providers.

Enable experimental-encryption-provider-config in the kube-apiserver. Edit /etc/kubernetes/manifests/kube-apiserver.yaml and add:

Restart the apis

# Setting up a Multi-tenant Amazon EKS cluster: a few things to consider

## Introduction

OpsGuru prides itself in heavy use of cloud-native technology, and Kubernetes is often the primary platform of choice to run containerized workloads. Because of its flexibility and support on bare metal, virtual machines and across all major cloud providers, Kubernetes has become the most popular container orchestration platform. The nature of the workloads have also changed: instead of being the advocated platform only for simple stateless workloads, Kubernetes is now also used for databases, machine learning workflows and a variety of complex applications.

Ever since Amazon EKS has been made Generally Available since 2018 (and it has just been made available in Canada. Yay\!), it has been noted as the first choice of running Kubernetes workloads on AWS. Hosting a Kubernetes platform on your own is complex, expensive and dubious in business value. In most cases, Amazon EKS ‚Äî the managed Kubernetes services provided by AWS ‚Äî is the unassailable choice to manage Kubernetes workloads on AWS.

Amazon EKS can be especially attractive to the needs of a multi-tenant service, because the Kubernetes orchestration layer supports running drastically different workloads on the same server and therefore increasing the density on the Amazon EC2 instances. However, in order to run a SaaS application on Amazon EKS, the hygiene and security concerning multi-tenant data and access have to be considered carefully. The following are a few points that any SaaS service should consider at using Amazon EKS to run their services.

## Each tenant should have its own namespace

Namespace is essential in a multi-tenant EKS cluster, where it can be used as logical boundaries separating tenants. Such boundaries are further fortified by security and policy constructs such as role-based access control (RBAC) and resource quotas (discussed below). The goal is to limit the blast radius by ensuring that only resources within the same namespace can access each other, that external entities (e.g. a user) can only access the objects when specific access to the namespace is granted. Once properly set up, the tenants of different namespaces should be protected from each other, similar to how resources in different AWS accounts are protected from each other.

## Soft Isolation via ResourceQuota to avoid erosion of resources by noisy tenants

Other than resource isolation from each other, namespaces can be used to ensure resources (CPU, memory, storage ‚Ä¶ ) are fairly shared instead of being monopolised by workloads in particular namespaces. The ResourceQuota object can be used to limit the total consumption of CPU, memory, storage and number of objects of all processes within the namespace (and therefore a tenant based on the 1:1 namespace/tenant mapping). To expand on the same idea, ResourceQuota can also be used to prioritise regular and premium tenants based on the agreements made with the SaaS provider. Meanwhile, to ensure resource sharing within a namespace, LimitRange is a handy object to make sure no container can run away with resources, and the globally defined PriorityClass objects can be used to assign available resources based on the priority of the workloads.

## Hard Isolation via enforcing 1:1 mapping between instance groups and tenants

In the above scenario, the pods for multiple tenants share the same Amazon EC2 instances that run as nodes in the same Amazon EKS clusters. In a scenario where harder separations are needed e.g. completely separate groups of Amazon EC2 node groups, the mechanism of taint,tolerations and nodeSelector can be used to achieve that.

tenant=A:NoSchedule kubectl label nodes node1 tenant=A

As an example, consider a number of nodes that should only execute workloads from tenant A. The first step is to ‚Äútaint‚Äù those nodes with a key/value pair and assign proper labels to it

This taint will ensure only pods with the key/value pair (tenant,A) can run the node. The label is used to help tenant A workloads identify where they should run. While taint and label look similar, they serve different purposes.

Meanwhile, the workloads should satisfy the taint requirement by containing the toleration

\- key: ‚Äútenant‚Äù value: ‚ÄúA‚Äù effect: ‚ÄúNoSchedule‚Äù

To ensure that the workload will only run on the dedicated nodes, nodeSelector needs to be used as well, leveraging the labels that have been set earlier.

nodeSelector: tenant: A

With the use of tolerations and taints, one can assign the workloads of namespace/tenant A with the proper tolerations to only nodes associated with tenant A (that are all tainted accordingly). The same toleration and taints can be applied to workloads of other talents. As a result, workloads are spread to different node groups (which are often autoscaling groups), sharing therefore only the control plane. This is a handy way to easily capture instance costs associated with tenants because billing reports with tenant-based tags on the instances is straightforward.

While the above is an ideal scenario to perform hard isolation by separating nodes used by tenants, extraneous security measures will then need to be considered to avoid malicious workloads trying to run workloads on node groups associated with other tenants. A ValidatingAdmissionWebhook is one way to ensure only pods that have expected tolerations are permitted. In fact, the CNCF project Open Policy Agent (OPA) is a good tool to support exactly that. Here is an AWS blog on how to get OPA started on Amazon EKS.

## Integrate Amazon IAM to AWS EKS Cluster to control cluster access

In earlier paragraphs, I have focused the discussion from the resource perspective, but good multi-tenant practice on Amazon EKS is heavily dependent on good security posture too. The first and foremost security practice on Amazon EKS ‚Äî particularly for a multi-tenant cluster ‚Äî is role-based access control, which can be tightly integrated with Amazon IAM.

At setting up RBAC on Kubernetes, the first thing to understand is the difference between cluster-roles and roles. While cluster-role may be convenient because it is applicable to the entire cluster (a reason why many beginner tutorials of Kubernetes default cluster-admin role because of its simplicity), it is highly discouraged in a multi-tenant setup. If any entities need access to a tenanted-namespace, the namespace-specific roles should be used instead (in fact, it can be argued that it is better to set up similar roles across each of the tenanted namespaces than to set up a cross-namespace cluster role for the same functionality). AWS IAM can be easily mapped to a role in the EKS cluster. AWS has provided clear documentation on the different ways to map AWS IAM credentials to an EKS cluster-role or (namespace-specific role).

## Integrate AWS IAM to EKS workloads to enforce AWS resource access

AWS IAM roles are not only useful for controlling the objects in the Amazon EKS cluster, but it is necessary to control access from workloads to AWS. The usual security practice of applying AWS IAM roles to the underlying Amazon EC2 instances is insufficient because they need to include all permissions necessary to support all actions that will take place on the nodes, namely the access of all possible workloads across all tenants. In a pool of worker nodes that can run workloads of multiple tenants, it is not possible to implement tenant-specific IAM policies on the Amazon EC2 instances. In fact, even in a single-tenanted cluster, IAM roles on the Amazon EC2 nodes are inadequate as a security construct because it requires to be a super-set of all permissions needed.

Since September 2019, AWS IAM has added support for EKS workloads on the pod level. This is done by first mapping namespace-specific service accounts to AWS IAM roles, then associate the service accounts with the individual pods. By doing so, you are not only using AWS IAM to control access to AWS resources by the workloads running on the EKS cluster, but also ensuring such workloads only have access to the tenant-specific resources. For example, often Amazon S3 buckets are used to store permanent data, and tenant-specific data can be stored under specific paths in the same bucket. By assigning IAM permissions to each pod running on the EKS clusters, you can ensure that there is no unintended cross-tenant data access to the data in the AWS S3 buckets, as each pod can be assigned access only associated with the tenant the pod is running with (A side note: the recent announcement of Amazon S3 Access Points look especially attractive for multi-tenant workloads).

## Control Communications via Network Policies

Network policies are used to control ingress and egress permissions based on multiple criteria. For a multi-tenant EKS set up that maps tenants to namespaces, namespaceSelector and podSelector can be used to limit cross-namespace communications and even communications amongst different pods within the same namespace.

Suppose there is a namespace tenantA already set up, and there is a label associated with it that is in the form of

kubectl label namespace/tenant-a tenant=a

The following example is a network policy that allows only same-namespace traffic within the tenantA namespace getting to pods with labels ‚Äúapp:api‚Äù

apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: same-namespace-only namespace: tenant-a spec: podSelector: matchLabels: app: api policyTypes: \- ingress \- egress ingress: \- from: \- namespaceSelector: matchLabels: tenant: a egress: \- to: \- namespaceSelector: matchLabels: tenant: a

## Pod-Host Access Control to Double Assurance on Multi-tenant Security

While PodSecurityPolicy is intended to limit access to the underlying EC2 instances in an Amazon EKS cluster, by the same token it can be used to limit access the shared resources in the EC2 instances in a multi-tenant cluster.

By not using pod-security policy, you are essentially implementing

apiVersion: policy/v1beta1 kind: PodSecurityPolicy metadata: name: privileged annotations: seccomp.security.alpha.kubernetes.io/allowedProfileNames: '\*' spec: privileged: true allowPrivilegeEscalation: true allowedCapabilities: \- '\*' volumes: \- '\*' hostNetwork: true hostPorts: \- min: 0 max: 65535 hostIPC: true hostPID: true runAsUser: rule: 'RunAsAny' seLinux: rule: 'RunAsAny' supplementalGroups: rule: 'RunAsAny' fsGroup: rule: 'RunAsAny'

It is easy to discern, by observing all permissions granted in the volumes stanza, what security pitfalls the default policy pose. In a scenario where pods from multiple namespaces/tenants are sharing the underlying resources of the Amazon EC2 instances, not locking down access from the pod to host can mean inadvertently exposing data across tenants, if the data is stored/cached in the associated volumes.

## Summary

The above is by no means a comprehensive set of rules to set up multi-tenants on Amazon EKS. However, as Amazon EKS is becoming a popular choice for hosting multi-tenant services, it is important to consider how to properly apply the resource and security boundaries between tenants to ensure functionality for the service. Amazon EKS presents many exciting possibilities ‚Äî especially with the increasing maturity of persistence data management.

Are you interested in learning more about using Amazon EKS to run your multi-tenant solution?

Are you wondering about other architectural design patterns applicable to multi-tenant solution on AWS? The OpsGuru team is ready to work with you to adopt the cloud and to optimise your workloads. Let us have a chat info@opsguru.com.

Written by: Mency Woo

# Using AWS Step Functions and GenAI to Accelerate Your Social Media Game

## Introduction

AWS has recently enhanced its capabilities by integrating Amazon Bedrock, a powerful Generative AI platform, directly into AWS Step Functions. This integration marks a significant advancement, paving the way for more streamlined and sophisticated workflows that leverage AI to generate content, analyze data, and automate complex processes.

In this blog, we‚Äôll explore how this enhanced integration can be leveraged in a real-world scenario of social media management. Focusing on the automation of content creation and scheduling for various events and announcements, we‚Äôll outline a workflow that begins with Amazon DynamoDB triggering AWS Step Functions. Amazon Bedrock then generates content for announcements and approval emails, with drafts reviewed via AWS Simple Email Service (SES). After refining and finalizing this content, it‚Äôs scheduled for posting on social media platforms, facilitated by AWS Lambda.

This innovative approach, combining AI-powered personal refinement through email approvals, streamlines the process of maintaining a dynamic and engaging social media presence. It not only ensures brand consistency and efficiency but also reflects the powerful capabilities AWS tools offer in transforming digital marketing strategies.

Join us as we delve into this practical application, demonstrating how the latest advancements from AWS can revolutionize event-driven social media strategies for businesses.

## The Workflow: From Data to Publication

### Event Data as a Trigger

‚Ä¢ Data Source: AWS DynamoDB is utilized to store detailed information about upcoming events and announcements. Each record includes a high-level prompt that encapsulates the theme or key message of the event.

‚Ä¢ Role in Workflow: The AWS Step Function is periodically triggered by the AWS EventBridge Scheduler. The first step is to fetch the next upcoming event from this DynamoDB table. When it finds an event that‚Äôs scheduled time has arrived, it runs the rest of the workflow, effectively kickstarting the automated content generation process.

### Content Generation

‚Ä¢ Service: Multiple steps using Amazon Bedrock InvokeModel are utilized for text generation.

‚Ä¢ Task: Leveraging the versatility of Bedrock, the workflow initiates the generation of a social media or internal chat message. Upon receiving the event prompt from DynamoDB, Bedrock first crafts the text for the post, capturing the essence of the event. Subsequent steps also generate the email subject, and the sentiment, using LLM summarization and sentiment analysis capabilities, which helps the reviewer respond to the pending approval emails faster. Generated content is also stored in the DynamoDB table.

### Feedback Loop via Email

‚Ä¢ Service: AWS Simple Email Service (SES) is used for communication.

‚Ä¢ Task: The announcement content generated by Bedrock, along with the subject and sentiment, is injected into the content approval email template and automatically emailed to designated reviewers using SES. Approve and reject links are also populated in the email, such that reviewers can provide instant feedback, facilitating a streamlined review process without a complex UI.

### Approval

‚Ä¢ Process: Final status is updated in the DynamoDB.

‚Ä¢ Outcome: Once final approval is received via email, AWS Step Functions schedules the content for publication at the optimal time.

## Seamless Publication

‚Ä¢ Integration: AWS Lambda functions facilitate integration with various social media platforms‚Äô APIs, like Facebook and Instagram, or internal chats like Slack.

‚Ä¢ Result: The approved and scheduled content is automatically posted on social media channels, ensuring a timely and effective online presence for each event. The final status is updated in the DynamoDB, so it‚Äôs possible to keep track of approved and rejected content and use approved announcements to further fine-tune the model.

## Understanding Amazon Bedrock and Its Models

Amazon Bedrock is AWS‚Äôs platform for Generative AI, providing a suite of models for various tasks, including content and image generation. Bedrock enables users to leverage large language models (LLMs) and image generation models to automate and enhance creative processes.

For this demonstration, we would utilize the Amazon Bedrock: Llama 2 model from Meta. Llama 2, a proficient language model, will be our tool for generating the textual content of social media posts, the approval email subject, as well as text classification for sentiment analysis. In our demo, the Llama 2 model on AWS Bedrock will take input prompts related to specific events and craft relevant, engaging text.

Please note that in a real scenario, testing multiple models and choosing the optimal model size are always recommended. Few-shot learning (FLS) and model fine-tuning can improve results even further.

We could also enrich our generated content by adding the visual aspect, e.g. by leveraging the Stable Diffusion XL, an advanced image generation model. This model could use the subject, sentiment, and descriptions from the previous steps to create complementary images, providing a visually appealing presentation for each social media post.

## Sample Event Workflow:

An event entry was saved to the DynamoDB with the following details:

First, we will use Meta‚Äôs Llama 2 FM to work with the text. A prompt, passing this raw event data, will help us craft an email to make a decision on whether a social media post should be created.

Prompt: Using the above event details, please provide a one or two-sentence overview describing the high-level nature of the event. Do not add any additional comments. Include the date if known.

Using the summary information, we will create an email body and then have the FM create a subject for the email.

Prompt: Using the email details above, please generate a summary with less than 100 characters that can be used for the subject of an email. The summary should start with ‚ÄúNew Social Media Opp: ‚Äù and include the event date if known. Do not provide any additional comments.

Using the generated subject and body, we will use the Simple Email Service (SES) to interact with a defined user. If the user responds that we should create a social media post, we will use Llama 2 to draft our social media post.

Prompt: Using the event details above, create content for a social media announcement.

Next, we will use the details to understand the main focus of the event.

Prompt: From the event details above, what are the three main keywords describing the event?

After that, we will use the keywords to suggest an appropriate image to include with our post.

Prompt: Using the above keywords, can you provide a recommendation for an image to use for a related social media post that incorporates at least two keywords?

Finally, we will use the suggestion to generate an image using the Stable Diffusion XL model.

Prompt: A photo of a dinner party being held at a Las Vegas restaurant, with the Las Vegas Strip visible in the background through a window. The dinner party could be a launch party for a new product or service, and the image could show guests enjoying their meals and mingling with each other.

Once we have the draft post created, we can respond back to the user to review and refine content, understand which platforms the post should appear on and then execute the post using APIs.

## Conclusion

This example showcases how leveraging GenAI with Amazon Bedrock and AWS Step Functions can significantly accelerate your marketing activities with minimal effort. The integration unlocks powerful capabilities for applying GenAI solutions to various business challenges, streamlining content creation, and sharing on social media.

If you need assistance in securely integrating cloud and AI technologies into your organization, don‚Äôt hesitate to contact us. OpsGuru is here to help guide you through implementing these advanced solutions effectively.

# Walkthrough \- ECS Local: Bringing ECS to your local environment

## Introduction

As someone who works with AWS on a day-to-day basis, It‚Äôs important to stay up to date with all the changes and new features of the different services on the platform. That‚Äôs how one recent announcement caught my eye ‚Äì the new capability of local testing of ECS.

I am particularly interested in ECS local testing because it is a feature that bridges a known gap. In recent years, containers have become increasingly popular. Without a doubt Kubernetes has been the leading standard for containers deployment. Whilst Kubernetes is feature-rich and powerful, it attracts users because the same Kubernetes can be run locally on a developer‚Äôs desktop.

When developing a feature, I want to be able to test it as quickly and quietly as possible. A tool that allows me to perform rapid and quiet testing while I am trying to figure things out is very helpful. Being able to deploy onto Kubernetes means that I can test the same manifests locally, making sure that all changes can be tested before the deployments are scrutinized by my peers and potentially impact shared resources, it gives me confidence that I am not wasting my teams‚Äô resources and time.

ECS was first announced in 2013 and has been the go-to tool for containers orchestration on AWS (as evidenced by the number of new features on the public containers roadmap). While others can get on the heated ‚Äî and even religious ‚Äî debate on the virtue of Kubernetes vs ECS, the straightforwardness of ECS, the tight and intuitive integration of ECS with the other AWS features, is very attractive. For someone who is heavily leveraging AWS, and needs some container orchestration for part of workloads, more often than not ECS is sufficient.

Until now, it has been challenging to encourage developers to work closely with ECS, because developers cannot test their work unless they deploy directly onto AWS ECS clusters that are running on AWS. Prior to this release, it is necessary to raise an ECS cluster on AWS and test the task definitions on the cloud to verify ECS workloads. Whilst this makes sense for feature integration and performance testing, it is a significant entry barrier for early development. Not every developer will have direct access to ECS resources, and to follow through with the CI/CD workflow for every minute change tends to be too slow for feedback. To workaround the overhead, developers often choose to develop with docker-compose, and leave the ECS testing for a later state. This delay can make deployments more error-prone and ECS more difficult to adopt. With the new feature, the barrier for ECS adoption seems to be significantly reduced.

The new ECS feature supports testing of network modes, secrets, volumes and credentials without the requirement of launching an ECS Task or an ECS Service on AWS and doing everything locally on the work-station. This is achieved by ecs-cli, which is a wrapper of Docker and Docker Compose that simulates a fully functional ECS environment via a special container. With this wrapper, the user can locally get mock endpoints and resources available to test tasks and verify application functionality.

## Pre-requisites

The usual tools associated with a container-based development environment still apply; namely, Docker and Docker Compose. On top of that; installing ecs-cli command line tool.

Valid AWS credentials are required too:

‚Ä¢ Install Docker: 

‚Ä¢ Install Docker Compose: 

‚Ä¢ Install AWS ecs-cli: 

‚Ä¢ Export your AWS ACCESS KEY and AWS SECRET KEY to Environment variables:

For this walkthrough, I have created several ECR Repositories:

export AWS\_ACCESS\_KEY\_ID=xxxxxxxxxxxxxxxxxx export AWS\_SECRET\_ACCESS\_KEY=yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy export AWS\_DEFAULT\_REGION=us-west-1

In each repository, there are several images e.g. app2

The same images are accessible on the command line with ecs-cli command line tool. For example, the following will list the available images, along with respective metadata.

ecs-cli images

In order to pull the images from the ECR to my workstation I need AWS credentials and Docker authentication with the remote repository. The following command will initiate and perform login from my local machine to ECR

$(aws ecr get-login \--no-include-email \--region us-west-1)

Before local execution of ECS takes place, I need to define the run-time parameters for the containers, such as the memory and CPU requirements and the ports opened. This is done in the ECS task definition. While the task definitions need to be created on AWS initially, I can download the task definition as JSON file to my local machine or use remote task definitions for local testing. For my test application (app-main), I have already created several versions of a task definition on ECS.

## The ecs-cli Walkthrough

ecs-cli for local testing have only 4 subcommands for now:

## Launching a Task

The following command will locally launch a Docker container based on a task definition:

ecs-cli local up \--task-def-remote app-main:6

app-main is the name of the task-definition and 6 is the revision of the task definition ‚Äì If the version is omitted, the latest version will be used.

Trying out with the latest version of my task definition.

and voil√†\!

ecs-cli local up

did a few things:

1\. Wrote docker-compose.ecs-local.yaml file

2\. Wrote an additional docker-compose.ecs-local.override.yml file

3\. Created local network ecs-local-network for containers

4\. Pulled Image amazon/amazon-ecs-local-container-endpoints

5\. Used docker-compose binary to start the relevant containers as described in our Task Definition

6\. Pulled a corresponding Image of the relevant container from ECR

7\. Created Container ecslocal-lab\_app-main\_1

## Examining the Running Task

ECS Local uses the Docker Compose installed on the local machine to create network for containers and provision those containers. The information is captured at the generated docker-compose.ecs-local.yaml file. The ecs-local.yaml file acts as the local counterpart of the task definition that is stored on AWS.

Content of docker-compose.ecs-local.yml

Another file docker-compose.ecs-local.override.yml created is used to define a single container that emulates how credentials are exposed to a running container.

Content of docker-compose.ecs-local.override.yml

At examining the running processes with ecs-cli, I can see that my test application is running.

ecs-cli local ps \--all

docker ps \--all

However, if I repeat the above using native docker command, an additional container running alongside the application container is found:

The abstracted amazon-ecs-local-container-endpoints container is essentially a mock service that simulates AWS endpoints with configured ECS Task IAM Role in the task definition.

The above also shows that the application container is running on port 32771 and forwarding traffic to port 80 on the container itself. Port 80 is open because it is defined as such in the task definition.

With this set-up, I can now access the container in two ways,

Via Local network IP address by running

docker network inspect ecs-local-network

and

curl

Via the assigned Port on the local machine by running

ifconfig

to get our local IP address in network ecs-local-network

then calling against the address and assigned port

curl 169.254.170.1:32771

The container in concern is a trivial example for a container, therefore testing it is straightforward. But imagine a complex task where multiple containers are running, instead of needing to emulate the task definition locally with a docker-compose (which would be thrown away once the troubleshooting is done) there is now an AWS provided tool that translates the task definition to something that can run locally, therefore bridging some of the difficulty of making ECS usable beyond AWS.

## Stopping the Task

Once I have my local troubleshooting of ECS, I can stop all running containers and delete local network quite easily by another ecs-cli local command:

ecs-cli local down \--all

## Observations and Musings

When I first heard about ECS local testing, I thought that AWS was stepping into the right direction, to make ECS more accessible to developers. When AWS published ECS CLI a few years ago, it seemed to be following the precedents set by competitors such as Heroku, to make the containers deployment an easy one-step process. The unenviable position for ECS, however, is that while it is not as complex Kubernetes, it is not as straightforward as other developer-friendly services such as Elastic Beanstalk. ECS appears after all, to be targeting infrastructure teams to run containers easily without the complexity, but it lacks the intuitiveness for developers to adopt it readily and enthusiastically. The local testing option of ECS appears to be an attempt to draw developers closer to the service, by emulating the environment locally, and therefore supporting local work and experiment.

It may be too early to give a full verdict to the ECS local testing functionality. As the prerequisites for ECS local are still heavily based on the cloud e.g. task definitions on ECS and containers being uploaded already to ECR, it is still arguably not ready for developers who are completely new to the containers ecosystem. However, it is certainly useful to gain some local understanding of the inner workings of ECS. I look forward to new features being introduced with the local tool, and maybe examples on how to best leverage it.

It is noteworthy that ecs-cli can be used for Fargate development as well. As Fargate is the serverless version of ECS, and is expected to marry the best of containers (in terms of duration, flexibility of resource allocations) and serverless (in terms of simplicity and lack of cluster management), perhaps ecs-local is another way to approach a serverless future.

## Conclusion

We have seen that ecs-cli local is a useful wrapper for Docker and Docker Compose, which you can leverage to quickly and efficiently test your ECS Task Definition configurations locally without actually deploying your task to a remote ECS Cluster.

This enables quick and more efficient development & test cycles and is overall a great addition to the evolving AWS ECS ecosystem.

Interested in hearing more about our containers expertise?

Contact us at info@opsguru.com

‚Äì Written by Denis Astahov

# What is Cloud-Native?

## Introduction

‚Ä¢ Cloud Native

At OpsGuru, we support a diverse set of workloads across different cloud providers. While it may seem eclectic, our driver is cloud-native adoption. However, given the definition of cloud-native seems to be evolving, sometimes it is worthwhile to revisit: What exactly do we mean by ‚Äúcloud-native‚Äù?

## Cloud-Native: The Definition

From the Charter of Cloud Native Computing Foundation ‚ÄúCNCF‚Äù:

‚ÄúCloud-native technologies empower organizations to build and run scalable applications in modern, dynamic environments such as public, private, and hybrid clouds. Containers, service meshes, microservices, immutable infrastructure, and declarative APIs exemplify this approach. These techniques enable loosely coupled systems that are resilient, manageable, and observable. Combined with robust automation, they allow engineers to make high-impact chang for microservices and DevOpses frequently and predictably with minimal toil.‚Äù

The OpsGuru follows this definition too. Cloud-native doesn‚Äôt mean only public cloud, but design and implementations that give you flexibility such that you can run workloads wherever it makes sense.

## Cloud-Native at OpsGuru

But how do we implement that? Here are some highlights:

### ‚Ä¢ Containers-driven

Lightweight and portable containers are the preferred mechanism for modern application deployment because it is fast, predictable and easy to scale. It is cost-effective because of higher server density and better resource utilization. The speed and scalability of containers, however, require a more sophisticated scheduling/orchestration mechanism ‚Äì that‚Äôs why we heavily recommend Kubernetes, whose rich features and extensive support across all vendors mean that it can be used to manage workloads on all major clouds and on-premise environments.

### ‚Ä¢ Resilient and Scalable Data Solutions

As data workloads get more sophisticated, it is impossible to find one single technology that will service all workloads. For a complex workload, relational databases, key-value data stores, document data storage, data lake/warehouse/lakehouse and many more all have their roles to play. The goal is to avoid single points of failure, ensuring predictable performance by preventing noisy neighbours, and abstracting service interfaces such that users can enjoy the reliable data access without needing to know about the underlying storage complexity.

### ‚Ä¢ Infrastructure as Code

Infrastructure as code is often precursors to infrastructure automation. However, more importantly, it enforces engineering discipline by enabling code reviews and methodical testing on infrastructure provisioning. At OpsGuru we are strong advocates of Terraform because it is easy to learn, widely supported and powerful as it covers many vendors and platforms.

### ‚Ä¢ Automated capabilities

The infrastructure may scale quickly, but if the application cannot be delivered with the same velocity, the value of fast infrastructure provisioning is severely diminished. At OpsGuru we implement CI/CD for both application and infrastructure, such that the full-stack ‚Äì infrastructure resources and applications ‚Äì can be automatically provisioned and scale in and out according to the evolving demand.

### ‚Ä¢ Observability

Every solution needs to identify and track key performance metrics, collect application logs and provide tracing capabilities to maintain system health. Being able to keep the system running is only half of the solution though; at OpsGuru, we believe in data-driven enhancement. By leveraging the operational data collected, we help clients identify performance bottlenecks, security loopholes and engineer solutions to improve the overall health of systems.

## The Future of Cloud-Native

As cloud-native becomes more popular, the definition keeps evolving. However, agility, operability, resiliency and scalability are some timeless goals that all solutions should adhere to. The values of the cloud are only achievable when the tech stack can grow with the features offered by the cloud. To that end, OpsGuru aims to help our clients to get the value of their adoption and investment into the cloud.

# Voices of Women in Tech: Highlighting Women in Cloud

## Introduction

A really big assumption that people have about the technology industry is that you have to have a tech background to enter the field. That‚Äôs absolutely untrue. The tech sector has opportunities to grow at any stage that you can start from anywhere, even in the cloud. Suppose you ask people how they got into tech; you‚Äôll get different answers from everyone ‚Äì we know this because we had the opportunity to ask five very talented women how they got into the cloud and got five different answers.

Take Rekha for example, she was asked to prepare a presentation about technology trends ‚Äì this sparked something in her that led her on a learning journey to working in cloud. Meanwhile Sonia Singh, Director of Sales at ISV/DNB at AWS had no intentions of entering tech. She wanted to change career paths to an industry with growing opportunities. Much like Delynn O‚ÄôSullivan, Senior Account Manager at Carbon60 wanted to go into an industry with the most future potential for her sales career. Maria Ellis, AWS Partner Sales & Development knew she wanted to take on a new challenge and prepared herself for it so when the time came for that job interview, she was ready. If you ask Mency Woo, Director of Enterprise & Public Sector at OpsGuru, she‚Äôll tell you she‚Äôs been in cloud for over 12 years.

Five women, five different stories, all working in the same industry. We asked them about their experiences, how they got to where they are and the advice and resources they have for others, here‚Äôs what they said:

## Rekha Rao-Mayya

Sales Leader ISV and DNB at AWS

How did you get into the cloud? In late 2007, I was asked to present to a large group about future technology trends. To prepare for the presentation, I dove deep into research and came across an article on the ‚ÄúTop 10 Disruptive Technologies of the Decade.‚Äù Cloud computing was on the list. This piqued my curiosity. Interestingly, the report mentioned AWSand how Amazon had launched a game changing, market making computing model with AWS. Little did I know that I would be fortunate enough to work for AWS\! I started learning about Cloud computing, Infrastructure as a Service (IaaS) and Platform as a Service (PaaS) and started my learning journey.

What is one piece of advice you would give to women looking to get into the cloud industry? Be curious and learn to learn. It has always worked for me\! I also believe that it‚Äôs never too late to learn something new. Technology is ever-changing. Make the learning journey fresh and new every day.

What is one of your favourite resources (or tools) for learning more about cloud computing? AWS Educate program is a great resource. AWS offers Cloud computing training for all levels of knowledge seekers, and there are many free courses.

## Maria Ellis

Partner Sales & Development at AWS

How did you get into the cloud? I, too, am a Prime Member\! How ‚Äòthings‚Äô got from one place to another fascinates me. I decided I wanted a career where I could continue learning and building from the ground up. I spent the summer preparing and learning all I could about the Cloud, reading, talking to partners and understanding the technology. When an opportunity emerged to interview at AWS, I was knowledgeable and ready to take on the challenge.

What is one piece of advice you would give to women looking to get into the cloud industry? Confidence is key\! Demonstrate your expertise, get certified to ensure your relevance and differentiate yourself. Be genuine; surround yourself with champions that will help you succeed and give you honest feedback. Find a mentor\! Give back by paying it forward. Cloud is fast-paced. Change is inevitable (and fascinating) ‚Ä¶so roll with it\!

What is one of your favourite resources (or tools) for learning more about cloud computing? AWS provides career-based certification paths via podcast. Learning modules with A Cloud Guru and Udemy offer great examples to prepare you for the initial CPC certification and on to associate-based accreditations.

## Sonia Singh

Director of Sales ‚Äì ISV/DNB at AWS

How did you get into the Cloud? Before getting into tech, I worked in finance and as an educator. I had no intentions of working in the tech sector. I was teaching and travelling around the world for five years and then decided that I wanted to come back to Canada and change careers. I had two primary goals in mind: find a new job that offered lots of opportunities in a growing field and transfer my skills to a new career. My mentor helped me see that my teaching skills were transferable to tech sales. For example, teaching a topic to a class of students was the same as presenting a product to a customer in a business setting. From there, my mentor and I worked on a shortlist of companies, and he helped make recommendations and referrals. The rest is history.

What is one piece of advice you would give to women looking to get into the cloud industry? Perseverance and purpose are essential. There are times where you will want to give up, but you should keep on going and seek guidance from your supporting cast. You never know what could happen in the future. Giving up too early could set you back even further. Research future trends and pick untapped areas that offer tremendous growth potential.

What is one of your favourite resources (or tools) for learning more about cloud computing? I always have two to four internal and external mentors that I bounce ideas off and learn from at any given time.

1\. Find a role model, mentor and/or coach early. This will put you years ahead.

2\. Advocate for yourself. Don‚Äôt assume others know your background, capabilities and skills.

3\. Continuous learning is critical ‚Äì read, join tech groups, listen to podcasts, create a solid supporting cast and learn how to transfer your skills from a different career into tech. You will likely change careers several times throughout your career.

4\. Be patient, set clear goals and manage your expectations.

## Delynn O‚ÄôSullivan

Senior Account Manager at Carbon60

How did you get into the cloud? After returning from a year-long overseas backpacking trip, I met with a college friend to understand which industry would have the most future potential to pursue my sales career. Technology was the answer. I signed up with multiple recruiters, but the best reference was reaching out to my network. In 1999, I was introduced to a new start-up, where there were only two of us for the first few months, apart from the investors. Colocation and Internet B2B were the product lines. Over the next 20+ years, the company added cloud and Security Services to its portfolio while experiencing various acquisitions. This past year an acquisition led me on a different product path. However, I realized that I missed the cloud Conversation. I have since moved to a new company, Carbon60, which specializes in all things cloud, with the differentiator being in-house expertise from start to finish for our clients‚Äô cloud journey.

What is one piece of advice you would give to women looking to get into the cloud industry? Network, Network, Network. Attend events, and even if you meet just one new contact, it will be worth it. Also, it should never just be about what you need. The relationships should always aim to be reciprocal. Request informational meetings, but make sure you have done your homework, and you are not just asking your new contact basic questions. Their time is valuable. That person could be a future advocate or referral for you. Learn from others whenever you can.

What is one of your favourite resources (or tools) for learning more about cloud computing? As I am in a sales role, the resources combine selling/consultation techniques and product training. Many company sites provide course modules as well as certifications. Additionally, sites such as Udemy offer a wide variety of course options.

1\. Women in Cloud (daily updates about programs, events, mentorship and industry news)

2\. TechCrunch & Betakit for information, other job sites such as Indeed and LinkedIn provide a good reference on who and what roles are being hired.

## Mency Woo

Director of Enterprise & Public Sector at OpsGuru

How did you get into the cloud? I first got into the cloud in about 2009 as a software release engineer in a start-up company wholly built on AWS. I was responsible for developing a toolkit to build, deploy and test in different environments and maintain the high availability of the application on AWS. After experiencing how straightforward it was to provision and manage cloud resources using script and automation, I have hit the point of no return. I haven‚Äôt been away from the cloud ever since.

What is one piece of advice you would give to women looking to get into the cloud industry? Don‚Äôt be afraid to try\! Starting a project on the cloud is often a lot easier than you think. There are a lot of Youtube videos, blogs, walkthroughs that offer step-by-step learning. Cloud providers also provide free credits and resources to help you with experience ‚Äî take advantage of that.

What is one of your favourite resources (or tools) for learning more about Cloud Computing? As a beginner, I recommend courses on Udemy or Coursera. But savvy on the cloud requires staying up-to-date with the latest innovations: daily doses of Hacker News, Reddit and Twitter generally help.

## Conclusion

We want to thank Rekha, Sonia, Maria, Mency and Delynn for sharing their expertise. Their answers prove whether you‚Äôve been working in tech for years, just getting started or thinking about where to take your career next, everyone can work in tech. You don‚Äôt have to start in this industry to get here; you can start from where you are, make a pivot, take a completely different path ‚Äì there‚Äôs no one way to end up in this field or a sector, and there‚Äôs room for everyone, even up there in the cloud.

## About Women in Tech World

Women in Tech World is a national nonprofit organization that is dedicated to creating community action plans and educational programming to support and advance women within the tech industry. WiTWorld has listened to over 1,600 voices from Canadian women and men in the tech industry, resulting in the largest qualitative data set about women in the Canadian tech space. Women in Tech World is both collaborative and inclusive and welcomes all professionals in tech, including stakeholders, advocates, and allies.

